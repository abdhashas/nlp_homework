{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8c38a02a9f45259b",
   "metadata": {
    "id": "46f519c8"
   },
   "source": [
    "#[Your_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a17096249196e35c",
   "metadata": {},
   "source": [
    "<h1 style=\"text-align: center;\">Text Classification<h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67a8f525e7cb9573",
   "metadata": {
    "id": "e7efe140"
   },
   "source": [
    "Prepare libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "id": "33a20126e2ce399e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-28T18:00:40.781684600Z",
     "start_time": "2023-11-28T18:00:27.949446900Z"
    },
    "id": "af6058ed"
   },
   "outputs": [],
   "source": [
    "# here put every import you need e.g. import nltk\n",
    "# it's better to load what you need from the package by from [] import [] instead of import the whole package\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from keras.models import Sequential\n",
    "from keras import layers\n",
    "from keras.layers import Dense, Conv1D, AveragePooling1D, Flatten\n",
    "from keras.utils import to_categorical\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "import re\n",
    "import numpy as np\n",
    "from cleantext import clean\n",
    "from arabicstopwords.stopwords_lexicon import stopwords_lexicon\n",
    "import arabicstopwords.arabicstopwords as stop_words\n",
    "import nltk\n",
    "from wordcloud import WordCloud\n",
    "from arabic_reshaper import arabic_reshaper\n",
    "from bidi.algorithm import get_display\n",
    "from nltk import bigrams, trigrams\n",
    "from collections import Counter\n",
    "from snowballstemmer import stemmer\n",
    "from tashaphyne.stemming import ArabicLightStemmer\n",
    "from farasa.segmenter import FarasaSegmenter\n",
    "from farasa.stemmer import FarasaStemmer\n",
    "from matplotlib.font_manager import FontProperties\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35983a59a0a8dc81",
   "metadata": {},
   "source": [
    "download "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a355ad265838eb2",
   "metadata": {
    "id": "229df102"
   },
   "source": [
    "Download data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "id": "cf91803826981217",
   "metadata": {
    "id": "5504e7ee"
   },
   "outputs": [],
   "source": [
    "# !wget 'https://drive.google.com/uc?export=download&id=1cMSjxa3nA706LIZDEhwMpaVRMY2IX9P0' -O 'data.zip'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "id": "53e4574cd5ad3978",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# !pip install Arabic-Stopwords\n",
    "# 2.2 nltk.download('punkt') # open vpn\n",
    "# 2.3 !pip install arabic-reshaper\n",
    "# 2.4 nltk.download('stopwords')\n",
    "# 3.8 !pip install snowballstemmer\n",
    "# 3.8 !pip install Tashaphyne\n",
    "# 3.8 !pip install -U farasapy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "id": "f5f4105090de8887",
   "metadata": {
    "id": "530e1bac"
   },
   "outputs": [],
   "source": [
    "# !unzip data.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "687ae8495df53493",
   "metadata": {},
   "source": [
    "<h2 dir=\"rtl\">مثال عن كيفية تنظيف حلول الطلبات:</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c6f6c78922eb16d",
   "metadata": {
    "id": "8E7xa9XLPW8t"
   },
   "source": [
    "\n",
    "<div dir=\"rtl\">شرح ما يقوم به الكود (like code documentation)<div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "id": "3a9387562e3e7fcf",
   "metadata": {
    "id": "d41a9612"
   },
   "outputs": [],
   "source": [
    "# your code here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "id": "87f6b7e34a75e652",
   "metadata": {
    "id": "aNNrMjaaQanf"
   },
   "outputs": [],
   "source": [
    "# example test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ca50c08527ef0ca",
   "metadata": {
    "id": "d9NfyVxZQXF7"
   },
   "source": [
    "<div dir=\"rtl\">ملاحظاتك في حال وجودها</div>\n",
    "<div dir=\"rtl\">يمكنك إضافة خلايا لكل طلب بقدر ما تشاء، المهم أن تحافظ على تنظيم الملف</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89b0f65e02a313cd",
   "metadata": {
    "id": "3400638b"
   },
   "source": [
    "# Question [1]: Load the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d12bfd6042d97cae",
   "metadata": {},
   "source": [
    "## [1.1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "id": "f6397927bb088417",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-28T17:47:44.500683400Z",
     "start_time": "2023-11-28T17:47:43.519931600Z"
    }
   },
   "outputs": [],
   "source": [
    "train_file_path = 'train.csv'\n",
    "test_file_path = 'test.csv'\n",
    "validation_file_path = 'val.csv'\n",
    "train_data = pd.read_csv(train_file_path)\n",
    "test_data = pd.read_csv(test_file_path)\n",
    "validation_data = pd.read_csv(validation_file_path)\n",
    "\n",
    "# print(\"number of data in train data : \",len(train_data))\n",
    "# print(\"number of data in test data : \",len(test_data))\n",
    "# print(\"number of data in validation data : \",len(validation_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "id": "bd4ed1d4d439b324",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-28T17:47:47.493979600Z",
     "start_time": "2023-11-28T17:47:47.408587600Z"
    }
   },
   "outputs": [],
   "source": [
    "# train_data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "id": "c285a9642f16e9ed",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-28T17:47:53.408385500Z",
     "start_time": "2023-11-28T17:47:53.337280500Z"
    }
   },
   "outputs": [],
   "source": [
    "# test_data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "id": "647198775377a576",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-28T17:47:54.293475800Z",
     "start_time": "2023-11-28T17:47:54.111683600Z"
    }
   },
   "outputs": [],
   "source": [
    "# validation_data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "id": "f902f028f4f22982",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-28T17:47:55.207559400Z",
     "start_time": "2023-11-28T17:47:54.981391300Z"
    }
   },
   "outputs": [],
   "source": [
    "#drop_duplicates يزيل الصفوف المتكرره\n",
    "#dropna يحذف الصفوف المكررة\n",
    "test_data=test_data.drop_duplicates().dropna()\n",
    "train_data=train_data.drop_duplicates().dropna()\n",
    "validation_data=validation_data.drop_duplicates().dropna()\n",
    "# print(\"number of data in train data after cleaning : \",len(train_data))\n",
    "# print(\"number of data in test data after cleaning : \",len(test_data))\n",
    "# print(\"number of data in validation data after cleaning : \",len(validation_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "id": "300d578799370a2e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-28T17:47:56.205456800Z",
     "start_time": "2023-11-28T17:47:56.151617600Z"
    }
   },
   "outputs": [],
   "source": [
    "# train_data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "id": "bb28317493f6f459",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-28T17:47:57.140122300Z",
     "start_time": "2023-11-28T17:47:56.924769600Z"
    }
   },
   "outputs": [],
   "source": [
    "# test_data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "id": "b88695c3a0a3c37e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-28T17:47:57.993922400Z",
     "start_time": "2023-11-28T17:47:57.693952400Z"
    }
   },
   "outputs": [],
   "source": [
    "# validation_data.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe89dfbbc315bb24",
   "metadata": {},
   "source": [
    "## [1.2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "id": "128745bced93a650",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-28T17:48:00.089441800Z",
     "start_time": "2023-11-28T17:47:58.657780700Z"
    }
   },
   "outputs": [],
   "source": [
    "def unify_specialty(name):\n",
    "    name = re.sub(r\"[^\\w\\s]\", \" \", name)  # يزيل الرموز الترقيمية\n",
    "    name = re.sub(r\"\\s+\", \" \", name).strip()  # يزيل الفراغات المتكرره #strip يزيل الفراغات يلي بالبداية والنهاية\n",
    "    name =re.sub(\"[\\_\\-]\",\" \",name)\n",
    "    return name\n",
    "\n",
    "train_data['label']=train_data['label'].apply(unify_specialty)\n",
    "test_data['label']=test_data['label'].apply(unify_specialty)\n",
    "validation_data['label']=validation_data['label'].apply(unify_specialty)   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "id": "1b08e09e599e7d08",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-28T17:48:01.035454700Z",
     "start_time": "2023-11-28T17:47:58.909457900Z"
    }
   },
   "outputs": [],
   "source": [
    "# x=set(train_data['label'])\n",
    "# x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "id": "c3686ec8976508a8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-28T17:48:02.013238200Z",
     "start_time": "2023-11-28T17:47:59.463844100Z"
    }
   },
   "outputs": [],
   "source": [
    "Dictionary_of_diseases= {\n",
    "    'أمراض الجهاز التنفسي': 'أمراض الجهاز التنفسي',\n",
    "    'الجهاز التنفسي': 'أمراض الجهاز التنفسي',\n",
    "    'امراض الجهاز التنفسي': 'أمراض الجهاز التنفسي',\n",
    "    'أمراض الدم': 'أمراض الدم',\n",
    "    'الدم': 'أمراض الدم',\n",
    "    'امراض الدم': 'أمراض الدم',\n",
    "    'أمراض الغدد الصماء': 'أمراض الغدد الصماء',\n",
    "    'الغدد الصماء': 'أمراض الغدد الصماء',\n",
    "    'امراض الغدد الصماء': 'أمراض الغدد الصماء',\n",
    "    'مرض السكري': 'مرض السكري',\n",
    "    'السكري': 'مرض السكري',\n",
    "    'الاورام الخبيثة والحميدة':'الأورام الخبيثة والحميدة'\n",
    "\n",
    "}\n",
    "\n",
    "def Unification_name_of_diseases(c):\n",
    "    c=c.map(Dictionary_of_diseases).fillna(c)\n",
    "    return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "id": "4fd4c2952b21be5d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-28T17:48:03.134507400Z",
     "start_time": "2023-11-28T17:47:59.494860300Z"
    }
   },
   "outputs": [],
   "source": [
    "train_data['label']=Unification_name_of_diseases(train_data['label'])\n",
    "test_data['label']=Unification_name_of_diseases(test_data['label'])\n",
    "validation_data['label']=Unification_name_of_diseases(validation_data['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "id": "f86de893df69c13",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-28T17:48:03.192630300Z",
     "start_time": "2023-11-28T17:47:59.950281200Z"
    }
   },
   "outputs": [],
   "source": [
    "# x=set(train_data['label'])\n",
    "# x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2de100ffb45a290",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-28T17:48:03.684914300Z",
     "start_time": "2023-11-28T17:48:00.006191700Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4a8d8362bec9237d",
   "metadata": {
    "id": "2f22713d"
   },
   "source": [
    "# Question [2]: Text Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3abc352213c467a",
   "metadata": {
    "id": "03ef90d1"
   },
   "source": [
    "## [2.1] \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "id": "347cfd18fdd3ae19",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-28T17:53:26.541719300Z",
     "start_time": "2023-11-28T17:53:26.526459900Z"
    }
   },
   "outputs": [],
   "source": [
    "# train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "id": "cac8463d6c28b1d3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-28T17:48:03.842803200Z",
     "start_time": "2023-11-28T17:48:01.020927500Z"
    },
    "id": "c1b8c64b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "عدد الداتا الخاصة بأمراض الدم: 1398\n",
      "عدد الداتا الخاصة بأمراض الجهاز التنفسي: 3676\n",
      "عدد الداتا الخاصة بأمراض الغدد الصماء: 5752\n",
      "عدد الداتا الخاصة بأمراض ارتفاع ضغط الدم: 2537\n",
      "عدد الداتا الخاصة بالأورام الخبيثة والحميدة: 6449\n",
      "عدد الداتا الخاصة بجراحةالعظام: 2211\n",
      "عدد الداتا الخاصة بالجراحة العامة: 6307\n",
      "عدد الداتا الخاصة بمرض السكري: 4268\n"
     ]
    }
   ],
   "source": [
    "# Train data\n",
    "\n",
    "# عدد الأجوبة الخاصة بأمراض الدم :\n",
    "answers_len_in_blood_diseases = len(train_data[train_data['label']=='أمراض الدم']['answer'].tolist())\n",
    "# print(answers_len_in_blood_diseases)\n",
    "\n",
    "# عدد الأسئلة الخاصة بأمراض الدم :\n",
    "questions_len_in_blood_diseases = len(train_data[train_data['label']=='أمراض الدم']['question'].tolist())\n",
    "# print(questions_len_in_blood_diseases)\n",
    "\n",
    "\n",
    "# عدد الداتا الخاصة بأمراض الدم :\n",
    "total_blood_diseases_len = len(train_data[train_data['label']=='أمراض الدم'])\n",
    "print('عدد الداتا الخاصة بأمراض الدم:',total_blood_diseases_len)\n",
    "\n",
    "# عدد الداتا في أمراض الجهاز التنفسي :\n",
    "total_blood_diseases_len = len(train_data[train_data['label']=='أمراض الجهاز التنفسي'])\n",
    "print('عدد الداتا الخاصة بأمراض الجهاز التنفسي:',total_blood_diseases_len)\n",
    "\n",
    "# غدد الداتا في أمراض الغدد الصماء :\n",
    "total_blood_diseases_len = len(train_data[train_data['label']=='أمراض الغدد الصماء'])\n",
    "print('عدد الداتا الخاصة بأمراض الغدد الصماء:',total_blood_diseases_len)\n",
    "\n",
    "# عدد  الداتا ارتفاع ضغط الدم :\n",
    "total_blood_diseases_len = len(train_data[train_data['label']=='ارتفاع ضغط الدم'])\n",
    "print('عدد الداتا الخاصة بأمراض ارتفاع ضغط الدم:',total_blood_diseases_len)\n",
    "\n",
    "# عدد الداتا في الأورام الخبيثة والحميدة :\n",
    "total_blood_diseases_len = len(train_data[train_data['label']=='الأورام الخبيثة والحميدة'])\n",
    "print('عدد الداتا الخاصة بالأورام الخبيثة والحميدة:',total_blood_diseases_len)\n",
    "\n",
    "# عدد الداتا في جراحة العظام\n",
    "total_blood_diseases_len = len(train_data[train_data['label']=='جراحة العظام'])\n",
    "print('عدد الداتا الخاصة بجراحةالعظام:',total_blood_diseases_len)\n",
    "\n",
    "# عدد الداتا في الجراحة العامة\n",
    "total_blood_diseases_len = len(train_data[train_data['label']=='جراحة عامة'])\n",
    "print('عدد الداتا الخاصة بالجراحة العامة:',total_blood_diseases_len)\n",
    "\n",
    "# عدد الداتا في مرض السكري\n",
    "total_blood_diseases_len = len(train_data[train_data['label']=='مرض السكري'])\n",
    "print('عدد الداتا الخاصة بمرض السكري:',total_blood_diseases_len)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "id": "10e8fc4ed5f610f6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-28T17:48:03.906972Z",
     "start_time": "2023-11-28T17:48:01.470011400Z"
    }
   },
   "outputs": [],
   "source": [
    "# # Drawing pie chart:\n",
    "# # ملاحظة بالعربي طلع الخط مفشكل ف تمت الكتابة بالانكليزي\n",
    "\n",
    "\n",
    "\n",
    "# data = {\n",
    "#     'الاختصاص': ['أمراض الدم', 'أمراض الجهاز التنفسي', 'أمراض الغدد الصماء','أمراض ارتفاع ضغط الدم','الأمراض الخبيثة والحميدة','جراحةالعظام','الجراحة العامة','مرض السكري'],\n",
    "#     'عدد النصوص': [2153, 6002, 9417,4190,10711,2641,10548,7096]\n",
    "# }\n",
    "\n",
    "# data\n",
    "\n",
    "\n",
    "# df = pd.DataFrame(data)\n",
    "\n",
    "# # print(df)\n",
    "# arabic_font = FontProperties(fname='font/NotoNaskhArabic-VariableFont_wght.ttf') \n",
    "\n",
    "# df['الاختصاص'] = [get_display(arabic_reshaper.reshape(label)) for label in df['الاختصاص']]\n",
    "# plt.figure(figsize=(8, 8))\n",
    "# plt.pie(df['عدد النصوص'], labels=df['الاختصاص'], autopct='%1.1f%%', startangle=140)\n",
    "# plt.title(get_display(arabic_reshaper.reshape('توزيع النصوص حسب الاختصاص')), fontproperties=arabic_font)\n",
    "# plt.show()\n",
    "\n",
    "# # import pandas as pd\n",
    "# # import matplotlib.pyplot as plt\n",
    "\n",
    "# # data = {\n",
    "# #     'Specialization': ['Blood Diseases', 'Respiratory System Diseases', 'Endocrine Diseases', 'Hypertension', 'Malignant and Benign Diseases', 'Orthopedic Surgery', 'General Surgery', 'Diabetes'],\n",
    "# #     'Number of Texts': [1398, 3676, 5752, 2537, 6449, 2211, 6307, 4268]\n",
    "# # }\n",
    "\n",
    "# # df = pd.DataFrame(data)\n",
    "\n",
    "# # print(df)\n",
    "\n",
    "# # plt.figure(figsize=(8, 8))\n",
    "# # plt.pie(df['Number of Texts'], labels=df['Specialization'], autopct='%1.1f%%', startangle=140)\n",
    "# # plt.title('Distribution of Texts by Specialization')\n",
    "# # plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7b6917b7ad847bb",
   "metadata": {
    "id": "24164796"
   },
   "source": [
    "## [2.2] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "id": "af188fd4f58322bb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-28T17:48:03.906972Z",
     "start_time": "2023-11-28T17:48:01.695343300Z"
    }
   },
   "outputs": [],
   "source": [
    "def delete_repeated_letter(text):\n",
    "    # cleaned_text = re.sub(r'(.)\\1{2,}', r'\\1', text)\n",
    "    cleaned_text = re.sub(r'([^\\w\\s\\.])\\1+', r'\\1', text)\n",
    "    cleaned_text = re.sub(r'(\\.\\s*)\\1+$', r'\\1', cleaned_text)\n",
    "    return cleaned_text\n",
    "\n",
    "def handle_connected_words(tokenized):\n",
    "    # Handle connected words with '-'\n",
    "    result_tokens = []\n",
    "    for token in tokenized:\n",
    "        # Split connected words with '-'\n",
    "        result_tokens.extend(token.split('-'))\n",
    "    return result_tokens\n",
    "\n",
    "\n",
    "def filter_text(list):\n",
    "    list_of_phrases =[]\n",
    "    all_tokens = []\n",
    "    for sentence in list:\n",
    "        tokenizer = RegexpTokenizer(r'\\b\\d+\\b|\\b[^\\d\\W_]{2,}\\b|[^\\d\\W_]+(?:-[^\\d\\W_]+)?|\\S')\n",
    "        tokenized = tokenizer.tokenize(delete_repeated_letter(sentence))\n",
    "        tokenized = handle_connected_words(tokenized)\n",
    "\n",
    "        # Remove '-' and '/' and one-letter words from each token\n",
    "        # cleaned_tokens = [token for token in tokenized if len(token) > 1 and token not in ['-', '/']]\n",
    "        # cleaned_tokens = [token if len(token) > 1 or token == 'و' else '' for token in tokenized if token not in ['-', '/']]\n",
    "\n",
    "        cleaned_tokens = [token if (token.isalpha() and len(token) > 1) or token == 'و' else '' for token in tokenized if token not in ['-', '/']]\n",
    "\n",
    "        combined_phrase = ' '.join(cleaned_tokens)\n",
    "        list_of_phrases.append(combined_phrase)\n",
    "\n",
    "    for phrase in list_of_phrases:\n",
    "        tokens = nltk.word_tokenize(phrase)\n",
    "        all_tokens.append(tokens)\n",
    "        \n",
    "    return all_tokens\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "id": "8841e5fae4cd12eb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-28T17:48:11.547801700Z",
     "start_time": "2023-11-28T17:48:02.015744800Z"
    },
    "id": "SexYXq1-P0Y0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1645245\n"
     ]
    }
   ],
   "source": [
    "train_data_answers_list = train_data['answer'].values\n",
    "answers = filter_text(train_data_answers_list)\n",
    "flattened_answers = [item for sublist in answers for item in sublist]\n",
    "print(len(flattened_answers))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "id": "462dd7146ffac428",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-28T17:48:17.557564200Z",
     "start_time": "2023-11-28T17:48:11.659377700Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "801979\n"
     ]
    }
   ],
   "source": [
    "train_data_questions_list = train_data['question'].values\n",
    "questions = filter_text(train_data_questions_list)\n",
    "flattened_quesions = [item for sublist in questions for item in sublist]\n",
    "print(len(flattened_quesions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "id": "a05d77a67ff63ee6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-28T17:48:17.560083300Z",
     "start_time": "2023-11-28T17:48:17.544042400Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2447224\n"
     ]
    }
   ],
   "source": [
    "# حساب عدد الكلمات الكلي :\n",
    "total_words_len = len(flattened_quesions) + len(flattened_answers)\n",
    "print(total_words_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "id": "c896baa27e7c92a5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-28T17:48:17.874055600Z",
     "start_time": "2023-11-28T17:48:17.552562600Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "عدد الكلمات الفريدة: 94614\n"
     ]
    }
   ],
   "source": [
    "# حساب عدد الكلمات الفريدة :\n",
    "total_unique_words=len(set(flattened_quesions+flattened_answers))\n",
    "print(\"عدد الكلمات الفريدة:\", total_unique_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c25bc0c76cd00150",
   "metadata": {
    "id": "KFerFDFqPwPI"
   },
   "source": [
    "## [2.3] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "id": "ec6d7dbea8e44a8b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-28T17:49:35.324646100Z",
     "start_time": "2023-11-28T17:48:17.867536700Z"
    }
   },
   "outputs": [],
   "source": [
    "# WCD=dict()\n",
    "# train_data_answers_list = flattened_answers\n",
    "# train_data_questions_list = flattened_quesions\n",
    "# train_data_list=train_data_answers_list+train_data_questions_list\n",
    "# answers_text = ' '.join(train_data_list)\n",
    "# reshaped_text = arabic_reshaper.reshape(answers_text)\n",
    "# arabic_text = get_display(reshaped_text)\n",
    "# wordcloud = WordCloud(font_path='font/NotoNaskhArabic-VariableFont_wght.ttf', background_color='white').generate(arabic_text)\n",
    "# WCD=wordcloud.words_\n",
    "# plt.figure(figsize=(50, 50))\n",
    "# plt.imshow(wordcloud, interpolation='bilinear')\n",
    "# plt.axis(\"off\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "420df21aa7c46632",
   "metadata": {
    "id": "-4LEgCtvPwaQ"
   },
   "source": [
    "## [2.4] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "id": "27677a3f844deab0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-28T17:49:35.421653500Z",
     "start_time": "2023-11-28T17:49:35.383741100Z"
    },
    "id": "d81d9f99"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "أكثر 15 كلمة مكررة: [('المريض', 2), ('قد', 2), ('يناسب', 2), ('الدكتور', 2), ('يزن', 2), ('علي', 2), ('خليف', 2), ('و', 2), ('لكل', 1), ('علاج', 1), ('ايجابيته', 1), ('وسلبياته', 1), ('والتي', 1), ('تعتمد', 1), ('على', 1)]\n",
      "أقل 10 كلمة مكررة: [('الصماء', 1), ('الغدد', 1), ('الانسولين', 1), ('ايضا', 1), ('جلوكوفانس', 1), ('الدواء', 1), ('عيوب', 1), ('مميزات', 1), ('هي', 1), ('ما', 1)]\n",
      "أكثر 10 ثنائيات مكررة في النص: [(('الدكتور', 'يزن'), 2), (('يزن', 'علي'), 2), (('علي', 'خليف'), 2), (('لكل', 'علاج'), 1), (('علاج', 'ايجابيته'), 1), (('ايجابيته', 'وسلبياته'), 1), (('وسلبياته', 'والتي'), 1), (('والتي', 'تعتمد'), 1), (('تعتمد', 'على'), 1), (('على', 'حالة'), 1)]\n",
      "أكثر 10 ثلاثيات كلمات مكررة في النص: [(('الدكتور', 'يزن', 'علي'), 2), (('يزن', 'علي', 'خليف'), 2), (('لكل', 'علاج', 'ايجابيته'), 1), (('علاج', 'ايجابيته', 'وسلبياته'), 1), (('ايجابيته', 'وسلبياته', 'والتي'), 1), (('وسلبياته', 'والتي', 'تعتمد'), 1), (('والتي', 'تعتمد', 'على'), 1), (('تعتمد', 'على', 'حالة'), 1), (('على', 'حالة', 'المريض'), 1), (('حالة', 'المريض', 'فما'), 1)]\n",
      "أكثر 10 ثنائيات كلمات مهمة في النص: [('الدكتور', 'يزن'), ('علي', 'خليف'), ('يزن', 'علي'), ('اسئلة', 'طبية'), ('الاخر', 'وهذا'), ('الدواء', 'جلوكوفانس'), ('السكري', 'ما'), ('الطبيب', 'خلال'), ('الغدد', 'الصماء'), ('ايجابيته', 'وسلبياته')]\n",
      "أكثر 10 ثلاثيات كلمات مهمة في النص: [('الدكتور', 'يزن', 'علي'), ('يزن', 'علي', 'خليف'), ('الانسولين', 'الدكتور', 'يزن'), ('علي', 'خليف', 'اسئلة'), ('علي', 'خليف', 'الغدد'), ('له', 'الدكتور', 'يزن'), ('اسئلة', 'طبية', 'مرض'), ('الاخر', 'وهذا', 'يحدده'), ('السكري', 'ما', 'هي'), ('الطبيب', 'خلال', 'مراجعة')]\n",
      "========================================================================================================================================================================================\n"
     ]
    }
   ],
   "source": [
    "def tokenized_text(text):\n",
    "    # allWords = nltk.tokenize.word_tokenize(text)\n",
    "\n",
    "    allWordDist = nltk.FreqDist(w for w in text)\n",
    "    stopwords = nltk.corpus.stopwords.words('english')\n",
    "    allWordExceptStopDist = nltk.FreqDist(w.lower() for w in text if w not in stopwords)  \n",
    "    mostCommon= allWordDist.most_common(15)\n",
    "    print('أكثر 15 كلمة مكررة:',mostCommon)\n",
    "    leastCommon = allWordDist.most_common()[:-11:-1]\n",
    "    print('أقل 10 كلمة مكررة:', leastCommon)\n",
    "    bigram_freq = nltk.FreqDist(bigrams(text))\n",
    "    most_common_bigrams = bigram_freq.most_common(10)\n",
    "    print('أكثر 10 ثنائيات مكررة في النص:',most_common_bigrams)\n",
    "    trigram_freq = nltk.FreqDist(trigrams(text))\n",
    "    most_common_trigrams = trigram_freq.most_common(10)\n",
    "    print('أكثر 10 ثلاثيات كلمات مكررة في النص:',most_common_trigrams)\n",
    "    collocations_bigram = nltk.collocations.BigramCollocationFinder.from_words(text)\n",
    "    most_common_collocations_bigram = collocations_bigram.nbest(nltk.collocations.BigramAssocMeasures.likelihood_ratio, 10)\n",
    "    print('أكثر 10 ثنائيات كلمات مهمة في النص:',most_common_collocations_bigram)\n",
    "    collocations_trigram = nltk.collocations.TrigramCollocationFinder.from_words(text)\n",
    "    most_common_collocations_trigram = collocations_trigram.nbest(nltk.collocations.TrigramAssocMeasures.likelihood_ratio, 10)\n",
    "    print('أكثر 10 ثلاثيات كلمات مهمة في النص:',most_common_collocations_trigram)  \n",
    "    print('========================================================================================================================================================================================')  \n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "# tokenized_text('tony botros is tony the tony best best best tony botros in in in the whole world tony botros')\n",
    "# tokenized_text(['طوني','طوني','بطرس','هو','طوني','هو','بطرس','ال','الأفضل'])\n",
    "# tokenized_text(['لدي', 'جرح', 'فوق', 'حاجبي', 'و', 'تمت', 'عملية', 'الخياطة', 'ثم', 'نزع', 'الغرزات', 'بشكل', 'عادي', 'لكن', 'مر', 'اسبوع', 'على', 'ذلك', 'و', 'مازال', 'هناك', 'الم', 'و', 'تطلب', 'شديد', 'في', 'المنطقة', 'لدرجة', 'اني', 'لا', 'استطيع', 'رفع', 'حاجبي', 'ابدا', 'إضافة', 'لوجود', 'انتفاخ', 'هل', 'هدا', 'طبيعي', 'ام', 'يمكن', 'ان', 'يكون', 'خلل', 'و', 'إن', 'كان', 'كذلك', 'ما', 'هو'])\n",
    "tokenized_text(answers[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "id": "8bfa567330bbb43b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-28T17:49:35.421653500Z",
     "start_time": "2023-11-28T17:49:35.392541500Z"
    }
   },
   "outputs": [],
   "source": [
    "#الشرح"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "id": "2e3f5f8258e84500",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-28T17:49:35.450007300Z",
     "start_time": "2023-11-28T17:49:35.403464800Z"
    }
   },
   "outputs": [],
   "source": [
    "# هاد التابع لمرق كل اختصاص وجيب منو الكلمات كلا متل مو مطلوب ب 2.4\n",
    "def all_tokinized_text(specialized):\n",
    "    filtered_questions = filter_text(train_data[train_data['label']==specialized]['question'].values)\n",
    "    filtered_answers = filter_text(train_data[train_data['label']==specialized]['answer'].values)\n",
    "    list = filtered_questions + filtered_answers\n",
    "    flattened_list = [item for sublist in list for item in sublist]\n",
    "    # flattened_list\n",
    "    tokenized_text(flattened_list)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "id": "d616630beb444368",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-28T17:49:41.895942500Z",
     "start_time": "2023-11-28T17:49:35.411488100Z"
    }
   },
   "outputs": [],
   "source": [
    "# all_tokinized_text('أمراض الدم')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "id": "2c5ef7cc8cf9dd59",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-28T17:49:56.166864500Z",
     "start_time": "2023-11-28T17:49:41.924108500Z"
    }
   },
   "outputs": [],
   "source": [
    "# all_tokinized_text('أمراض الجهاز التنفسي')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "id": "e861c909e1a23012",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-28T17:50:15.625987600Z",
     "start_time": "2023-11-28T17:49:56.158856Z"
    }
   },
   "outputs": [],
   "source": [
    "# all_tokinized_text('أمراض الغدد الصماء')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "id": "8a54f923b682dbb5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-28T17:50:25.137230300Z",
     "start_time": "2023-11-28T17:50:15.626987300Z"
    }
   },
   "outputs": [],
   "source": [
    "# all_tokinized_text('ارتفاع ضغط الدم')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "id": "4dc049d6662db815",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-28T17:50:47.456008500Z",
     "start_time": "2023-11-28T17:50:25.136228500Z"
    }
   },
   "outputs": [],
   "source": [
    "# all_tokinized_text('الأورام الخبيثة والحميدة')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "id": "a450181732413cb3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-28T17:50:54.892514800Z",
     "start_time": "2023-11-28T17:50:47.456008500Z"
    }
   },
   "outputs": [],
   "source": [
    "# all_tokinized_text('جراحة العظام')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "id": "4b5d01e09d1a8bff",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-28T17:51:15.432650500Z",
     "start_time": "2023-11-28T17:50:54.885006900Z"
    }
   },
   "outputs": [],
   "source": [
    "# all_tokinized_text('جراحة عامة')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "id": "5c2d128fa7313066",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-28T17:51:29.530449900Z",
     "start_time": "2023-11-28T17:51:15.410544600Z"
    }
   },
   "outputs": [],
   "source": [
    "# all_tokinized_text('مرض السكري')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "id": "c5a9bfb6e9d84e64",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-28T17:53:16.009343700Z",
     "start_time": "2023-11-28T17:51:29.526942900Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "أكثر 15 كلمة مكررة: [('من', 53731), ('في', 47548), ('الدكتور', 44651), ('طبية', 32856), ('اسئلة', 32618), ('و', 27315), ('جراحة', 20023), ('هل', 19845), ('الدم', 18946), ('على', 17807), ('امراض', 14809), ('عامة', 14630), ('لا', 13177), ('الغدد', 12601), ('مع', 12048)]\n",
      "أقل 10 كلمة مكررة: [('قسوى', 1), ('إجبار', 1), ('إتباعه', 1), ('ومسكنة', 1), ('والمتعبة', 1), ('للنتوءات', 1), ('خياراتها', 1), ('Spurs', 1), ('بافراازات', 1), ('لوتتسبب', 1)]\n",
      "أكثر 10 ثنائيات مكررة في النص: [(('اسئلة', 'طبية'), 32598), (('جراحة', 'عامة'), 14448), (('الغدد', 'الصماء'), 11130), (('طبية', 'امراض'), 10883), (('طبية', 'جراحة'), 8607), (('ضغط', 'الدم'), 6735), (('اعاني', 'من'), 6247), (('الاورام', 'الخبيثة'), 6106), (('الخبيثة', 'والحميدة'), 6091), (('طبية', 'الاورام'), 6089)]\n",
      "أكثر 10 ثلاثيات كلمات مكررة في النص: [(('اسئلة', 'طبية', 'امراض'), 10883), (('اسئلة', 'طبية', 'جراحة'), 8607), (('اسئلة', 'طبية', 'الاورام'), 6089), (('طبية', 'الاورام', 'الخبيثة'), 6089), (('الاورام', 'الخبيثة', 'والحميدة'), 6089), (('طبية', 'جراحة', 'عامة'), 6085), (('امراض', 'الغدد', 'الصماء'), 5614), (('طبية', 'امراض', 'الغدد'), 5607), (('الدكتور', 'باسم', 'مرقص'), 5414), (('الدكتور', 'يزن', 'علي'), 5296)]\n",
      "أكثر 10 ثنائيات كلمات مهمة في النص: [('اسئلة', 'طبية'), ('جراحة', 'عامة'), ('الغدد', 'الصماء'), ('الخبيثة', 'والحميدة'), ('طبية', 'امراض'), ('باسم', 'مرقص'), ('الاورام', 'الخبيثة'), ('طاقم', 'الطبي'), ('علي', 'خليف'), ('ضغط', 'الدم')]\n",
      "أكثر 10 ثلاثيات كلمات مهمة في النص: [('اسئلة', 'طبية', 'امراض'), ('اسئلة', 'طبية', 'جراحة'), ('اسئلة', 'طبية', 'الاورام'), ('اسئلة', 'طبية', 'مرض'), ('مرقص', 'اسئلة', 'طبية'), ('الطبي', 'اسئلة', 'طبية'), ('خليف', 'اسئلة', 'طبية'), ('اسئلة', 'طبية', 'ارتفاع'), ('العبدالله', 'اسئلة', 'طبية'), ('العواودة', 'اسئلة', 'طبية')]\n",
      "========================================================================================================================================================================================\n"
     ]
    }
   ],
   "source": [
    "# على كل الداتا تبع الترين:\n",
    "filtered_questions = filter_text(train_data['question'].values)\n",
    "filtered_answers = filter_text(train_data['answer'].values)\n",
    "list = filtered_questions + filtered_answers\n",
    "flattened_list = [item for sublist in list for item in sublist]\n",
    "flattened_list\n",
    "tokenized_text(flattened_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "id": "a47127af0446c39b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-28T17:53:16.015367500Z",
     "start_time": "2023-11-28T17:53:16.007830600Z"
    }
   },
   "outputs": [],
   "source": [
    "#الشرح"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9c557f09745b419",
   "metadata": {
    "id": "ZgzDVRqCQ72Q"
   },
   "source": [
    "## [2.5] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "id": "9c774bae3acab54c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-28T17:53:23.740569800Z",
     "start_time": "2023-11-28T17:53:16.012342800Z"
    }
   },
   "outputs": [],
   "source": [
    "train_data['all_word']=train_data['answer']+train_data['question']\n",
    "train_data_list=train_data['all_word'].values\n",
    "all_word = filter_text(train_data_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "id": "50d09f794f36668b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-28T17:53:23.849212500Z",
     "start_time": "2023-11-28T17:53:23.741569700Z"
    }
   },
   "outputs": [],
   "source": [
    "flattened_all_word = [item for sublist in all_word for item in sublist]\n",
    "train_data['length_of_sentinse']=train_data['all_word'].apply(len)\n",
    "# train_data['length_of_sentinse']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "id": "f9f823f086995cf3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-28T17:53:24.518763200Z",
     "start_time": "2023-11-28T17:53:23.851207500Z"
    }
   },
   "outputs": [],
   "source": [
    "w=[]\n",
    "for i in all_word:\n",
    "    w.append(set(i))\n",
    "train_data['unique_words']=w\n",
    "train_data['number_of_unique_words']=train_data['unique_words'].apply(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "id": "c743371b1d15adb6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-28T17:53:24.524456700Z",
     "start_time": "2023-11-28T17:53:24.521451100Z"
    },
    "id": "3WFp0g3DQ72V"
   },
   "outputs": [],
   "source": [
    "def histogram(length_of_sentinse,number_of_unique_words):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.subplot(2, 1, 1)\n",
    "    # plt.bar(number_of_unique_words, length_of_sentinse, color='blue', alpha=0.7)\n",
    "    plt.hist(length_of_sentinse, label='length_of_sentinse',edgecolor='black', linewidth=1.2,bins=30, alpha=0.5)\n",
    "    plt.hist(number_of_unique_words, label='number_of_unique_words',edgecolor='black', linewidth=1.2,bins=30, alpha=0.5)\n",
    "    plt.title('histogram')\n",
    "    plt.xlabel('length_of_sentinse')\n",
    "    plt.ylabel('number_of_unique_words')\n",
    "    # plt.tight_layout()\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "id": "bf894bf5f98822c4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-28T17:53:24.747773900Z",
     "start_time": "2023-11-28T17:53:24.525460100Z"
    }
   },
   "outputs": [],
   "source": [
    "# لكل النص\n",
    "# histogram(train_data['length_of_sentinse'],train_data['number_of_unique_words'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "id": "674223e30b687e8a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-28T17:53:24.966337700Z",
     "start_time": "2023-11-28T17:53:24.746176900Z"
    }
   },
   "outputs": [],
   "source": [
    "# # أمراض الجهاز التنفسي\n",
    "# histogram(train_data[train_data['label']=='أمراض الجهاز التنفسي']['length_of_sentinse'],train_data[train_data['label']=='أمراض الجهاز التنفسي']['number_of_unique_words'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "id": "3c448c8ccbd1775a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-28T17:53:25.179734200Z",
     "start_time": "2023-11-28T17:53:24.963334300Z"
    }
   },
   "outputs": [],
   "source": [
    "# # أمراض الدم\n",
    "# histogram(train_data[train_data['label']=='أمراض الدم']['length_of_sentinse'],train_data[train_data['label']=='أمراض الدم']['number_of_unique_words'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "id": "7fe7cc50508fb9b2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-28T17:53:25.398841600Z",
     "start_time": "2023-11-28T17:53:25.178733400Z"
    }
   },
   "outputs": [],
   "source": [
    "# # أمراض الغدد الصماء\n",
    "# histogram(train_data[train_data['label']=='أمراض الغدد الصماء']['length_of_sentinse'],train_data[train_data['label']=='أمراض الغدد الصماء']['number_of_unique_words'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "id": "9f90261cce6f277d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-28T17:53:25.625906200Z",
     "start_time": "2023-11-28T17:53:25.398841600Z"
    }
   },
   "outputs": [],
   "source": [
    "# # ارتفاع ضغط الدم\n",
    "# histogram(train_data[train_data['label']=='ارتفاع ضغط الدم']['length_of_sentinse'],train_data[train_data['label']=='ارتفاع ضغط الدم']['number_of_unique_words'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "id": "30d9687be40b9584",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-28T17:53:25.854342800Z",
     "start_time": "2023-11-28T17:53:25.629420Z"
    }
   },
   "outputs": [],
   "source": [
    "# # الأورام الخبيثة والحميدة\n",
    "# histogram(train_data[train_data['label']=='الأورام الخبيثة والحميدة']['length_of_sentinse'],train_data[train_data['label']=='الأورام الخبيثة والحميدة']['number_of_unique_words'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "id": "faa22e8858b487d7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-28T17:53:26.064133700Z",
     "start_time": "2023-11-28T17:53:25.853339600Z"
    }
   },
   "outputs": [],
   "source": [
    "# # جراحة العظام\n",
    "# histogram(train_data[train_data['label']=='جراحة العظام']['length_of_sentinse'],train_data[train_data['label']=='جراحة العظام']['number_of_unique_words'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "id": "84a6d01ad78172f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-28T17:53:26.289134Z",
     "start_time": "2023-11-28T17:53:26.063132100Z"
    }
   },
   "outputs": [],
   "source": [
    "# # جراحة عامة\n",
    "# histogram(train_data[train_data['label']=='جراحة عامة']['length_of_sentinse'],train_data[train_data['label']=='جراحة عامة']['number_of_unique_words'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "id": "a6c4311d2ebf74ab",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-28T17:53:26.517446500Z",
     "start_time": "2023-11-28T17:53:26.291149400Z"
    }
   },
   "outputs": [],
   "source": [
    "# # مرض السكري\n",
    "# histogram(train_data[train_data['label']=='مرض السكري']['length_of_sentinse'],train_data[train_data['label']=='مرض السكري']['number_of_unique_words'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1835943bcbf4693",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-28T17:53:26.519451500Z",
     "start_time": "2023-11-28T17:53:26.515428200Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "eaf1b8732c7316e1",
   "metadata": {
    "id": "oFmJ2HWqRNZa"
   },
   "source": [
    "# Question [3]: Text Cleaning and Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68fbb0f8741051d0",
   "metadata": {
    "id": "cwaCNfS9RNZc"
   },
   "source": [
    "## [3.1] \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "id": "274c297e31ec5bf",
   "metadata": {
    "id": "LhllhojDRNZd",
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "# train_data\n",
    "# clean_tweet = re.sub(r'^RT(\\s)+|https?\\S+|#|@\\S+', '', tweet) ## من أجل أزالت \n",
    "#r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+'\n",
    "def remove_links(text):\n",
    "    clean_text = re.sub(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', '', text)\n",
    "    return clean_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove_links('https://chat.openai.com/c/dc21a08a-5be6-42e3-a6e4-0c1d26791a46 hi')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c720cae3191f34c8",
   "metadata": {
    "id": "MbJeF_CxRNZd"
   },
   "source": [
    "## [3.2] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def remove_doctors_prefix(text):\n",
    "    pattern = r'\\b(?:الدكتور(?:ة)?)\\s+(?:الصيدلاني(?:ة)?)?\\s*(?:([\\u0600-\\u06FF]+)\\s*[\\u0600-\\u06FF]+\\s*([\\u0600-\\u06FF]+))\\b'\n",
    "\n",
    "    def replacer(match):\n",
    "        full_name = match.group(1)\n",
    "        first_name = full_name.split()[0]\n",
    "        return f'د.{first_name}'\n",
    "\n",
    "    result = re.sub(pattern, replacer, text)\n",
    "\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = train_data[train_data['label']=='أمراض الدم']['answer']\n",
    "# texts.apply(remove_doctors_prefix)[0]\n",
    "# for text in texts[:5]:\n",
    "#     # print(text)\n",
    "#     print(remove_doctors_prefix(text))\n",
    "#     print(\"=================================================================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "774c28db38969b02",
   "metadata": {
    "id": "GY-HhD2YRNZe"
   },
   "source": [
    "## [3.3] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "id": "11b9f861c5d67e1b",
   "metadata": {
    "id": "l-5TSuHWRNZe"
   },
   "outputs": [],
   "source": [
    "def remove_sentences_after_doctor(text):\n",
    "    result = re.sub(r'(الدكتور[^\\n]*\\n|د\\.[^\\n]*\\n)([^\\n]*\\n){0,6}', r'\\1', text)\n",
    "    return result\n",
    "\n",
    "# for text in texts[:5]:\n",
    "#     print(remove_sentences_after_doctor(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad580f71f7101173",
   "metadata": {
    "id": "KV9D86kbRNZe"
   },
   "source": [
    "## [3.4] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "id": "ffdb6de003d15333",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_duplicate_letters_except_alif(text):\n",
    "    pattern = re.compile(r'(?:(?<!ا)ا(?!ا)|(.)(?:\\1)+)', re.UNICODE) # بشلي كلشي أحرف مكررة وخاصة الألف مابكررها اكتر من مرة\n",
    "    result = pattern.sub(lambda x: x.group(1) if x.group(1) else 'ا', text)\n",
    "    return result\n",
    "\n",
    "# for text in texts[:5]:\n",
    "#     print(remove_duplicate_letters_except_alif(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "978ddb92b76b7f1f",
   "metadata": {},
   "source": [
    "## [3.5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1041d0eeb848aab",
   "metadata": {
    "id": "3fM_FPldSCxp"
   },
   "source": [
    "### [3.5.1] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [],
   "source": [
    "# هاد بيقلب لل 1و2و3و4و....\n",
    "def convert_numbers_to_arabic_v2(text):\n",
    "    digit_mapping = {'٠': '0', '١': '1', '٢': '2', '٣': '3', '٤': '4', '٥': '5', '٦': '6', '٧': '7', '٨': '8', '٩': '9'}\n",
    "    english_text = re.sub(r'[٠-٩]', lambda x: digit_mapping[x.group()], text)\n",
    "\n",
    "    return english_text\n",
    "\n",
    "\n",
    "# for text in texts[:5]:\n",
    "#     print(convert_numbers_to_arabic_v2(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "id": "59d7bd3624496465",
   "metadata": {
    "id": "ohGu5R14RNZe"
   },
   "outputs": [],
   "source": [
    "# هاد بيلقلب لل ١,٢,٣,٤\n",
    "def convert_numbers_to_arabic(text):\n",
    "    digit_mapping = {'0': '٠', '1': '١', '2': '٢', '3': '٣', '4': '٤', '5': '٥', '6': '٦', '7': '٧', '8': '٨', '9': '٩'}\n",
    "    arabic_text = re.sub(r'[0-9]', lambda x: digit_mapping[x.group()], text)\n",
    "\n",
    "    return arabic_text\n",
    "\n",
    "\n",
    "# for text in texts[:5]:\n",
    "#     print(convert_numbers_to_arabic(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9efb2c08d7a0eae5",
   "metadata": {
    "id": "N2z1KmQXSAFs"
   },
   "source": [
    "### [3.5.2] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "id": "584a485a3ab32cff",
   "metadata": {
    "id": "fmOXIMWoSAFt"
   },
   "outputs": [],
   "source": [
    "def standardize_numbers(text, replacement_symbol='T'):\n",
    "    digit_pattern = r'[0-9٠-٩]+'\n",
    "    standardized_text = re.sub(digit_pattern, lambda x: replacement_symbol, text)\n",
    "\n",
    "    return standardized_text\n",
    "\n",
    "\n",
    "# for text in texts[:5]:\n",
    "#     print(standardize_numbers(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a45d054195bd5cc",
   "metadata": {
    "id": "fCBHc2y6SAL5"
   },
   "source": [
    "### [3.5.3] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "id": "db36eb4fffdef9e7",
   "metadata": {
    "id": "QJX_PtSzSAL6"
   },
   "outputs": [],
   "source": [
    "def remove_numbers(text):\n",
    "    digit_pattern = r'\\d+'\n",
    "    \n",
    "    text_without_numbers = re.sub(digit_pattern, '', text)\n",
    "\n",
    "    return text_without_numbers\n",
    "\n",
    "# for text in texts[:5]:\n",
    "#     print(remove_numbers(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "124e337a2e9808a9",
   "metadata": {},
   "source": [
    "### [3.5.4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "id": "4d298a9526a3ae7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = train_data[train_data['label']=='أمراض الجهاز التنفسي']['answer'].values\n",
    "\n",
    "def remove_dates_and_times(text):\n",
    "    date_time_patterns = [\n",
    "        r'\\d{4}-\\d{2}-\\d{2}\\s+\\d{1,2}:\\d{2}:\\d{2}'  # Match dates and times like 2015-01-04 20:08:51\n",
    "        # r'\\d{1,2}/\\d{1,2}/\\d{2,4}',         # Match dates like 12/31/2022\n",
    "        # r'\\d{1,2}-\\d{1,2}-\\d{2,4}',         # Match dates like 12-31-2022\n",
    "        # r'\\d{1,2}:\\d{2}:\\d{2}'               # Match times like 12:34:56\n",
    "        # r'\\d{1,2}\\s+(?:Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)\\s*\\d{2,4}',  # Match dates like 31 Dec 2022\n",
    "        # r'\\d{1,2}:\\d{2}\\s*(?:AM|PM|am|pm)?',  # Match times like 12:34 AM\n",
    "    ]\n",
    "\n",
    "    for pattern in date_time_patterns:\n",
    "        text = re.sub(pattern, '', text)\n",
    "\n",
    "    return text\n",
    "\n",
    "# for text in texts[:5]:\n",
    "#     print(remove_dates_and_times(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c897aae5bc65e80",
   "metadata": {
    "id": "P1aLjPbzRNZe"
   },
   "source": [
    "## [3.6] "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c7db1ab76a0c0f8",
   "metadata": {
    "id": "dbqN7BceSRhS"
   },
   "source": [
    "### [3.6.1] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 318,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string\n",
    "string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "id": "8e6d37a9e0c0365",
   "metadata": {
    "id": "caLTQ8EtSRhT"
   },
   "outputs": [],
   "source": [
    "def remove_punctuation(text):\n",
    "    text=re.sub(r'[!\\\"#$%&\\'()*+,\\-./:;<=>?@\\[\\\\\\]^_`{|}~،؛؟ـ]',' ',text)\n",
    "    return text\n",
    "\n",
    "# for text in texts[:5]:\n",
    "#     print(remove_punctuation(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aa2beec5d82e166",
   "metadata": {
    "id": "rad8wPeSSRhU"
   },
   "source": [
    "### [3.6.2] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "id": "426725eac6677209",
   "metadata": {
    "id": "UZuYtSHaSRhU"
   },
   "outputs": [],
   "source": [
    "def keep_arabic_punctuations_numbers(text):\n",
    "    text= re.sub(r'[^\\u0600-\\u06FF0-9،؛؟ـ!\\\"#$%&\\'()*+,\\-./:;<=>?@\\[\\\\\\]^_`{|}~]', ' ', text)\n",
    "    return text\n",
    "\n",
    "# for text in texts[:5]:\n",
    "#     print(keep_arabic_punctuations_numbers(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f48de648b46f28a6",
   "metadata": {
    "id": "N4WDktTgRNZf"
   },
   "source": [
    "## [3.7] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "id": "94667dc98883d735",
   "metadata": {
    "id": "B6icsYsFRNZf"
   },
   "outputs": [],
   "source": [
    "stop_words=set(stop_words.classed_stopwords_list())\n",
    "Some_stop_words_we_need=[\"قبلما\",\"بعدما\",\"فقط\",\"مازال\",\"ليست\",\"ليس\",\"ليسا\",\"لسنا\",\n",
    "                         \"لسن\",\"لازالت\",\"مساء\",\"صباح\",\"قبل\",\"بعد\",\"إياك\",\"إياكن\",\n",
    "                         \"إياكما\",\"إياكم\",\"لم\",\"عدا\",\"إلا\",\"د\",\"كلا\",\"عامة\",\"لا\",\n",
    "                         \"حبذا\",\"أقل\",\"أكثر\"]\n",
    "Some_stop_words_dont_we_need=['عليكم','السلام','انا','أرجو','الرد','المزيد','إقرأ']\n",
    "for word in Some_stop_words_we_need:\n",
    "    if word in stop_words:\n",
    "        stop_words.remove(word)\n",
    "\n",
    "for word in Some_stop_words_we_need:\n",
    "    if word is not stop_words:\n",
    "        stop_words.add(word)\n",
    "\n",
    "def remove_stop_words(text):\n",
    "    text =re.sub(\"[\\_\\-\\/]\",\" \",text)\n",
    "    text=word_tokenize(text)\n",
    "    text=[w for w in text if not w in stop_words]\n",
    "    return text\n",
    "\n",
    "# for text in texts[:5]:\n",
    "#     print(remove_stop_words(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ملاحــــــــــــــــــظه\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46805849c2d2623b",
   "metadata": {
    "id": "wbGPWNtoR4p-"
   },
   "source": [
    "## [3.8] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "id": "6cf48a4d57cf455d",
   "metadata": {
    "id": "fAiyofe3R4qA"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'سمي'"
      ]
     },
     "execution_count": 322,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ar_stemmer=stemmer(\"arabic\")\n",
    "ar_stemmer.stemWord(u\"فسميتموها\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "كتاب في مكتبة متنوع مفيد\n"
     ]
    }
   ],
   "source": [
    "def stem_arabic_words_farasa(text):\n",
    "    stemmer = FarasaStemmer()\n",
    "    text=stemmer.stem(text)\n",
    "    return text\n",
    "\n",
    "text = \"الكتب في المكتبة متنوعة ومفيدة\"\n",
    "stemmed_text = stem_arabic_words_farasa(text)\n",
    "print(stemmed_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def stem_arabic_words_tashaphyne(text):\n",
    "#     stemmer = ArabicLightStemmer()\n",
    "#     words = text.split()\n",
    "#     stemmed_words = [stemmer.light_stem(word) for word in words]\n",
    "#     return ' '.join(stemmed_words)\n",
    "\n",
    "# text = \"الكتب في المكتبة متنوعة ومفيدة\"\n",
    "# stemmed_text = stem_arabic_words_tashaphyne(text)\n",
    "# print(stemmed_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "كتب في مكتب متنوع مفيد\n"
     ]
    }
   ],
   "source": [
    "def stem_arabic_words_snowballstemmer(text):\n",
    "    ar_stemmer = stemmer(\"arabic\")\n",
    "    words = text.split()\n",
    "    stemmed_words = [ar_stemmer.stemWord(word) for word in words]\n",
    "    return ' '.join(stemmed_words)\n",
    "\n",
    "text = \"الكتب في المكتبة متنوعة ومفيدة\"\n",
    "stemmed_text = stem_arabic_words_snowballstemmer(text)\n",
    "print(stemmed_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "راجع طبيب من اجر جراحه افضل من يجيب لان شاهد حاله عل وضع اول وهو من قام لفعل تمنيات لشفاء 0 2023-06-17 05:56:49 دكتور حسن ابراهيم خليف /اسيلة-طبية/جراحة-عامة/عملت-عملية-دوالي-الساقين-قسطرة-الليزر-من-شهر-وعندي-1715059 دكتور حسن ابراهيم خليف جراح عام\n",
      "لا يوحد مشكل مشي. اكثار من شرب سوايل ومم استخدام ملين والمغاطس دافء مع ملح 1 2018-03-13 08:07:00 دكتور اسام عبدالله /اسيلة-طبية/جراحة-عامة/عملت-عن-البواسير-شهر-وعانيت-خلال-التبرز-كثيرا-حدث-لي-1113406 دكتور اسام عبدالله جراح عام\n",
      "اوافق علي را د/ محمدالشيخ و ايض ارجو مراجع طبيب باطن لضبط سكر بالدم 0 2022-08-13 10:24:38 د. سوزان حسن /اسيلة-طبية/الاورام-الخبيثة-والحميدة/حدث-معي-قبل-او-شيء-ك-تنميل-في-الوجه-وفي-اليد-وبعده-1610184 د. سوزان حسن اورام حميد و خبيث\n",
      "بعد تمام بلوغ لايحدث نمو في طول 1 2015-08-29 20:56:25 دكتور احمد اسام عبدالغني صقر /اسيلة-طبية/جراحة-العظام/انا-بنت-و-لدي-مشكلة-هي-انني-قصيرة-القامة-من-الطول-سم-هل-525552 دكتور احمد اسام عبدالغني صقر جراح عظام والمفاصل\n",
      "ليس خطير لكن يتوافق مع نقص في يتام ب 12 0 2016-04-12 08:18:38 دكتور اسماعيل غزال /اسيلة-طبية/جراحة-العظام/من-شهر-اعاني-تنميل-في-اطرافي-و-من-يومين-اعاني-من-حرارة-734578 دكتور اسماعيل غزال جراح عظام والمفاصل\n"
     ]
    }
   ],
   "source": [
    "for text in texts[:5]:\n",
    "    print(stem_arabic_words_snowballstemmer(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for text in texts[:5]:\n",
    "#     print(stem_arabic_words_tashaphyne(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "راجع طبيب من أجري جرح أفضل من أجاب لان شاهد حال علي وضع أول هو من قام فعل تمني بلشفاء\n",
      "0\n",
      "2023-06-17 05 : 56 : 49\n",
      "\n",
      "\n",
      "دكتور حسن إبراهيم خليفة\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "/ سؤال - طبي / جراحة - عام / عمل - عملية - دالية - ساق - قسطرة - ليزر - من - شهر - عند - 1715059\n",
      "\n",
      "دكتور حسن إبراهيم خليفة\n",
      "\n",
      "\n",
      "جراحة عام\n",
      "لا وحد مشكلة مشي . اكثار من شرب سائل ممكن استخدام ملين مغاطس دافئ مع ملح\n",
      "1\n",
      "2018-03-13 08 : 07 : 00\n",
      "\n",
      "\n",
      "دكتور أسامة عبدالله\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "/ سؤال - طبي / جراحة - عام / عمل - عن - باسور - شهر - عانى - خلال - برز - كثير - حدث - ل - 1113406\n",
      "\n",
      "دكتور أسامة عبدالله\n",
      "\n",
      "\n",
      "جراحة عام\n",
      "أوافق على راي د / محمدالشيخ و ايض أرجو مراجعة طبيب باطني ضبط سكر دم  \n",
      "0\n",
      "2022-08-13 10 : 24 : 38\n",
      "\n",
      "\n",
      "د . سوزان حسنين\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "/ سؤال - طبي / اورام - خبيث - حميدة / حدث - مع - قبل - أو - شيء - ك - تنميل - في - وجه - في - يد - بعد - 1610184\n",
      "\n",
      "د . سوزان حسنين\n",
      "\n",
      "\n",
      "اورام حميدة و خبيث\n",
      "بعد تمام بلوغ لايحدث نمو في طول\n",
      "1\n",
      "2015-08-29 20 : 56 : 25\n",
      "\n",
      "\n",
      "دكتور أحمد أسامة عبدالغنى صقر\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "/ سؤال - طبي / جراحة - عظمة / أنا - بنت - و - لدى - مشكلة - هي - انني - قصير - قامة - من - طول - سم - هل - 525552\n",
      "\n",
      "دكتور أحمد أسامة عبدالغنى صقر\n",
      "\n",
      "\n",
      "جراحة عظمة مفصل\n",
      "ليس خطير لكن توافق مع نقص في فيتامين ب 12\n",
      "0\n",
      "2016-04-12 08 : 18 : 38\n",
      "\n",
      "\n",
      "دكتور إسماعيل غزال\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "/ سؤال - طبي / جراحة - عظمة / من - شهر - اعاني - تنميل - في - اطراف - و - من - يوم - اعاني - من - حرارة - 734578\n",
      "\n",
      "دكتور إسماعيل غزال\n",
      "\n",
      "\n",
      "جراحة عظمة مفصل\n"
     ]
    }
   ],
   "source": [
    "# for text in texts[:5]:\n",
    "#     print(stem_arabic_words_farasa(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80016c32a6453254",
   "metadata": {
    "id": "V-rCzzoMR41T"
   },
   "source": [
    "## [3.9] "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43f9be9fd522e36a",
   "metadata": {
    "id": "iXF2L4Z-Sd3N"
   },
   "source": [
    "### [3.9.1] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "id": "d09fdfc13c8e9d32",
   "metadata": {
    "id": "wk-Z3Gw4Sd3O"
   },
   "outputs": [],
   "source": [
    "def unify_hamzat(text):\n",
    "    unified_text = re.sub(r'[ءؤئ]', 'ء', text)\n",
    "    return unified_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "أحمد قرأ القرآن بتأنٍ وإتقان\n"
     ]
    }
   ],
   "source": [
    "text = \"أحمد قرأ القرآن بتأنٍ وإتقان\"\n",
    "unified_text = unify_hamzat(text)\n",
    "print(unified_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "952d14ba938901a4",
   "metadata": {
    "id": "ij5gdJA4Sd3O"
   },
   "source": [
    "### [3.9.2] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "id": "64925506e5dfc1d4",
   "metadata": {
    "id": "bDLOsd4tSd3O"
   },
   "outputs": [],
   "source": [
    "def unify_Alfat(text):\n",
    "    unified_text = re.sub(r'[أإآ]', 'أ', text)\n",
    "    return unified_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "أحمد قرأ القرأن بتأنٍ وأتقان\n"
     ]
    }
   ],
   "source": [
    "text = \"أحمد قرأ القرآن بتأنٍ وإتقان\"\n",
    "unified_text = unify_Alfat(text)\n",
    "print(unified_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6914083b35745e6",
   "metadata": {
    "id": "H6k8JnJDSd3P"
   },
   "source": [
    "### [3.9.3] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "id": "ac11de1bf61d2af7",
   "metadata": {
    "id": "JhSOCYLmSd3P"
   },
   "outputs": [],
   "source": [
    "def remove_tatweel(text):\n",
    "    text =re.sub( r'ـ', '', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "اللغة العربية جميلة\n"
     ]
    }
   ],
   "source": [
    "text = \"اللغة العربيــــــة جميلة\"\n",
    "clean_text = remove_tatweel(text)\n",
    "print(clean_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9eeb1a5672b59b",
   "metadata": {
    "id": "1TmDb343Sls9"
   },
   "source": [
    "### [3.9.4] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "id": "5c81a4e0fcae8662",
   "metadata": {
    "id": "wWJ-Z11_Sls-"
   },
   "outputs": [],
   "source": [
    "def remove_arabic_diacritics(text):\n",
    "    text=re.sub(r'[\\u064B-\\u065F]', '', text)\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for text in texts[:5]:\n",
    "#     print(remove_arabic_diacritics(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b18e9774ff7851a",
   "metadata": {
    "id": "0V5LD9EWVgY7"
   },
   "source": [
    "## [3.10] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "id": "f2e4214581691a7b",
   "metadata": {
    "id": "AkVzG3AWVgZA"
   },
   "outputs": [],
   "source": [
    "def remove_space(text):\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8d8811ac8263334",
   "metadata": {
    "id": "exdhuH6GtLez"
   },
   "source": [
    "## Extra [3.11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c02431e73710b9c",
   "metadata": {
    "id": "_UOzIn6-tKlJ"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "22792fd6fb27623",
   "metadata": {
    "id": "Jp-C0ZwrS8AY"
   },
   "source": [
    "# Question [4]: Prepare Training Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10dd4630ce82793b",
   "metadata": {
    "id": "pwtVy8DES8AZ"
   },
   "source": [
    "## [4.1] \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "id": "548f25e826fab2dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\r\\nما هي مميزات و عيوب الدواء جلوكوفانس 500 5 و ايضا الأنسولين مكس تارد 30 \\r\\n\\r\\nلكل علاج ايجابيته وسلبياته والتي تعتمد على حالة المريض فما قد يناسب احدهم قد لا يناسب الاخر وهذا يحدده الطبيب خلال مراجعة المريض له\\r\\n1\\r\\n2015-01-04 20:08:51\\r\\n\\r\\n\\r\\nالدكتور يزن علي خليف\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n/اسئلة-طبية/مرض-السكري/ما-هي-مميزات-و-عيوب-الدواء-جلوكوفانس-و-ايضا-الانسولين-351157\\r\\n\\r\\nالدكتور يزن علي خليف \\r\\n\\r\\n\\r\\nالغدد الصماء \\r\\n\\r\\n\\r\\n',\n",
       " '\\r\\nاليك نتيجة تحليل هرمونات الغدة الدرقية علما بانه تم استأصال الغدة منذ اكثر من سنتينTT3=163TT4=12.6TSH=.01 هل مطلوب تعديل الجرعة \\r\\n\\r\\nنعم. يجب تخفيض الجرعة، الا اذا كان سبب استئصال الغدة هو سرطان الغدة\\r\\n0\\r\\n2017-02-13 06:34:19\\r\\n\\r\\n\\r\\nالدكتور باسم مرقص\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n/اسئلة-طبية/امراض-الغدد-الصماء/اليك-نتيجة-تحليل-هرمونات-الغدة-الدرقية-علما-بانه-تم-909154\\r\\n\\r\\nالدكتور باسم مرقص \\r\\n\\r\\n\\r\\nجراحة عامة \\r\\n\\r\\n\\r\\n',\n",
       " '\\r\\nحلول منزلية لأعراض ارتفاع ضغط الدم \\r\\n\\r\\nيفضل عدم الاستغناء عن العلاج الدوائي لمرضى الضغط لكن يعد النظام الغذائي و الحركي اليومي للمريض جزء مهم و أساسي في الحفاظ على ضغط المريض ضمن الحدود الطبيعية.\\r\\nينصح ممارسة دورية للرياضة و المشي و محاولة تخفيف الوزن، التخفيف من المنبهات بأنواعها و محاولة الإقلاع عن التدخين.\\r\\nتناول الأطعمة الغنية بالبوتاسيوم كالموز و تجنب الأطعمة التي تحتوي على أملاح لتأثيرها المباشر على الضغط، و تناول المغذيات مثل مستخلص الثوم و زيوت السمك.\\r\\nتجنّب التوتر و المحافظة على نظام يومي هادئ و المتابعة الدورية لقراءات الضغط.\\r\\n3\\r\\n2014-12-09 18:05:33\\r\\n\\r\\n\\r\\nالدكتور انور سالم العواودة\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n/اسئلة-طبية/ارتفاع-ضغط-الدم/حلول-منزلية-لاعراض-ارتفاع-ضغط-الدم-342609\\r\\n\\r\\nالدكتور انور سالم العواودة \\r\\n\\r\\n\\r\\nالقلب والاوعية الدموية \\r\\n\\r\\n\\r\\n',\n",
       " '\\r\\nعملت عملية دوالي الساقين قسطرة الليزر من شهر وعندي الم من اسفل ساق الى اعلى ساق و برودة اطراف القدم وحرقان وخز و امكان يابسة وحكة و طيبة عليا هل هدا طبيعي احس في سكين تقطع \\r\\n\\r\\nراجع طبيبك من اجري الجراحه افضل من يجيب لانه شاهد الحاله علي وضعها الاول وهوا من قام بلفعل تمنياتي بلشفاء\\r\\n0\\r\\n2023-06-17 05:56:49\\r\\n\\r\\n\\r\\nالدكتور حسن ابراهيم خليفة\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n/اسئلة-طبية/جراحة-عامة/عملت-عملية-دوالي-الساقين-قسطرة-الليزر-من-شهر-وعندي-1715059\\r\\n\\r\\nالدكتور حسن ابراهيم خليفة \\r\\n\\r\\n\\r\\nجراحة عامة \\r\\n\\r\\n\\r\\n',\n",
       " '\\r\\nما حقيقة ان تمرين العضلة النعلية يخفض السكر بالدم؟؟ \\r\\n\\r\\nإذا قصدت تدليك العضلة فهذا كلام غير صحيح . ولكن ممارسة الرياضة مثل المشي يعني حركة العضلات والمساهمة في حرق السكر\\r\\n0\\r\\n2023-06-03 07:52:03\\r\\n\\r\\n\\r\\nالدكتور اغيد محمد بيريص\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n/اسئلة-طبية/مرض-السكري/ما-حقيقة-ان-تمرين-العضلة-النعلية-يخفض-السكر-بالدم-1709936\\r\\n\\r\\nالدكتور اغيد محمد بيريص \\r\\n\\r\\n\\r\\nالغدد الصماء \\r\\n\\r\\n\\r\\n']"
      ]
     },
     "execution_count": 338,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def preparing_training_data(text):\n",
    "    text_after_cleaning_links = remove_links(text)\n",
    "    text_after_removing_doctors_prefix = remove_doctors_prefix(text_after_cleaning_links)\n",
    "    text_after_removing_specialization = remove_sentences_after_doctor(text_after_removing_doctors_prefix)\n",
    "    text_after_removing_doublication_chars = remove_duplicate_letters_except_alif(text_after_removing_specialization)\n",
    "    text_after_removing_date_and_time = remove_dates_and_times(text_after_removing_doublication_chars)\n",
    "    text_after_removing_numbers = remove_numbers(text_after_removing_date_and_time)\n",
    "    text_after_removing_non_arabic_words = keep_arabic_punctuations_numbers(text_after_removing_numbers)\n",
    "    # text after removing stop words\n",
    "    text_after_unify_hamzat = unify_hamzat(text_after_removing_non_arabic_words)\n",
    "    text_after_unify_alafat = unify_Alfat(text_after_unify_hamzat)\n",
    "    text_after_removing_tatwel = remove_tatweel(text_after_unify_alafat)\n",
    "    text_after_removing_tashkel = remove_arabic_diacritics(text_after_removing_tatwel)\n",
    "    text_after_removing_spaces_and_keeping_one_space = remove_space(text_after_removing_tashkel)\n",
    "    # text after removing wrong arabic words and correct them\n",
    "\n",
    "\n",
    "    return text_after_removing_spaces_and_keeping_one_space\n",
    "    \n",
    "    \n",
    "    \n",
    "questions = train_data['question'].values\n",
    "answers = train_data['answer'].values\n",
    "\n",
    "train_data['cleann_questions']=questions\n",
    "train_data['cleann_answer']=answers\n",
    "questions_answers = questions[0:5] + answers[0:5]\n",
    "\n",
    "questions_answers.tolist()\n",
    "# for i in questions_answers:\n",
    "#     print(preparing_training_data(i))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77be5ba87989eb2b",
   "metadata": {
    "id": "npObP2JLUBgw"
   },
   "source": [
    "## [4.2] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "id": "b945acaab7ef7888",
   "metadata": {
    "id": "V_KxmQdgUBg5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1924068\n"
     ]
    }
   ],
   "source": [
    "all_train_data = questions + answers\n",
    "all_train_data_After_cleaning = []\n",
    "all_tokens = []\n",
    "for line in all_train_data:\n",
    "    training_data_after_cleaning = preparing_training_data(line)\n",
    "    all_train_data_After_cleaning.append(training_data_after_cleaning)\n",
    "    # print(training_data_after_cleaning)\n",
    "\n",
    "\n",
    "# all_train_data_After_cleaning\n",
    "# # print(tokens)\n",
    "for phrase in all_train_data_After_cleaning:\n",
    "    tokens = nltk.word_tokenize(phrase)\n",
    "    all_tokens.append(tokens)   \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "flattened_all_tokens = [item for sublist in all_tokens for item in sublist]\n",
    "print(len(flattened_all_tokens)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "عدد الكلمات الفريدة: 126989\n"
     ]
    }
   ],
   "source": [
    "# حساب عدد الكلمات الفريدة :\n",
    "total_unique_words=len(set(flattened_all_tokens))\n",
    "print(\"عدد الكلمات الفريدة:\", total_unique_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [],
   "source": [
    "# نلاحظ ان عدد الكلمات الكلية قد قلت هنا أما عدد الكلمات المميزة قد زاد"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9545f003ed979ad",
   "metadata": {
    "id": "pTIrTECWUBg5"
   },
   "source": [
    "## [4.3] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "id": "e8358e6f447495af",
   "metadata": {
    "id": "OqmCKe5lUBg6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32598\n",
      "32598\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df = pd.DataFrame(all_train_data_After_cleaning)\n",
    "\n",
    "df = df.map(lambda x: '' if isinstance(x, (float, int)) else x.strip() if isinstance(x, str) and len(x) >= 5 else None)\n",
    "\n",
    "df = df.dropna()\n",
    "print(len(df))\n",
    "print(len(all_train_data_After_cleaning))\n",
    "# all_train_data_After_cleaning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43333415d395ec67",
   "metadata": {},
   "source": [
    "# Question [5]: Baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3f3f370c22073bd",
   "metadata": {},
   "source": [
    "The Comparison Dictionary example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "id": "779a44f1fdc3be2",
   "metadata": {},
   "outputs": [],
   "source": [
    "comparison_table = {}\n",
    "\n",
    "comparison_table['question_step_number'] = []\n",
    "comparison_table['model_name'] = []\n",
    "comparison_table['features'] = []\n",
    "comparison_table['model_parameters'] = []\n",
    "comparison_table['preprocessing_methods'] = []\n",
    "comparison_table['accuracy'] = []\n",
    "comparison_table['balance_accuracy'] = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "100516edd49d694f",
   "metadata": {},
   "source": [
    "Filling the dictionary example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "id": "7af1fa16c41d67ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# comparison_table['question_step_number'].append(\"3.5\")\n",
    "# comparison_table['model_name'].append('logistic_regression')\n",
    "# comparison_table['features'].append('bag of words')\n",
    "# comparison_table['model_parameters'].append('default')\n",
    "# comparison_table['preprocessing_methods'].append(\"remove links\")\n",
    "# comparison_table['accuracy'].append(accuracy)\n",
    "# comparison_table['balance_accuracy'].append(balance_accuracy_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ea60039714ed596",
   "metadata": {},
   "source": [
    "## [5.1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "id": "32f1bc127a2a5794",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = train_data[['question', 'answer']], train_data['label']\n",
    "X_test, y_test = test_data[['question', 'answer']], test_data['label']\n",
    "X_validation, y_validation = validation_data[['question', 'answer']], validation_data['label']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "237204d8bc60265f",
   "metadata": {},
   "source": [
    "## [5.2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_combined = X_train['question'] + ' ' + X_train['answer']\n",
    "X_test_combined = X_test['question'] + ' ' + X_test['answer']\n",
    "X_validation_combined = X_validation['question'] + ' ' + X_validation['answer']\n",
    "\n",
    "\n",
    "def logistic_regression(x_train,x_test,x_val):\n",
    "\n",
    "    vectorizer = CountVectorizer()\n",
    "\n",
    "    X_train_bow = vectorizer.fit_transform(x_train)\n",
    "    X_test_bow = vectorizer.transform(x_test)\n",
    "    X_validation_bow = vectorizer.transform(x_val)\n",
    "\n",
    "    logreg_model = LogisticRegression(max_iter=1000)\n",
    "\n",
    "    logreg_model.fit(X_train_bow, y_train)\n",
    "\n",
    "    y_test_pred = logreg_model.predict(X_test_bow)\n",
    "    y_validation_pred = logreg_model.predict(X_validation_bow)\n",
    "\n",
    "    accuracy_test = accuracy_score(y_test, y_test_pred)\n",
    "    accuracy_validation = accuracy_score(y_validation, y_validation_pred)\n",
    "    balanced_accuracy_test = balanced_accuracy_score(y_test, y_test_pred)\n",
    "\n",
    "\n",
    "    print(f\"Test Accuracy: {accuracy_test}\")\n",
    "    print(f\"Validation Accuracy: {accuracy_validation}\")\n",
    "    print(f\"Balanced Accuracy: {balanced_accuracy_test}\")\n",
    "\n",
    "\n",
    "    return accuracy_test, accuracy_validation, balanced_accuracy_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.6437688353128878\n",
      "Validation Accuracy: 0.6507362666431532\n",
      "Balanced Accuracy: 0.6249945192325319\n"
     ]
    }
   ],
   "source": [
    "accuracy_test, accuracy_validation, balanced_accuracy_test = logistic_regression(X_train_combined,X_test_combined,X_validation_combined)\n",
    "\n",
    "comparison_table['question_step_number'].append(\"5.2\")\n",
    "comparison_table['model_name'].append('logistic_regression')\n",
    "comparison_table['features'].append('bag of words')\n",
    "comparison_table['model_parameters'].append('default')\n",
    "comparison_table['preprocessing_methods'].append(\"nothing\")\n",
    "comparison_table['accuracy'].append(accuracy_test)\n",
    "comparison_table['balance_accuracy'].append(balanced_accuracy_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "ملاحظااااااااااااااااااااااااااااااااااات\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "نلاحظ ان ال test accuracy دقتو أكثر من ال balanced accuracy بنسبة بسيطة\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8b0b98768fcd170",
   "metadata": {},
   "source": [
    "## [5.3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "id": "26a46fbbad90de4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32598\n",
      "Test Accuracy: 0.6434142882467647\n",
      "Validation Accuracy: 0.6510889692266996\n",
      "Balanced Accuracy: 0.6247697063643611\n"
     ]
    }
   ],
   "source": [
    "# تنظيف النص من الروابط\n",
    "\n",
    "X_train_combined = X_train['question'].apply(remove_links) + ' ' + X_train['answer'].apply(remove_links)\n",
    "\n",
    "\n",
    "# print(len(X_train_without_links))\n",
    "print(len(X_train_combined))\n",
    "\n",
    "accuracy_test, accuracy_validation, balanced_accuracy_test = logistic_regression(X_train_combined,X_test_combined,X_validation_combined)\n",
    "\n",
    "comparison_table['question_step_number'].append(\"5.3\")\n",
    "comparison_table['model_name'].append('logistic_regression')\n",
    "comparison_table['features'].append('bag of words')\n",
    "comparison_table['model_parameters'].append('default')\n",
    "comparison_table['preprocessing_methods'].append(\"Removing Links\")\n",
    "comparison_table['accuracy'].append(accuracy_test)\n",
    "comparison_table['balance_accuracy'].append(balanced_accuracy_test)\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32598\n",
      "Test Accuracy: 0.639691544052473\n",
      "Validation Accuracy: 0.6475619433912353\n",
      "Balanced Accuracy: 0.6181974031602561\n"
     ]
    }
   ],
   "source": [
    "# حذف اسماء الأطباء مع اللقب\n",
    "X_train_combined = X_train['question'].apply(remove_doctors_prefix) + ' ' + X_train['answer'].apply(remove_doctors_prefix)\n",
    "\n",
    "\n",
    "# print(len(X_train_without_links))\n",
    "print(len(X_train_combined))\n",
    "\n",
    "accuracy_test, accuracy_validation, balanced_accuracy_test = logistic_regression(X_train_combined,X_test_combined,X_validation_combined)\n",
    "\n",
    "comparison_table['question_step_number'].append(\"5.3\")\n",
    "comparison_table['model_name'].append('logistic_regression')\n",
    "comparison_table['features'].append('bag of words')\n",
    "comparison_table['model_parameters'].append('default')\n",
    "comparison_table['preprocessing_methods'].append(\"ٌRemoving doctors prefix\")\n",
    "comparison_table['accuracy'].append(accuracy_test)\n",
    "comparison_table['balance_accuracy'].append(balanced_accuracy_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32598\n",
      "Test Accuracy: 0.6353483424924659\n",
      "Validation Accuracy: 0.640243364782647\n",
      "Balanced Accuracy: 0.6191241254584431\n"
     ]
    }
   ],
   "source": [
    "# حذف المحارف المكررة من النص\n",
    "X_train_combined = X_train['question'].apply(remove_duplicate_letters_except_alif) + ' ' + X_train['answer'].apply(remove_duplicate_letters_except_alif)\n",
    "\n",
    "\n",
    "# print(len(X_train_without_links))\n",
    "print(len(X_train_combined))\n",
    "\n",
    "accuracy_test, accuracy_validation, balanced_accuracy_test = logistic_regression(X_train_combined,X_test_combined,X_validation_combined)\n",
    "\n",
    "comparison_table['question_step_number'].append(\"5.3\")\n",
    "comparison_table['model_name'].append('logistic_regression')\n",
    "comparison_table['features'].append('bag of words')\n",
    "comparison_table['model_parameters'].append('default')\n",
    "comparison_table['preprocessing_methods'].append(\"Removing duplicate letters except alif\")\n",
    "comparison_table['accuracy'].append(accuracy_test)\n",
    "comparison_table['balance_accuracy'].append(balanced_accuracy_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32598\n",
      "Test Accuracy: 0.6424392838149264\n",
      "Validation Accuracy: 0.6497663345384005\n",
      "Balanced Accuracy: 0.6240931361367352\n"
     ]
    }
   ],
   "source": [
    "# تحويل الارقام الى العربية\n",
    "X_train_combined = X_train['question'].apply(convert_numbers_to_arabic_v2) + ' ' + X_train['answer'].apply(convert_numbers_to_arabic_v2)\n",
    "\n",
    "\n",
    "# print(len(X_train_without_links))\n",
    "print(len(X_train_combined))\n",
    "\n",
    "accuracy_test, accuracy_validation, balanced_accuracy_test = logistic_regression(X_train_combined,X_test_combined,X_validation_combined)\n",
    "\n",
    "comparison_table['question_step_number'].append(\"5.3\")\n",
    "comparison_table['model_name'].append('logistic_regression')\n",
    "comparison_table['features'].append('bag of words')\n",
    "comparison_table['model_parameters'].append('default')\n",
    "comparison_table['preprocessing_methods'].append(\"Removing Links\")\n",
    "comparison_table['accuracy'].append(accuracy_test)\n",
    "comparison_table['balance_accuracy'].append(balanced_accuracy_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32598\n",
      "Test Accuracy: 0.6348165218932813\n",
      "Validation Accuracy: 0.6417423507627193\n",
      "Balanced Accuracy: 0.6157853154416597\n"
     ]
    }
   ],
   "source": [
    "# تحويل الارقام الى رمز من اختياري\n",
    "X_train_combined = X_train['question'].apply(standardize_numbers) + ' ' + X_train['answer'].apply(standardize_numbers)\n",
    "\n",
    "\n",
    "# print(len(X_train_without_links))\n",
    "print(len(X_train_combined))\n",
    "\n",
    "accuracy_test, accuracy_validation, balanced_accuracy_test = logistic_regression(X_train_combined,X_test_combined,X_validation_combined)\n",
    "\n",
    "comparison_table['question_step_number'].append(\"5.3\")\n",
    "comparison_table['model_name'].append('logistic_regression')\n",
    "comparison_table['features'].append('bag of words')\n",
    "comparison_table['model_parameters'].append('default')\n",
    "comparison_table['preprocessing_methods'].append(\"standardize numbers to specific char\")\n",
    "comparison_table['accuracy'].append(accuracy_test)\n",
    "comparison_table['balance_accuracy'].append(balanced_accuracy_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32598\n",
      "Test Accuracy: 0.6348165218932813\n",
      "Validation Accuracy: 0.6407724186579666\n",
      "Balanced Accuracy: 0.6160329300852934\n"
     ]
    }
   ],
   "source": [
    "# حذف الأرقام كلها\n",
    "X_train_combined = X_train['question'].apply(remove_numbers) + ' ' + X_train['answer'].apply(remove_numbers)\n",
    "\n",
    "\n",
    "# print(len(X_train_without_links))\n",
    "print(len(X_train_combined))\n",
    "\n",
    "accuracy_test, accuracy_validation, balanced_accuracy_test = logistic_regression(X_train_combined,X_test_combined,X_validation_combined)\n",
    "\n",
    "comparison_table['question_step_number'].append(\"5.3\")\n",
    "comparison_table['model_name'].append('logistic_regression')\n",
    "comparison_table['features'].append('bag of words')\n",
    "comparison_table['model_parameters'].append('default')\n",
    "comparison_table['preprocessing_methods'].append(\"Removing numbers\")\n",
    "comparison_table['accuracy'].append(accuracy_test)\n",
    "comparison_table['balance_accuracy'].append(balanced_accuracy_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32598\n",
      "Test Accuracy: 0.6217869172132601\n",
      "Validation Accuracy: 0.628163301296182\n",
      "Balanced Accuracy: 0.5997778691628503\n"
     ]
    }
   ],
   "source": [
    "# حذف التاريخ والوقت\n",
    "X_train_combined = X_train['question'].apply(remove_dates_and_times) + ' ' + X_train['answer'].apply(remove_dates_and_times)\n",
    "\n",
    "\n",
    "# print(len(X_train_without_links))\n",
    "print(len(X_train_combined))\n",
    "\n",
    "accuracy_test, accuracy_validation, balanced_accuracy_test = logistic_regression(X_train_combined,X_test_combined,X_validation_combined)\n",
    "\n",
    "comparison_table['question_step_number'].append(\"5.3\")\n",
    "comparison_table['model_name'].append('logistic_regression')\n",
    "comparison_table['features'].append('bag of words')\n",
    "comparison_table['model_parameters'].append('default')\n",
    "comparison_table['preprocessing_methods'].append(\"Removing Date and time\")\n",
    "comparison_table['accuracy'].append(accuracy_test)\n",
    "comparison_table['balance_accuracy'].append(balanced_accuracy_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32598\n",
      "Test Accuracy: 0.6435915617798262\n",
      "Validation Accuracy: 0.6483555242042148\n",
      "Balanced Accuracy: 0.6242531728086279\n"
     ]
    }
   ],
   "source": [
    "# حذف المحارف الغير عربية\n",
    "X_train_combined = X_train['question'].apply(keep_arabic_punctuations_numbers) + ' ' + X_train['answer'].apply(keep_arabic_punctuations_numbers)\n",
    "\n",
    "\n",
    "# print(len(X_train_without_links))\n",
    "print(len(X_train_combined))\n",
    "\n",
    "accuracy_test, accuracy_validation, balanced_accuracy_test = logistic_regression(X_train_combined,X_test_combined,X_validation_combined)\n",
    "\n",
    "comparison_table['question_step_number'].append(\"5.3\")\n",
    "comparison_table['model_name'].append('logistic_regression')\n",
    "comparison_table['features'].append('bag of words')\n",
    "comparison_table['model_parameters'].append('default')\n",
    "comparison_table['preprocessing_methods'].append(\"Remove non arabic words\")\n",
    "comparison_table['accuracy'].append(accuracy_test)\n",
    "comparison_table['balance_accuracy'].append(balanced_accuracy_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32598\n",
      "Test Accuracy: 0.6435915617798262\n",
      "Validation Accuracy: 0.6483555242042148\n",
      "Balanced Accuracy: 0.6242531728086279\n"
     ]
    }
   ],
   "source": [
    "# حذف كلمات التوقف\n",
    "X_train_combined = X_train['question'].apply(keep_arabic_punctuations_numbers) + ' ' + X_train['answer'].apply(keep_arabic_punctuations_numbers)\n",
    "\n",
    "\n",
    "# print(len(X_train_without_links))\n",
    "print(len(X_train_combined))\n",
    "\n",
    "accuracy_test, accuracy_validation, balanced_accuracy_test = logistic_regression(X_train_combined,X_test_combined,X_validation_combined)\n",
    "\n",
    "comparison_table['question_step_number'].append(\"5.3\")\n",
    "comparison_table['model_name'].append('logistic_regression')\n",
    "comparison_table['features'].append('bag of words')\n",
    "comparison_table['model_parameters'].append('default')\n",
    "comparison_table['preprocessing_methods'].append(\"Remove non arabic words\")\n",
    "comparison_table['accuracy'].append(accuracy_test)\n",
    "comparison_table['balance_accuracy'].append(balanced_accuracy_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32598\n",
      "Test Accuracy: 0.553270696684985\n",
      "Validation Accuracy: 0.5588572436293096\n",
      "Balanced Accuracy: 0.5175959620880198\n"
     ]
    }
   ],
   "source": [
    "# تجذير الكلمات العربية\n",
    "X_train_combined = X_train['question'].apply(stem_arabic_words_snowballstemmer) + ' ' + X_train['answer'].apply(stem_arabic_words_snowballstemmer)\n",
    "\n",
    "\n",
    "# print(len(X_train_without_links))\n",
    "print(len(X_train_combined))\n",
    "\n",
    "accuracy_test, accuracy_validation, balanced_accuracy_test = logistic_regression(X_train_combined,X_test_combined,X_validation_combined)\n",
    "\n",
    "comparison_table['question_step_number'].append(\"5.3\")\n",
    "comparison_table['model_name'].append('logistic_regression')\n",
    "comparison_table['features'].append('bag of words')\n",
    "comparison_table['model_parameters'].append('default')\n",
    "comparison_table['preprocessing_methods'].append(\"stem arabic words\")\n",
    "comparison_table['accuracy'].append(accuracy_test)\n",
    "comparison_table['balance_accuracy'].append(balanced_accuracy_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32598\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\ASUS\\Desktop\\nlp_homework\\NLP_HOMEWORK_1_BY_[your_name].ipynb Cell 160\u001b[0m line \u001b[0;36m8\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/ASUS/Desktop/nlp_homework/NLP_HOMEWORK_1_BY_%5Byour_name%5D.ipynb#Y320sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39m# print(len(X_train_without_links))\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/ASUS/Desktop/nlp_homework/NLP_HOMEWORK_1_BY_%5Byour_name%5D.ipynb#Y320sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mlen\u001b[39m(X_train_combined))\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/ASUS/Desktop/nlp_homework/NLP_HOMEWORK_1_BY_%5Byour_name%5D.ipynb#Y320sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m accuracy_test, accuracy_validation, balanced_accuracy_test \u001b[39m=\u001b[39m logistic_regression(X_train_combined,X_test_combined,X_validation_combined)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ASUS/Desktop/nlp_homework/NLP_HOMEWORK_1_BY_%5Byour_name%5D.ipynb#Y320sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m comparison_table[\u001b[39m'\u001b[39m\u001b[39mquestion_step_number\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mappend(\u001b[39m\"\u001b[39m\u001b[39m5.3\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ASUS/Desktop/nlp_homework/NLP_HOMEWORK_1_BY_%5Byour_name%5D.ipynb#Y320sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m comparison_table[\u001b[39m'\u001b[39m\u001b[39mmodel_name\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mappend(\u001b[39m'\u001b[39m\u001b[39mlogistic_regression\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[1;32mc:\\Users\\ASUS\\Desktop\\nlp_homework\\NLP_HOMEWORK_1_BY_[your_name].ipynb Cell 160\u001b[0m line \u001b[0;36m1\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ASUS/Desktop/nlp_homework/NLP_HOMEWORK_1_BY_%5Byour_name%5D.ipynb#Y320sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m X_validation_bow \u001b[39m=\u001b[39m vectorizer\u001b[39m.\u001b[39mtransform(x_val)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ASUS/Desktop/nlp_homework/NLP_HOMEWORK_1_BY_%5Byour_name%5D.ipynb#Y320sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m logreg_model \u001b[39m=\u001b[39m LogisticRegression(max_iter\u001b[39m=\u001b[39m\u001b[39m1000\u001b[39m)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/ASUS/Desktop/nlp_homework/NLP_HOMEWORK_1_BY_%5Byour_name%5D.ipynb#Y320sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m logreg_model\u001b[39m.\u001b[39mfit(X_train_bow, y_train)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ASUS/Desktop/nlp_homework/NLP_HOMEWORK_1_BY_%5Byour_name%5D.ipynb#Y320sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m y_test_pred \u001b[39m=\u001b[39m logreg_model\u001b[39m.\u001b[39mpredict(X_test_bow)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ASUS/Desktop/nlp_homework/NLP_HOMEWORK_1_BY_%5Byour_name%5D.ipynb#Y320sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m y_validation_pred \u001b[39m=\u001b[39m logreg_model\u001b[39m.\u001b[39mpredict(X_validation_bow)\n",
      "File \u001b[1;32mc:\\Users\\ASUS\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:1151\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1144\u001b[0m     estimator\u001b[39m.\u001b[39m_validate_params()\n\u001b[0;32m   1146\u001b[0m \u001b[39mwith\u001b[39;00m config_context(\n\u001b[0;32m   1147\u001b[0m     skip_parameter_validation\u001b[39m=\u001b[39m(\n\u001b[0;32m   1148\u001b[0m         prefer_skip_nested_validation \u001b[39mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1149\u001b[0m     )\n\u001b[0;32m   1150\u001b[0m ):\n\u001b[1;32m-> 1151\u001b[0m     \u001b[39mreturn\u001b[39;00m fit_method(estimator, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\ASUS\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1302\u001b[0m, in \u001b[0;36mLogisticRegression.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m   1299\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1300\u001b[0m     n_threads \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m-> 1302\u001b[0m fold_coefs_ \u001b[39m=\u001b[39m Parallel(n_jobs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_jobs, verbose\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mverbose, prefer\u001b[39m=\u001b[39mprefer)(\n\u001b[0;32m   1303\u001b[0m     path_func(\n\u001b[0;32m   1304\u001b[0m         X,\n\u001b[0;32m   1305\u001b[0m         y,\n\u001b[0;32m   1306\u001b[0m         pos_class\u001b[39m=\u001b[39mclass_,\n\u001b[0;32m   1307\u001b[0m         Cs\u001b[39m=\u001b[39m[C_],\n\u001b[0;32m   1308\u001b[0m         l1_ratio\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39ml1_ratio,\n\u001b[0;32m   1309\u001b[0m         fit_intercept\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfit_intercept,\n\u001b[0;32m   1310\u001b[0m         tol\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtol,\n\u001b[0;32m   1311\u001b[0m         verbose\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mverbose,\n\u001b[0;32m   1312\u001b[0m         solver\u001b[39m=\u001b[39msolver,\n\u001b[0;32m   1313\u001b[0m         multi_class\u001b[39m=\u001b[39mmulti_class,\n\u001b[0;32m   1314\u001b[0m         max_iter\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmax_iter,\n\u001b[0;32m   1315\u001b[0m         class_weight\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclass_weight,\n\u001b[0;32m   1316\u001b[0m         check_input\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[0;32m   1317\u001b[0m         random_state\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrandom_state,\n\u001b[0;32m   1318\u001b[0m         coef\u001b[39m=\u001b[39mwarm_start_coef_,\n\u001b[0;32m   1319\u001b[0m         penalty\u001b[39m=\u001b[39mpenalty,\n\u001b[0;32m   1320\u001b[0m         max_squared_sum\u001b[39m=\u001b[39mmax_squared_sum,\n\u001b[0;32m   1321\u001b[0m         sample_weight\u001b[39m=\u001b[39msample_weight,\n\u001b[0;32m   1322\u001b[0m         n_threads\u001b[39m=\u001b[39mn_threads,\n\u001b[0;32m   1323\u001b[0m     )\n\u001b[0;32m   1324\u001b[0m     \u001b[39mfor\u001b[39;00m class_, warm_start_coef_ \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(classes_, warm_start_coef)\n\u001b[0;32m   1325\u001b[0m )\n\u001b[0;32m   1327\u001b[0m fold_coefs_, _, n_iter_ \u001b[39m=\u001b[39m \u001b[39mzip\u001b[39m(\u001b[39m*\u001b[39mfold_coefs_)\n\u001b[0;32m   1328\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_iter_ \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39masarray(n_iter_, dtype\u001b[39m=\u001b[39mnp\u001b[39m.\u001b[39mint32)[:, \u001b[39m0\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\ASUS\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\parallel.py:65\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m     60\u001b[0m config \u001b[39m=\u001b[39m get_config()\n\u001b[0;32m     61\u001b[0m iterable_with_config \u001b[39m=\u001b[39m (\n\u001b[0;32m     62\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[0;32m     63\u001b[0m     \u001b[39mfor\u001b[39;00m delayed_func, args, kwargs \u001b[39min\u001b[39;00m iterable\n\u001b[0;32m     64\u001b[0m )\n\u001b[1;32m---> 65\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__call__\u001b[39m(iterable_with_config)\n",
      "File \u001b[1;32mc:\\Users\\ASUS\\anaconda3\\Lib\\site-packages\\joblib\\parallel.py:1085\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1076\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1077\u001b[0m     \u001b[39m# Only set self._iterating to True if at least a batch\u001b[39;00m\n\u001b[0;32m   1078\u001b[0m     \u001b[39m# was dispatched. In particular this covers the edge\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1082\u001b[0m     \u001b[39m# was very quick and its callback already dispatched all the\u001b[39;00m\n\u001b[0;32m   1083\u001b[0m     \u001b[39m# remaining jobs.\u001b[39;00m\n\u001b[0;32m   1084\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_iterating \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m-> 1085\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdispatch_one_batch(iterator):\n\u001b[0;32m   1086\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_iterating \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_original_iterator \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m   1088\u001b[0m     \u001b[39mwhile\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdispatch_one_batch(iterator):\n",
      "File \u001b[1;32mc:\\Users\\ASUS\\anaconda3\\Lib\\site-packages\\joblib\\parallel.py:901\u001b[0m, in \u001b[0;36mParallel.dispatch_one_batch\u001b[1;34m(self, iterator)\u001b[0m\n\u001b[0;32m    899\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m    900\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 901\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dispatch(tasks)\n\u001b[0;32m    902\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ASUS\\anaconda3\\Lib\\site-packages\\joblib\\parallel.py:819\u001b[0m, in \u001b[0;36mParallel._dispatch\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    817\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[0;32m    818\u001b[0m     job_idx \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jobs)\n\u001b[1;32m--> 819\u001b[0m     job \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backend\u001b[39m.\u001b[39mapply_async(batch, callback\u001b[39m=\u001b[39mcb)\n\u001b[0;32m    820\u001b[0m     \u001b[39m# A job can complete so quickly than its callback is\u001b[39;00m\n\u001b[0;32m    821\u001b[0m     \u001b[39m# called before we get here, causing self._jobs to\u001b[39;00m\n\u001b[0;32m    822\u001b[0m     \u001b[39m# grow. To ensure correct results ordering, .insert is\u001b[39;00m\n\u001b[0;32m    823\u001b[0m     \u001b[39m# used (rather than .append) in the following line\u001b[39;00m\n\u001b[0;32m    824\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jobs\u001b[39m.\u001b[39minsert(job_idx, job)\n",
      "File \u001b[1;32mc:\\Users\\ASUS\\anaconda3\\Lib\\site-packages\\joblib\\_parallel_backends.py:208\u001b[0m, in \u001b[0;36mSequentialBackend.apply_async\u001b[1;34m(self, func, callback)\u001b[0m\n\u001b[0;32m    206\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mapply_async\u001b[39m(\u001b[39mself\u001b[39m, func, callback\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m    207\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Schedule a func to be run\"\"\"\u001b[39;00m\n\u001b[1;32m--> 208\u001b[0m     result \u001b[39m=\u001b[39m ImmediateResult(func)\n\u001b[0;32m    209\u001b[0m     \u001b[39mif\u001b[39;00m callback:\n\u001b[0;32m    210\u001b[0m         callback(result)\n",
      "File \u001b[1;32mc:\\Users\\ASUS\\anaconda3\\Lib\\site-packages\\joblib\\_parallel_backends.py:597\u001b[0m, in \u001b[0;36mImmediateResult.__init__\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    594\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, batch):\n\u001b[0;32m    595\u001b[0m     \u001b[39m# Don't delay the application, to avoid keeping the input\u001b[39;00m\n\u001b[0;32m    596\u001b[0m     \u001b[39m# arguments in memory\u001b[39;00m\n\u001b[1;32m--> 597\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mresults \u001b[39m=\u001b[39m batch()\n",
      "File \u001b[1;32mc:\\Users\\ASUS\\anaconda3\\Lib\\site-packages\\joblib\\parallel.py:288\u001b[0m, in \u001b[0;36mBatchedCalls.__call__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    284\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    285\u001b[0m     \u001b[39m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[0;32m    286\u001b[0m     \u001b[39m# change the default number of processes to -1\u001b[39;00m\n\u001b[0;32m    287\u001b[0m     \u001b[39mwith\u001b[39;00m parallel_backend(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backend, n_jobs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_n_jobs):\n\u001b[1;32m--> 288\u001b[0m         \u001b[39mreturn\u001b[39;00m [func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    289\u001b[0m                 \u001b[39mfor\u001b[39;00m func, args, kwargs \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mitems]\n",
      "File \u001b[1;32mc:\\Users\\ASUS\\anaconda3\\Lib\\site-packages\\joblib\\parallel.py:288\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    284\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    285\u001b[0m     \u001b[39m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[0;32m    286\u001b[0m     \u001b[39m# change the default number of processes to -1\u001b[39;00m\n\u001b[0;32m    287\u001b[0m     \u001b[39mwith\u001b[39;00m parallel_backend(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backend, n_jobs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_n_jobs):\n\u001b[1;32m--> 288\u001b[0m         \u001b[39mreturn\u001b[39;00m [func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    289\u001b[0m                 \u001b[39mfor\u001b[39;00m func, args, kwargs \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mitems]\n",
      "File \u001b[1;32mc:\\Users\\ASUS\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\parallel.py:127\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    125\u001b[0m     config \u001b[39m=\u001b[39m {}\n\u001b[0;32m    126\u001b[0m \u001b[39mwith\u001b[39;00m config_context(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mconfig):\n\u001b[1;32m--> 127\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfunction(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\ASUS\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:452\u001b[0m, in \u001b[0;36m_logistic_regression_path\u001b[1;34m(X, y, pos_class, Cs, fit_intercept, max_iter, tol, verbose, solver, coef, class_weight, dual, penalty, intercept_scaling, multi_class, random_state, check_input, max_squared_sum, sample_weight, l1_ratio, n_threads)\u001b[0m\n\u001b[0;32m    448\u001b[0m l2_reg_strength \u001b[39m=\u001b[39m \u001b[39m1.0\u001b[39m \u001b[39m/\u001b[39m C\n\u001b[0;32m    449\u001b[0m iprint \u001b[39m=\u001b[39m [\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39m50\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m100\u001b[39m, \u001b[39m101\u001b[39m][\n\u001b[0;32m    450\u001b[0m     np\u001b[39m.\u001b[39msearchsorted(np\u001b[39m.\u001b[39marray([\u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m, \u001b[39m3\u001b[39m]), verbose)\n\u001b[0;32m    451\u001b[0m ]\n\u001b[1;32m--> 452\u001b[0m opt_res \u001b[39m=\u001b[39m optimize\u001b[39m.\u001b[39mminimize(\n\u001b[0;32m    453\u001b[0m     func,\n\u001b[0;32m    454\u001b[0m     w0,\n\u001b[0;32m    455\u001b[0m     method\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mL-BFGS-B\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m    456\u001b[0m     jac\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[0;32m    457\u001b[0m     args\u001b[39m=\u001b[39m(X, target, sample_weight, l2_reg_strength, n_threads),\n\u001b[0;32m    458\u001b[0m     options\u001b[39m=\u001b[39m{\u001b[39m\"\u001b[39m\u001b[39miprint\u001b[39m\u001b[39m\"\u001b[39m: iprint, \u001b[39m\"\u001b[39m\u001b[39mgtol\u001b[39m\u001b[39m\"\u001b[39m: tol, \u001b[39m\"\u001b[39m\u001b[39mmaxiter\u001b[39m\u001b[39m\"\u001b[39m: max_iter},\n\u001b[0;32m    459\u001b[0m )\n\u001b[0;32m    460\u001b[0m n_iter_i \u001b[39m=\u001b[39m _check_optimize_result(\n\u001b[0;32m    461\u001b[0m     solver,\n\u001b[0;32m    462\u001b[0m     opt_res,\n\u001b[0;32m    463\u001b[0m     max_iter,\n\u001b[0;32m    464\u001b[0m     extra_warning_msg\u001b[39m=\u001b[39m_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n\u001b[0;32m    465\u001b[0m )\n\u001b[0;32m    466\u001b[0m w0, loss \u001b[39m=\u001b[39m opt_res\u001b[39m.\u001b[39mx, opt_res\u001b[39m.\u001b[39mfun\n",
      "File \u001b[1;32mc:\\Users\\ASUS\\anaconda3\\Lib\\site-packages\\scipy\\optimize\\_minimize.py:710\u001b[0m, in \u001b[0;36mminimize\u001b[1;34m(fun, x0, args, method, jac, hess, hessp, bounds, constraints, tol, callback, options)\u001b[0m\n\u001b[0;32m    707\u001b[0m     res \u001b[39m=\u001b[39m _minimize_newtoncg(fun, x0, args, jac, hess, hessp, callback,\n\u001b[0;32m    708\u001b[0m                              \u001b[39m*\u001b[39m\u001b[39m*\u001b[39moptions)\n\u001b[0;32m    709\u001b[0m \u001b[39melif\u001b[39;00m meth \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39ml-bfgs-b\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m--> 710\u001b[0m     res \u001b[39m=\u001b[39m _minimize_lbfgsb(fun, x0, args, jac, bounds,\n\u001b[0;32m    711\u001b[0m                            callback\u001b[39m=\u001b[39mcallback, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39moptions)\n\u001b[0;32m    712\u001b[0m \u001b[39melif\u001b[39;00m meth \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mtnc\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[0;32m    713\u001b[0m     res \u001b[39m=\u001b[39m _minimize_tnc(fun, x0, args, jac, bounds, callback\u001b[39m=\u001b[39mcallback,\n\u001b[0;32m    714\u001b[0m                         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39moptions)\n",
      "File \u001b[1;32mc:\\Users\\ASUS\\anaconda3\\Lib\\site-packages\\scipy\\optimize\\_lbfgsb_py.py:365\u001b[0m, in \u001b[0;36m_minimize_lbfgsb\u001b[1;34m(fun, x0, args, jac, bounds, disp, maxcor, ftol, gtol, eps, maxfun, maxiter, iprint, callback, maxls, finite_diff_rel_step, **unknown_options)\u001b[0m\n\u001b[0;32m    359\u001b[0m task_str \u001b[39m=\u001b[39m task\u001b[39m.\u001b[39mtobytes()\n\u001b[0;32m    360\u001b[0m \u001b[39mif\u001b[39;00m task_str\u001b[39m.\u001b[39mstartswith(\u001b[39mb\u001b[39m\u001b[39m'\u001b[39m\u001b[39mFG\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[0;32m    361\u001b[0m     \u001b[39m# The minimization routine wants f and g at the current x.\u001b[39;00m\n\u001b[0;32m    362\u001b[0m     \u001b[39m# Note that interruptions due to maxfun are postponed\u001b[39;00m\n\u001b[0;32m    363\u001b[0m     \u001b[39m# until the completion of the current minimization iteration.\u001b[39;00m\n\u001b[0;32m    364\u001b[0m     \u001b[39m# Overwrite f and g:\u001b[39;00m\n\u001b[1;32m--> 365\u001b[0m     f, g \u001b[39m=\u001b[39m func_and_grad(x)\n\u001b[0;32m    366\u001b[0m \u001b[39melif\u001b[39;00m task_str\u001b[39m.\u001b[39mstartswith(\u001b[39mb\u001b[39m\u001b[39m'\u001b[39m\u001b[39mNEW_X\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[0;32m    367\u001b[0m     \u001b[39m# new iteration\u001b[39;00m\n\u001b[0;32m    368\u001b[0m     n_iterations \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\ASUS\\anaconda3\\Lib\\site-packages\\scipy\\optimize\\_differentiable_functions.py:285\u001b[0m, in \u001b[0;36mScalarFunction.fun_and_grad\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    283\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m np\u001b[39m.\u001b[39marray_equal(x, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mx):\n\u001b[0;32m    284\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_update_x_impl(x)\n\u001b[1;32m--> 285\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_update_fun()\n\u001b[0;32m    286\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_update_grad()\n\u001b[0;32m    287\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mf, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mg\n",
      "File \u001b[1;32mc:\\Users\\ASUS\\anaconda3\\Lib\\site-packages\\scipy\\optimize\\_differentiable_functions.py:251\u001b[0m, in \u001b[0;36mScalarFunction._update_fun\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    249\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_update_fun\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    250\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mf_updated:\n\u001b[1;32m--> 251\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_update_fun_impl()\n\u001b[0;32m    252\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mf_updated \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ASUS\\anaconda3\\Lib\\site-packages\\scipy\\optimize\\_differentiable_functions.py:155\u001b[0m, in \u001b[0;36mScalarFunction.__init__.<locals>.update_fun\u001b[1;34m()\u001b[0m\n\u001b[0;32m    154\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mupdate_fun\u001b[39m():\n\u001b[1;32m--> 155\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mf \u001b[39m=\u001b[39m fun_wrapped(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mx)\n",
      "File \u001b[1;32mc:\\Users\\ASUS\\anaconda3\\Lib\\site-packages\\scipy\\optimize\\_differentiable_functions.py:137\u001b[0m, in \u001b[0;36mScalarFunction.__init__.<locals>.fun_wrapped\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m    133\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnfev \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m    134\u001b[0m \u001b[39m# Send a copy because the user may overwrite it.\u001b[39;00m\n\u001b[0;32m    135\u001b[0m \u001b[39m# Overwriting results in undefined behaviour because\u001b[39;00m\n\u001b[0;32m    136\u001b[0m \u001b[39m# fun(self.x) will change self.x, with the two no longer linked.\u001b[39;00m\n\u001b[1;32m--> 137\u001b[0m fx \u001b[39m=\u001b[39m fun(np\u001b[39m.\u001b[39mcopy(x), \u001b[39m*\u001b[39margs)\n\u001b[0;32m    138\u001b[0m \u001b[39m# Make sure the function returns a true scalar\u001b[39;00m\n\u001b[0;32m    139\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m np\u001b[39m.\u001b[39misscalar(fx):\n",
      "File \u001b[1;32mc:\\Users\\ASUS\\anaconda3\\Lib\\site-packages\\scipy\\optimize\\_optimize.py:77\u001b[0m, in \u001b[0;36mMemoizeJac.__call__\u001b[1;34m(self, x, *args)\u001b[0m\n\u001b[0;32m     75\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, x, \u001b[39m*\u001b[39margs):\n\u001b[0;32m     76\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\" returns the function value \"\"\"\u001b[39;00m\n\u001b[1;32m---> 77\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compute_if_needed(x, \u001b[39m*\u001b[39margs)\n\u001b[0;32m     78\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_value\n",
      "File \u001b[1;32mc:\\Users\\ASUS\\anaconda3\\Lib\\site-packages\\scipy\\optimize\\_optimize.py:71\u001b[0m, in \u001b[0;36mMemoizeJac._compute_if_needed\u001b[1;34m(self, x, *args)\u001b[0m\n\u001b[0;32m     69\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m np\u001b[39m.\u001b[39mall(x \u001b[39m==\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mx) \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_value \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mjac \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m     70\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mx \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39masarray(x)\u001b[39m.\u001b[39mcopy()\n\u001b[1;32m---> 71\u001b[0m     fg \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfun(x, \u001b[39m*\u001b[39margs)\n\u001b[0;32m     72\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mjac \u001b[39m=\u001b[39m fg[\u001b[39m1\u001b[39m]\n\u001b[0;32m     73\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_value \u001b[39m=\u001b[39m fg[\u001b[39m0\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\ASUS\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_linear_loss.py:296\u001b[0m, in \u001b[0;36mLinearModelLoss.loss_gradient\u001b[1;34m(self, coef, X, y, sample_weight, l2_reg_strength, n_threads, raw_prediction)\u001b[0m\n\u001b[0;32m    294\u001b[0m grad \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mempty((n_classes, n_dof), dtype\u001b[39m=\u001b[39mweights\u001b[39m.\u001b[39mdtype, order\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mF\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    295\u001b[0m \u001b[39m# grad_pointwise.shape = (n_samples, n_classes)\u001b[39;00m\n\u001b[1;32m--> 296\u001b[0m grad[:, :n_features] \u001b[39m=\u001b[39m grad_pointwise\u001b[39m.\u001b[39mT \u001b[39m@\u001b[39m X \u001b[39m+\u001b[39m l2_reg_strength \u001b[39m*\u001b[39m weights\n\u001b[0;32m    297\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfit_intercept:\n\u001b[0;32m    298\u001b[0m     grad[:, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m] \u001b[39m=\u001b[39m grad_pointwise\u001b[39m.\u001b[39msum(axis\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\ASUS\\anaconda3\\Lib\\site-packages\\scipy\\sparse\\_base.py:630\u001b[0m, in \u001b[0;36m_spbase.__rmatmul__\u001b[1;34m(self, other)\u001b[0m\n\u001b[0;32m    627\u001b[0m \u001b[39mif\u001b[39;00m isscalarlike(other):\n\u001b[0;32m    628\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mScalar operands are not allowed, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    629\u001b[0m                      \u001b[39m\"\u001b[39m\u001b[39muse \u001b[39m\u001b[39m'\u001b[39m\u001b[39m*\u001b[39m\u001b[39m'\u001b[39m\u001b[39m instead\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m--> 630\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_rmul_dispatch(other)\n",
      "File \u001b[1;32mc:\\Users\\ASUS\\anaconda3\\Lib\\site-packages\\scipy\\sparse\\_base.py:608\u001b[0m, in \u001b[0;36m_spbase._rmul_dispatch\u001b[1;34m(self, other)\u001b[0m\n\u001b[0;32m    606\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m:\n\u001b[0;32m    607\u001b[0m     tr \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39masarray(other)\u001b[39m.\u001b[39mtranspose()\n\u001b[1;32m--> 608\u001b[0m ret \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtranspose()\u001b[39m.\u001b[39m_mul_dispatch(tr)\n\u001b[0;32m    609\u001b[0m \u001b[39mif\u001b[39;00m ret \u001b[39mis\u001b[39;00m \u001b[39mNotImplemented\u001b[39m:\n\u001b[0;32m    610\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mNotImplemented\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\ASUS\\anaconda3\\Lib\\site-packages\\scipy\\sparse\\_base.py:526\u001b[0m, in \u001b[0;36m_spbase._mul_dispatch\u001b[1;34m(self, other)\u001b[0m\n\u001b[0;32m    524\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_mul_vector(other\u001b[39m.\u001b[39mravel())\u001b[39m.\u001b[39mreshape(M, \u001b[39m1\u001b[39m)\n\u001b[0;32m    525\u001b[0m     \u001b[39melif\u001b[39;00m other\u001b[39m.\u001b[39mndim \u001b[39m==\u001b[39m \u001b[39m2\u001b[39m \u001b[39mand\u001b[39;00m other\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m] \u001b[39m==\u001b[39m N:\n\u001b[1;32m--> 526\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_mul_multivector(other)\n\u001b[0;32m    528\u001b[0m \u001b[39mif\u001b[39;00m isscalarlike(other):\n\u001b[0;32m    529\u001b[0m     \u001b[39m# scalar value\u001b[39;00m\n\u001b[0;32m    530\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_mul_scalar(other)\n",
      "File \u001b[1;32mc:\\Users\\ASUS\\anaconda3\\Lib\\site-packages\\scipy\\sparse\\_compressed.py:501\u001b[0m, in \u001b[0;36m_cs_matrix._mul_multivector\u001b[1;34m(self, other)\u001b[0m\n\u001b[0;32m    499\u001b[0m \u001b[39m# csr_matvecs or csc_matvecs\u001b[39;00m\n\u001b[0;32m    500\u001b[0m fn \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(_sparsetools, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mformat \u001b[39m+\u001b[39m \u001b[39m'\u001b[39m\u001b[39m_matvecs\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m--> 501\u001b[0m fn(M, N, n_vecs, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mindptr, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mindices, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata,\n\u001b[0;32m    502\u001b[0m    other\u001b[39m.\u001b[39mravel(), result\u001b[39m.\u001b[39mravel())\n\u001b[0;32m    504\u001b[0m \u001b[39mreturn\u001b[39;00m result\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# توحيد طريقة كتابة الهمزات\n",
    "X_train_combined = X_train['question'].apply(unify_hamzat) + ' ' + X_train['answer'].apply(unify_hamzat)\n",
    "\n",
    "\n",
    "# print(len(X_train_without_links))\n",
    "print(len(X_train_combined))\n",
    "\n",
    "accuracy_test, accuracy_validation, balanced_accuracy_test = logistic_regression(X_train_combined,X_test_combined,X_validation_combined)\n",
    "\n",
    "comparison_table['question_step_number'].append(\"5.3\")\n",
    "comparison_table['model_name'].append('logistic_regression')\n",
    "comparison_table['features'].append('bag of words')\n",
    "comparison_table['model_parameters'].append('default')\n",
    "comparison_table['preprocessing_methods'].append(\"unify hamzat\")\n",
    "comparison_table['accuracy'].append(accuracy_test)\n",
    "comparison_table['balance_accuracy'].append(balanced_accuracy_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# توحيد طريقة كتابة الألفات\n",
    "X_train_combined = X_train['question'].apply(unify_Alfat) + ' ' + X_train['answer'].apply(unify_Alfat)\n",
    "\n",
    "\n",
    "# print(len(X_train_without_links))\n",
    "print(len(X_train_combined))\n",
    "\n",
    "accuracy_test, accuracy_validation, balanced_accuracy_test = logistic_regression(X_train_combined,X_test_combined,X_validation_combined)\n",
    "\n",
    "comparison_table['question_step_number'].append(\"5.3\")\n",
    "comparison_table['model_name'].append('logistic_regression')\n",
    "comparison_table['features'].append('bag of words')\n",
    "comparison_table['model_parameters'].append('default')\n",
    "comparison_table['preprocessing_methods'].append(\"unify Alafat\")\n",
    "comparison_table['accuracy'].append(accuracy_test)\n",
    "comparison_table['balance_accuracy'].append(balanced_accuracy_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# حذف التطويل\n",
    "X_train_combined = X_train['question'].apply(remove_tatweel) + ' ' + X_train['answer'].apply(remove_tatweel)\n",
    "\n",
    "\n",
    "# print(len(X_train_without_links))\n",
    "print(len(X_train_combined))\n",
    "\n",
    "accuracy_test, accuracy_validation, balanced_accuracy_test = logistic_regression(X_train_combined,X_test_combined,X_validation_combined)\n",
    "\n",
    "comparison_table['question_step_number'].append(\"5.3\")\n",
    "comparison_table['model_name'].append('logistic_regression')\n",
    "comparison_table['features'].append('bag of words')\n",
    "comparison_table['model_parameters'].append('default')\n",
    "comparison_table['preprocessing_methods'].append(\"remove tatweel\")\n",
    "comparison_table['accuracy'].append(accuracy_test)\n",
    "comparison_table['balance_accuracy'].append(balanced_accuracy_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# حذف التشكيل\n",
    "X_train_combined = X_train['question'].apply(remove_arabic_diacritics) + ' ' + X_train['answer'].apply(remove_arabic_diacritics)\n",
    "\n",
    "\n",
    "# print(len(X_train_without_links))\n",
    "print(len(X_train_combined))\n",
    "\n",
    "accuracy_test, accuracy_validation, balanced_accuracy_test = logistic_regression(X_train_combined,X_test_combined,X_validation_combined)\n",
    "\n",
    "comparison_table['question_step_number'].append(\"5.3\")\n",
    "comparison_table['model_name'].append('logistic_regression')\n",
    "comparison_table['features'].append('bag of words')\n",
    "comparison_table['model_parameters'].append('default')\n",
    "comparison_table['preprocessing_methods'].append(\"remove arabic diacritics\")\n",
    "comparison_table['accuracy'].append(accuracy_test)\n",
    "comparison_table['balance_accuracy'].append(balanced_accuracy_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# حذف الفراغات\n",
    "X_train_combined = X_train['question'].apply(remove_space) + ' ' + X_train['answer'].apply(remove_space)\n",
    "\n",
    "\n",
    "# print(len(X_train_without_links))\n",
    "print(len(X_train_combined))\n",
    "\n",
    "accuracy_test, accuracy_validation, balanced_accuracy_test = logistic_regression(X_train_combined,X_test_combined,X_validation_combined)\n",
    "\n",
    "comparison_table['question_step_number'].append(\"5.3\")\n",
    "comparison_table['model_name'].append('logistic_regression')\n",
    "comparison_table['features'].append('bag of words')\n",
    "comparison_table['model_parameters'].append('default')\n",
    "comparison_table['preprocessing_methods'].append(\"remove spaces with one space\")\n",
    "comparison_table['accuracy'].append(accuracy_test)\n",
    "comparison_table['balance_accuracy'].append(balanced_accuracy_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# حذف اختصاص الطبيب\n",
    "X_train_combined = X_train['question'].apply(remove_sentences_after_doctor) + ' ' + X_train['answer'].apply(remove_sentences_after_doctor)\n",
    "\n",
    "\n",
    "# print(len(X_train_without_links))\n",
    "print(len(X_train_combined))\n",
    "\n",
    "accuracy_test, accuracy_validation, balanced_accuracy_test = logistic_regression(X_train_combined,X_test_combined,X_validation_combined)\n",
    "\n",
    "comparison_table['question_step_number'].append(\"5.3\")\n",
    "comparison_table['model_name'].append('logistic_regression')\n",
    "comparison_table['features'].append('bag of words')\n",
    "comparison_table['model_parameters'].append('default')\n",
    "comparison_table['preprocessing_methods'].append(\"remove doctore specialization\")\n",
    "comparison_table['accuracy'].append(accuracy_test)\n",
    "comparison_table['balance_accuracy'].append(balanced_accuracy_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91088d7e7ab7f906",
   "metadata": {},
   "source": [
    "## [5.4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aae6614075309ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8de567c997ade2a8",
   "metadata": {},
   "source": [
    "## [5.5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer  = TfidfVectorizer(min_df=2,max_df=0.5,ngram_range=(1,2))\n",
    "train_data_X_train_combined = np.array(X_train_combined)\n",
    "train_data_X_test_combined = np.array(X_test_combined)\n",
    "train_data_X_validation_combined = np.array(X_validation_combined)\n",
    "tfidf_matrix_X_train_combined = tfidf_vectorizer .fit_transform(train_data_X_train_combined)\n",
    "tfidf_matrix_X_test_combined = tfidf_vectorizer .fit_transform(train_data_X_test_combined)\n",
    "tfidf_matrix_X_validation_combined = tfidf_vectorizer .fit_transform(train_data_X_validation_combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_test, accuracy_validation, balanced_accuracy_test = logistic_regression(tfidf_matrix_X_train_combined,tfidf_matrix_X_test_combined,tfidf_matrix_X_validation_combined)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = tfidf_vectorizer.transform(X_test)\n",
    "predicted_labels = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "964993c49f7a92a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ea32f682a2894679",
   "metadata": {},
   "source": [
    "## [5.6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27abe69834b04663",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "aff6ddfc1ceb5594",
   "metadata": {},
   "source": [
    "# Question [6]: Semantic Representation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a880357d5d916109",
   "metadata": {},
   "source": [
    "## [6.1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "931b87ba185753e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "da09109b1d0dea66",
   "metadata": {},
   "source": [
    "## [6.2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40ccfb829f657e4b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "62e9c6831c1fcf5b",
   "metadata": {},
   "source": [
    "## [6.3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80e0a31cf8caa789",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ff51d40919fe48e8",
   "metadata": {},
   "source": [
    "## [6.4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c7eb6c99e2df59d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c5f600c066a9f996",
   "metadata": {},
   "source": [
    "## [6.5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c815304128188bdc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "198b722c1bbcdcf5",
   "metadata": {},
   "source": [
    "# Question [7]: Deep Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aff5925a2f8f2a0",
   "metadata": {},
   "source": [
    "## [7.1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97f755371c085f16",
   "metadata": {},
   "source": [
    "### [7.1.1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6daf2e41c9731bb8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e30d9a52819bcc24",
   "metadata": {},
   "source": [
    "### [7.1.2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8be530a9b8e25845",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "248bd4a3e1cb55a6",
   "metadata": {},
   "source": [
    "## [7.2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26576136e27fd19c",
   "metadata": {},
   "source": [
    "### [7.2.1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf9b0d5ff2781c1b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1659f00477d637aa",
   "metadata": {},
   "source": [
    "### [7.2.2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84450bd910df78ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d846ec6bd7b7b91f",
   "metadata": {},
   "source": [
    "### [7.2.3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67f8cddd3a0aabc7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a5f15d18ac74bcc2",
   "metadata": {},
   "source": [
    "### [7.2.4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fdcd49751c5a953",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9d3f4520e4493202",
   "metadata": {},
   "source": [
    "## [7.3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b878b51740738504",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "52a6d74027ab71b",
   "metadata": {},
   "source": [
    "## Extra [7.4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32b1af7e548effea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d4c5fcdd614e46d",
   "metadata": {},
   "source": [
    "# Final Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42a27f608e5a696b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(comparison_table)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b30ba7f2c380c036",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"[your_name].csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "KFerFDFqPwPI",
    "-4LEgCtvPwaQ",
    "ZgzDVRqCQ72Q",
    "qQlGbCI3Q77R",
    "hIwnQVuas5hE",
    "7orTRN5vs9vp",
    "cwaCNfS9RNZc",
    "MbJeF_CxRNZd",
    "GY-HhD2YRNZe",
    "KV9D86kbRNZe",
    "3fM_FPldSCxp",
    "N2z1KmQXSAFs",
    "fCBHc2y6SAL5",
    "P1aLjPbzRNZe",
    "dbqN7BceSRhS",
    "rad8wPeSSRhU",
    "FbIUIkezSRhU",
    "N4WDktTgRNZf",
    "wbGPWNtoR4p-",
    "V-rCzzoMR41T",
    "iXF2L4Z-Sd3N",
    "ij5gdJA4Sd3O",
    "H6k8JnJDSd3P",
    "1TmDb343Sls9",
    "0V5LD9EWVgY7",
    "vc96CuQStFA9",
    "exdhuH6GtLez",
    "pwtVy8DES8AZ",
    "npObP2JLUBgw",
    "pTIrTECWUBg5",
    "6v0njm5qUBg6"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
