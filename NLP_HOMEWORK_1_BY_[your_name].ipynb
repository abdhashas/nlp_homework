{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8c38a02a9f45259b",
   "metadata": {
    "id": "46f519c8"
   },
   "source": [
    "#[Your_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a17096249196e35c",
   "metadata": {},
   "source": [
    "<h1 style=\"text-align: center;\">Text Classification<h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67a8f525e7cb9573",
   "metadata": {
    "id": "e7efe140"
   },
   "source": [
    "Prepare libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "33a20126e2ce399e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-28T18:00:40.781684600Z",
     "start_time": "2023-11-28T18:00:27.949446900Z"
    },
    "id": "af6058ed"
   },
   "outputs": [],
   "source": [
    "# here put every import you need e.g. import nltk\n",
    "# it's better to load what you need from the package by from [] import [] instead of import the whole package\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from keras.models import Sequential\n",
    "from keras import layers\n",
    "from keras.layers import Dense, Conv1D, AveragePooling1D, Flatten, Dropout\n",
    "from keras.utils import to_categorical\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "import re\n",
    "import numpy as np\n",
    "from cleantext import clean\n",
    "from arabicstopwords.stopwords_lexicon import stopwords_lexicon\n",
    "import arabicstopwords.arabicstopwords as stop_words\n",
    "import nltk\n",
    "from nltk.stem.isri import ISRIStemmer\n",
    "from wordcloud import WordCloud\n",
    "from arabic_reshaper import arabic_reshaper\n",
    "from bidi.algorithm import get_display\n",
    "from nltk import bigrams, trigrams\n",
    "from collections import Counter\n",
    "from snowballstemmer import stemmer\n",
    "from tashaphyne.stemming import ArabicLightStemmer\n",
    "from farasa.segmenter import FarasaSegmenter\n",
    "from farasa.stemmer import FarasaStemmer\n",
    "from matplotlib.font_manager import FontProperties\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, balanced_accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import tensorflow as tf\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35983a59a0a8dc81",
   "metadata": {},
   "source": [
    "download "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a355ad265838eb2",
   "metadata": {
    "id": "229df102"
   },
   "source": [
    "Download data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "cf91803826981217",
   "metadata": {
    "id": "5504e7ee"
   },
   "outputs": [],
   "source": [
    "# !wget 'https://drive.google.com/uc?export=download&id=1cMSjxa3nA706LIZDEhwMpaVRMY2IX9P0' -O 'data.zip'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "53e4574cd5ad3978",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# !pip install Arabic-Stopwords\n",
    "# 2.2 nltk.download('punkt') # open vpn\n",
    "# 2.3 !pip install arabic-reshaper\n",
    "# 2.4 nltk.download('stopwords')\n",
    "# 3.8 !pip install snowballstemmer\n",
    "# 3.8 !pip install Tashaphyne\n",
    "# 3.8 !pip install -U farasapy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "f5f4105090de8887",
   "metadata": {
    "id": "530e1bac"
   },
   "outputs": [],
   "source": [
    "# !unzip data.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "687ae8495df53493",
   "metadata": {},
   "source": [
    "<h2 dir=\"rtl\">مثال عن كيفية تنظيف حلول الطلبات:</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c6f6c78922eb16d",
   "metadata": {
    "id": "8E7xa9XLPW8t"
   },
   "source": [
    "\n",
    "<div dir=\"rtl\">شرح ما يقوم به الكود (like code documentation)<div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "3a9387562e3e7fcf",
   "metadata": {
    "id": "d41a9612"
   },
   "outputs": [],
   "source": [
    "# your code here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "87f6b7e34a75e652",
   "metadata": {
    "id": "aNNrMjaaQanf"
   },
   "outputs": [],
   "source": [
    "# example test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ca50c08527ef0ca",
   "metadata": {
    "id": "d9NfyVxZQXF7"
   },
   "source": [
    "<div dir=\"rtl\">ملاحظاتك في حال وجودها</div>\n",
    "<div dir=\"rtl\">يمكنك إضافة خلايا لكل طلب بقدر ما تشاء، المهم أن تحافظ على تنظيم الملف</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89b0f65e02a313cd",
   "metadata": {
    "id": "3400638b"
   },
   "source": [
    "# Question [1]: Load the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d12bfd6042d97cae",
   "metadata": {},
   "source": [
    "## [1.1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "f6397927bb088417",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-28T17:47:44.500683400Z",
     "start_time": "2023-11-28T17:47:43.519931600Z"
    }
   },
   "outputs": [],
   "source": [
    "train_file_path = 'train.csv'\n",
    "test_file_path = 'test.csv'\n",
    "validation_file_path = 'val.csv'\n",
    "train_data = pd.read_csv(train_file_path)\n",
    "test_data = pd.read_csv(test_file_path)\n",
    "validation_data = pd.read_csv(validation_file_path)\n",
    "\n",
    "# print(\"number of data in train data : \",len(train_data))\n",
    "# print(\"number of data in test data : \",len(test_data))\n",
    "# print(\"number of data in validation data : \",len(validation_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "bd4ed1d4d439b324",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-28T17:47:47.493979600Z",
     "start_time": "2023-11-28T17:47:47.408587600Z"
    }
   },
   "outputs": [],
   "source": [
    "# train_data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "c285a9642f16e9ed",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-28T17:47:53.408385500Z",
     "start_time": "2023-11-28T17:47:53.337280500Z"
    }
   },
   "outputs": [],
   "source": [
    "# test_data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "647198775377a576",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-28T17:47:54.293475800Z",
     "start_time": "2023-11-28T17:47:54.111683600Z"
    }
   },
   "outputs": [],
   "source": [
    "# validation_data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "f902f028f4f22982",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-28T17:47:55.207559400Z",
     "start_time": "2023-11-28T17:47:54.981391300Z"
    }
   },
   "outputs": [],
   "source": [
    "#drop_duplicates يزيل الصفوف المتكرره\n",
    "#dropna يحذف الصفوف المكررة\n",
    "test_data=test_data.drop_duplicates().dropna()\n",
    "train_data=train_data.drop_duplicates().dropna()\n",
    "validation_data=validation_data.drop_duplicates().dropna()\n",
    "# print(\"number of data in train data after cleaning : \",len(train_data))\n",
    "# print(\"number of data in test data after cleaning : \",len(test_data))\n",
    "# print(\"number of data in validation data after cleaning : \",len(validation_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "300d578799370a2e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-28T17:47:56.205456800Z",
     "start_time": "2023-11-28T17:47:56.151617600Z"
    }
   },
   "outputs": [],
   "source": [
    "# train_data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "bb28317493f6f459",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-28T17:47:57.140122300Z",
     "start_time": "2023-11-28T17:47:56.924769600Z"
    }
   },
   "outputs": [],
   "source": [
    "# test_data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "b88695c3a0a3c37e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-28T17:47:57.993922400Z",
     "start_time": "2023-11-28T17:47:57.693952400Z"
    }
   },
   "outputs": [],
   "source": [
    "# validation_data.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe89dfbbc315bb24",
   "metadata": {},
   "source": [
    "## [1.2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "128745bced93a650",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-28T17:48:00.089441800Z",
     "start_time": "2023-11-28T17:47:58.657780700Z"
    }
   },
   "outputs": [],
   "source": [
    "def unify_specialty(name):\n",
    "    name = re.sub(r\"[^\\w\\s]\", \" \", name)  # يزيل الرموز الترقيمية\n",
    "    name = re.sub(r\"\\s+\", \" \", name).strip()  # يزيل الفراغات المتكرره #strip يزيل الفراغات يلي بالبداية والنهاية\n",
    "    name =re.sub(\"[\\_\\-]\",\" \",name)\n",
    "    return name\n",
    "\n",
    "train_data['label']=train_data['label'].apply(unify_specialty)\n",
    "test_data['label']=test_data['label'].apply(unify_specialty)\n",
    "validation_data['label']=validation_data['label'].apply(unify_specialty)   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "1b08e09e599e7d08",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-28T17:48:01.035454700Z",
     "start_time": "2023-11-28T17:47:58.909457900Z"
    }
   },
   "outputs": [],
   "source": [
    "# x=set(train_data['label'])\n",
    "# x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "c3686ec8976508a8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-28T17:48:02.013238200Z",
     "start_time": "2023-11-28T17:47:59.463844100Z"
    }
   },
   "outputs": [],
   "source": [
    "Dictionary_of_diseases= {\n",
    "    'أمراض الجهاز التنفسي': 'أمراض الجهاز التنفسي',\n",
    "    'الجهاز التنفسي': 'أمراض الجهاز التنفسي',\n",
    "    'امراض الجهاز التنفسي': 'أمراض الجهاز التنفسي',\n",
    "    'أمراض الدم': 'أمراض الدم',\n",
    "    'الدم': 'أمراض الدم',\n",
    "    'امراض الدم': 'أمراض الدم',\n",
    "    'أمراض الغدد الصماء': 'أمراض الغدد الصماء',\n",
    "    'الغدد الصماء': 'أمراض الغدد الصماء',\n",
    "    'امراض الغدد الصماء': 'أمراض الغدد الصماء',\n",
    "    'مرض السكري': 'مرض السكري',\n",
    "    'السكري': 'مرض السكري',\n",
    "    'الاورام الخبيثة والحميدة':'الأورام الخبيثة والحميدة'\n",
    "\n",
    "}\n",
    "\n",
    "def Unification_name_of_diseases(c):\n",
    "    c=c.map(Dictionary_of_diseases).fillna(c)\n",
    "    return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "4fd4c2952b21be5d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-28T17:48:03.134507400Z",
     "start_time": "2023-11-28T17:47:59.494860300Z"
    }
   },
   "outputs": [],
   "source": [
    "train_data['label']=Unification_name_of_diseases(train_data['label'])\n",
    "test_data['label']=Unification_name_of_diseases(test_data['label'])\n",
    "validation_data['label']=Unification_name_of_diseases(validation_data['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "f86de893df69c13",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-28T17:48:03.192630300Z",
     "start_time": "2023-11-28T17:47:59.950281200Z"
    }
   },
   "outputs": [],
   "source": [
    "# x=set(train_data['label'])\n",
    "# x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2de100ffb45a290",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-28T17:48:03.684914300Z",
     "start_time": "2023-11-28T17:48:00.006191700Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4a8d8362bec9237d",
   "metadata": {
    "id": "2f22713d"
   },
   "source": [
    "# Question [2]: Text Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3abc352213c467a",
   "metadata": {
    "id": "03ef90d1"
   },
   "source": [
    "## [2.1] \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "347cfd18fdd3ae19",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-28T17:53:26.541719300Z",
     "start_time": "2023-11-28T17:53:26.526459900Z"
    }
   },
   "outputs": [],
   "source": [
    "# train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "cac8463d6c28b1d3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-28T17:48:03.842803200Z",
     "start_time": "2023-11-28T17:48:01.020927500Z"
    },
    "id": "c1b8c64b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "عدد الداتا الخاصة بأمراض الدم: 1398\n",
      "عدد الداتا الخاصة بأمراض الجهاز التنفسي: 3676\n",
      "عدد الداتا الخاصة بأمراض الغدد الصماء: 5752\n",
      "عدد الداتا الخاصة بأمراض ارتفاع ضغط الدم: 2537\n",
      "عدد الداتا الخاصة بالأورام الخبيثة والحميدة: 6449\n",
      "عدد الداتا الخاصة بجراحةالعظام: 2211\n",
      "عدد الداتا الخاصة بالجراحة العامة: 6307\n",
      "عدد الداتا الخاصة بمرض السكري: 4268\n"
     ]
    }
   ],
   "source": [
    "# Train data\n",
    "\n",
    "# عدد الأجوبة الخاصة بأمراض الدم :\n",
    "answers_len_in_blood_diseases = len(train_data[train_data['label']=='أمراض الدم']['answer'].tolist())\n",
    "# print(answers_len_in_blood_diseases)\n",
    "\n",
    "# عدد الأسئلة الخاصة بأمراض الدم :\n",
    "questions_len_in_blood_diseases = len(train_data[train_data['label']=='أمراض الدم']['question'].tolist())\n",
    "# print(questions_len_in_blood_diseases)\n",
    "\n",
    "\n",
    "# عدد الداتا الخاصة بأمراض الدم :\n",
    "total_blood_diseases_len = len(train_data[train_data['label']=='أمراض الدم'])\n",
    "print('عدد الداتا الخاصة بأمراض الدم:',total_blood_diseases_len)\n",
    "\n",
    "# عدد الداتا في أمراض الجهاز التنفسي :\n",
    "total_blood_diseases_len = len(train_data[train_data['label']=='أمراض الجهاز التنفسي'])\n",
    "print('عدد الداتا الخاصة بأمراض الجهاز التنفسي:',total_blood_diseases_len)\n",
    "\n",
    "# غدد الداتا في أمراض الغدد الصماء :\n",
    "total_blood_diseases_len = len(train_data[train_data['label']=='أمراض الغدد الصماء'])\n",
    "print('عدد الداتا الخاصة بأمراض الغدد الصماء:',total_blood_diseases_len)\n",
    "\n",
    "# عدد  الداتا ارتفاع ضغط الدم :\n",
    "total_blood_diseases_len = len(train_data[train_data['label']=='ارتفاع ضغط الدم'])\n",
    "print('عدد الداتا الخاصة بأمراض ارتفاع ضغط الدم:',total_blood_diseases_len)\n",
    "\n",
    "# عدد الداتا في الأورام الخبيثة والحميدة :\n",
    "total_blood_diseases_len = len(train_data[train_data['label']=='الأورام الخبيثة والحميدة'])\n",
    "print('عدد الداتا الخاصة بالأورام الخبيثة والحميدة:',total_blood_diseases_len)\n",
    "\n",
    "# عدد الداتا في جراحة العظام\n",
    "total_blood_diseases_len = len(train_data[train_data['label']=='جراحة العظام'])\n",
    "print('عدد الداتا الخاصة بجراحةالعظام:',total_blood_diseases_len)\n",
    "\n",
    "# عدد الداتا في الجراحة العامة\n",
    "total_blood_diseases_len = len(train_data[train_data['label']=='جراحة عامة'])\n",
    "print('عدد الداتا الخاصة بالجراحة العامة:',total_blood_diseases_len)\n",
    "\n",
    "# عدد الداتا في مرض السكري\n",
    "total_blood_diseases_len = len(train_data[train_data['label']=='مرض السكري'])\n",
    "print('عدد الداتا الخاصة بمرض السكري:',total_blood_diseases_len)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "10e8fc4ed5f610f6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-28T17:48:03.906972Z",
     "start_time": "2023-11-28T17:48:01.470011400Z"
    }
   },
   "outputs": [],
   "source": [
    "# # Drawing pie chart:\n",
    "# # ملاحظة بالعربي طلع الخط مفشكل ف تمت الكتابة بالانكليزي\n",
    "\n",
    "\n",
    "\n",
    "# data = {\n",
    "#     'الاختصاص': ['أمراض الدم', 'أمراض الجهاز التنفسي', 'أمراض الغدد الصماء','أمراض ارتفاع ضغط الدم','الأمراض الخبيثة والحميدة','جراحةالعظام','الجراحة العامة','مرض السكري'],\n",
    "#     'عدد النصوص': [2153, 6002, 9417,4190,10711,2641,10548,7096]\n",
    "# }\n",
    "\n",
    "# data\n",
    "\n",
    "\n",
    "# df = pd.DataFrame(data)\n",
    "\n",
    "# # print(df)\n",
    "# arabic_font = FontProperties(fname='font/NotoNaskhArabic-VariableFont_wght.ttf') \n",
    "\n",
    "# df['الاختصاص'] = [get_display(arabic_reshaper.reshape(label)) for label in df['الاختصاص']]\n",
    "# plt.figure(figsize=(8, 8))\n",
    "# plt.pie(df['عدد النصوص'], labels=df['الاختصاص'], autopct='%1.1f%%', startangle=140)\n",
    "# plt.title(get_display(arabic_reshaper.reshape('توزيع النصوص حسب الاختصاص')), fontproperties=arabic_font)\n",
    "# plt.show()\n",
    "\n",
    "# # import pandas as pd\n",
    "# # import matplotlib.pyplot as plt\n",
    "\n",
    "# # data = {\n",
    "# #     'Specialization': ['Blood Diseases', 'Respiratory System Diseases', 'Endocrine Diseases', 'Hypertension', 'Malignant and Benign Diseases', 'Orthopedic Surgery', 'General Surgery', 'Diabetes'],\n",
    "# #     'Number of Texts': [1398, 3676, 5752, 2537, 6449, 2211, 6307, 4268]\n",
    "# # }\n",
    "\n",
    "# # df = pd.DataFrame(data)\n",
    "\n",
    "# # print(df)\n",
    "\n",
    "# # plt.figure(figsize=(8, 8))\n",
    "# # plt.pie(df['Number of Texts'], labels=df['Specialization'], autopct='%1.1f%%', startangle=140)\n",
    "# # plt.title('Distribution of Texts by Specialization')\n",
    "# # plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7b6917b7ad847bb",
   "metadata": {
    "id": "24164796"
   },
   "source": [
    "## [2.2] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "af188fd4f58322bb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-28T17:48:03.906972Z",
     "start_time": "2023-11-28T17:48:01.695343300Z"
    }
   },
   "outputs": [],
   "source": [
    "def delete_repeated_letter(text):\n",
    "    # cleaned_text = re.sub(r'(.)\\1{2,}', r'\\1', text)\n",
    "    cleaned_text = re.sub(r'([^\\w\\s\\.])\\1+', r'\\1', text)\n",
    "    cleaned_text = re.sub(r'(\\.\\s*)\\1+$', r'\\1', cleaned_text)\n",
    "    return cleaned_text\n",
    "\n",
    "def handle_connected_words(tokenized):\n",
    "    # Handle connected words with '-'\n",
    "    result_tokens = []\n",
    "    for token in tokenized:\n",
    "        # Split connected words with '-'\n",
    "        result_tokens.extend(token.split('-'))\n",
    "    return result_tokens\n",
    "\n",
    "\n",
    "def filter_text(list):\n",
    "    list_of_phrases =[]\n",
    "    all_tokens = []\n",
    "    for sentence in list:\n",
    "        tokenizer = RegexpTokenizer(r'\\b\\d+\\b|\\b[^\\d\\W_]{2,}\\b|[^\\d\\W_]+(?:-[^\\d\\W_]+)?|\\S')\n",
    "        tokenized = tokenizer.tokenize(delete_repeated_letter(sentence))\n",
    "        tokenized = handle_connected_words(tokenized)\n",
    "\n",
    "        # Remove '-' and '/' and one-letter words from each token\n",
    "        # cleaned_tokens = [token for token in tokenized if len(token) > 1 and token not in ['-', '/']]\n",
    "        # cleaned_tokens = [token if len(token) > 1 or token == 'و' else '' for token in tokenized if token not in ['-', '/']]\n",
    "\n",
    "        cleaned_tokens = [token if (token.isalpha() and len(token) > 1) or token == 'و' else '' for token in tokenized if token not in ['-', '/']]\n",
    "\n",
    "        combined_phrase = ' '.join(cleaned_tokens)\n",
    "        list_of_phrases.append(combined_phrase)\n",
    "\n",
    "    for phrase in list_of_phrases:\n",
    "        tokens = nltk.word_tokenize(phrase)\n",
    "        all_tokens.append(tokens)\n",
    "        \n",
    "    return all_tokens\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "8841e5fae4cd12eb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-28T17:48:11.547801700Z",
     "start_time": "2023-11-28T17:48:02.015744800Z"
    },
    "id": "SexYXq1-P0Y0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1645245\n"
     ]
    }
   ],
   "source": [
    "train_data_answers_list = train_data['answer'].values\n",
    "answers = filter_text(train_data_answers_list)\n",
    "flattened_answers = [item for sublist in answers for item in sublist]\n",
    "print(len(flattened_answers))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "462dd7146ffac428",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-28T17:48:17.557564200Z",
     "start_time": "2023-11-28T17:48:11.659377700Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "801979\n"
     ]
    }
   ],
   "source": [
    "train_data_questions_list = train_data['question'].values\n",
    "questions = filter_text(train_data_questions_list)\n",
    "flattened_quesions = [item for sublist in questions for item in sublist]\n",
    "print(len(flattened_quesions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "a05d77a67ff63ee6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-28T17:48:17.560083300Z",
     "start_time": "2023-11-28T17:48:17.544042400Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2447224\n"
     ]
    }
   ],
   "source": [
    "# حساب عدد الكلمات الكلي :\n",
    "total_words_len = len(flattened_quesions) + len(flattened_answers)\n",
    "print(total_words_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "c896baa27e7c92a5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-28T17:48:17.874055600Z",
     "start_time": "2023-11-28T17:48:17.552562600Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "عدد الكلمات الفريدة: 94614\n"
     ]
    }
   ],
   "source": [
    "# حساب عدد الكلمات الفريدة :\n",
    "total_unique_words=len(set(flattened_quesions+flattened_answers))\n",
    "print(\"عدد الكلمات الفريدة:\", total_unique_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c25bc0c76cd00150",
   "metadata": {
    "id": "KFerFDFqPwPI"
   },
   "source": [
    "## [2.3] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "ec6d7dbea8e44a8b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-28T17:49:35.324646100Z",
     "start_time": "2023-11-28T17:48:17.867536700Z"
    }
   },
   "outputs": [],
   "source": [
    "# WCD=dict()\n",
    "# train_data_answers_list = flattened_answers\n",
    "# train_data_questions_list = flattened_quesions\n",
    "# train_data_list=train_data_answers_list+train_data_questions_list\n",
    "# answers_text = ' '.join(train_data_list)\n",
    "# reshaped_text = arabic_reshaper.reshape(answers_text)\n",
    "# arabic_text = get_display(reshaped_text)\n",
    "# wordcloud = WordCloud(font_path='font/NotoNaskhArabic-VariableFont_wght.ttf', background_color='white').generate(arabic_text)\n",
    "# WCD=wordcloud.words_\n",
    "# plt.figure(figsize=(50, 50))\n",
    "# plt.imshow(wordcloud, interpolation='bilinear')\n",
    "# plt.axis(\"off\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "420df21aa7c46632",
   "metadata": {
    "id": "-4LEgCtvPwaQ"
   },
   "source": [
    "## [2.4] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "27677a3f844deab0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-28T17:49:35.421653500Z",
     "start_time": "2023-11-28T17:49:35.383741100Z"
    },
    "id": "d81d9f99"
   },
   "outputs": [],
   "source": [
    "def tokenized_text(text):\n",
    "    # allWords = nltk.tokenize.word_tokenize(text)\n",
    "\n",
    "    allWordDist = nltk.FreqDist(w for w in text)\n",
    "    stopwords = nltk.corpus.stopwords.words('english')\n",
    "    allWordExceptStopDist = nltk.FreqDist(w.lower() for w in text if w not in stopwords)  \n",
    "    mostCommon= allWordDist.most_common(15)\n",
    "    print('أكثر 15 كلمة مكررة:',mostCommon)\n",
    "    leastCommon = allWordDist.most_common()[:-11:-1]\n",
    "    print('أقل 10 كلمة مكررة:', leastCommon)\n",
    "    bigram_freq = nltk.FreqDist(bigrams(text))\n",
    "    most_common_bigrams = bigram_freq.most_common(10)\n",
    "    print('أكثر 10 ثنائيات مكررة في النص:',most_common_bigrams)\n",
    "    trigram_freq = nltk.FreqDist(trigrams(text))\n",
    "    most_common_trigrams = trigram_freq.most_common(10)\n",
    "    print('أكثر 10 ثلاثيات كلمات مكررة في النص:',most_common_trigrams)\n",
    "    collocations_bigram = nltk.collocations.BigramCollocationFinder.from_words(text)\n",
    "    most_common_collocations_bigram = collocations_bigram.nbest(nltk.collocations.BigramAssocMeasures.likelihood_ratio, 10)\n",
    "    print('أكثر 10 ثنائيات كلمات مهمة في النص:',most_common_collocations_bigram)\n",
    "    collocations_trigram = nltk.collocations.TrigramCollocationFinder.from_words(text)\n",
    "    most_common_collocations_trigram = collocations_trigram.nbest(nltk.collocations.TrigramAssocMeasures.likelihood_ratio, 10)\n",
    "    print('أكثر 10 ثلاثيات كلمات مهمة في النص:',most_common_collocations_trigram)  \n",
    "    print('========================================================================================================================================================================================')  \n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "# tokenized_text('tony botros is tony the tony best best best tony botros in in in the whole world tony botros')\n",
    "# tokenized_text(['طوني','طوني','بطرس','هو','طوني','هو','بطرس','ال','الأفضل'])\n",
    "# tokenized_text(['لدي', 'جرح', 'فوق', 'حاجبي', 'و', 'تمت', 'عملية', 'الخياطة', 'ثم', 'نزع', 'الغرزات', 'بشكل', 'عادي', 'لكن', 'مر', 'اسبوع', 'على', 'ذلك', 'و', 'مازال', 'هناك', 'الم', 'و', 'تطلب', 'شديد', 'في', 'المنطقة', 'لدرجة', 'اني', 'لا', 'استطيع', 'رفع', 'حاجبي', 'ابدا', 'إضافة', 'لوجود', 'انتفاخ', 'هل', 'هدا', 'طبيعي', 'ام', 'يمكن', 'ان', 'يكون', 'خلل', 'و', 'إن', 'كان', 'كذلك', 'ما', 'هو'])\n",
    "# tokenized_text(answers[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "8bfa567330bbb43b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-28T17:49:35.421653500Z",
     "start_time": "2023-11-28T17:49:35.392541500Z"
    }
   },
   "outputs": [],
   "source": [
    "#الشرح"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "2e3f5f8258e84500",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-28T17:49:35.450007300Z",
     "start_time": "2023-11-28T17:49:35.403464800Z"
    }
   },
   "outputs": [],
   "source": [
    "# هاد التابع لمرق كل اختصاص وجيب منو الكلمات كلا متل مو مطلوب ب 2.4\n",
    "def all_tokinized_text(specialized):\n",
    "    filtered_questions = filter_text(train_data[train_data['label']==specialized]['question'].values)\n",
    "    filtered_answers = filter_text(train_data[train_data['label']==specialized]['answer'].values)\n",
    "    list = filtered_questions + filtered_answers\n",
    "    flattened_list = [item for sublist in list for item in sublist]\n",
    "    # flattened_list\n",
    "    tokenized_text(flattened_list)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "d616630beb444368",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-28T17:49:41.895942500Z",
     "start_time": "2023-11-28T17:49:35.411488100Z"
    }
   },
   "outputs": [],
   "source": [
    "# all_tokinized_text('أمراض الدم')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "2c5ef7cc8cf9dd59",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-28T17:49:56.166864500Z",
     "start_time": "2023-11-28T17:49:41.924108500Z"
    }
   },
   "outputs": [],
   "source": [
    "# all_tokinized_text('أمراض الجهاز التنفسي')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "e861c909e1a23012",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-28T17:50:15.625987600Z",
     "start_time": "2023-11-28T17:49:56.158856Z"
    }
   },
   "outputs": [],
   "source": [
    "# all_tokinized_text('أمراض الغدد الصماء')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "8a54f923b682dbb5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-28T17:50:25.137230300Z",
     "start_time": "2023-11-28T17:50:15.626987300Z"
    }
   },
   "outputs": [],
   "source": [
    "# all_tokinized_text('ارتفاع ضغط الدم')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "4dc049d6662db815",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-28T17:50:47.456008500Z",
     "start_time": "2023-11-28T17:50:25.136228500Z"
    }
   },
   "outputs": [],
   "source": [
    "# all_tokinized_text('الأورام الخبيثة والحميدة')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "a450181732413cb3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-28T17:50:54.892514800Z",
     "start_time": "2023-11-28T17:50:47.456008500Z"
    }
   },
   "outputs": [],
   "source": [
    "# all_tokinized_text('جراحة العظام')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "4b5d01e09d1a8bff",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-28T17:51:15.432650500Z",
     "start_time": "2023-11-28T17:50:54.885006900Z"
    }
   },
   "outputs": [],
   "source": [
    "# all_tokinized_text('جراحة عامة')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "5c2d128fa7313066",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-28T17:51:29.530449900Z",
     "start_time": "2023-11-28T17:51:15.410544600Z"
    }
   },
   "outputs": [],
   "source": [
    "# all_tokinized_text('مرض السكري')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "c5a9bfb6e9d84e64",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-28T17:53:16.009343700Z",
     "start_time": "2023-11-28T17:51:29.526942900Z"
    }
   },
   "outputs": [],
   "source": [
    "# # على كل الداتا تبع الترين:\n",
    "# filtered_questions = filter_text(train_data['question'].values)\n",
    "# filtered_answers = filter_text(train_data['answer'].values)\n",
    "# list = filtered_questions + filtered_answers\n",
    "# flattened_list = [item for sublist in list for item in sublist]\n",
    "# flattened_list\n",
    "# tokenized_text(flattened_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "a47127af0446c39b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-28T17:53:16.015367500Z",
     "start_time": "2023-11-28T17:53:16.007830600Z"
    }
   },
   "outputs": [],
   "source": [
    "#الشرح"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9c557f09745b419",
   "metadata": {
    "id": "ZgzDVRqCQ72Q"
   },
   "source": [
    "## [2.5] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "9c774bae3acab54c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-28T17:53:23.740569800Z",
     "start_time": "2023-11-28T17:53:16.012342800Z"
    }
   },
   "outputs": [],
   "source": [
    "train_data['all_word']=train_data['answer']+train_data['question']\n",
    "train_data_list=train_data['all_word'].values\n",
    "all_word = filter_text(train_data_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "50d09f794f36668b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-28T17:53:23.849212500Z",
     "start_time": "2023-11-28T17:53:23.741569700Z"
    }
   },
   "outputs": [],
   "source": [
    "flattened_all_word = [item for sublist in all_word for item in sublist]\n",
    "train_data['length_of_sentinse']=train_data['all_word'].apply(len)\n",
    "# train_data['length_of_sentinse']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "f9f823f086995cf3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-28T17:53:24.518763200Z",
     "start_time": "2023-11-28T17:53:23.851207500Z"
    }
   },
   "outputs": [],
   "source": [
    "w=[]\n",
    "for i in all_word:\n",
    "    w.append(set(i))\n",
    "train_data['unique_words']=w\n",
    "train_data['number_of_unique_words']=train_data['unique_words'].apply(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "c743371b1d15adb6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-28T17:53:24.524456700Z",
     "start_time": "2023-11-28T17:53:24.521451100Z"
    },
    "id": "3WFp0g3DQ72V"
   },
   "outputs": [],
   "source": [
    "def histogram(length_of_sentinse,number_of_unique_words):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.subplot(2, 1, 1)\n",
    "    # plt.bar(number_of_unique_words, length_of_sentinse, color='blue', alpha=0.7)\n",
    "    plt.hist(length_of_sentinse, label='length_of_sentinse',edgecolor='black', linewidth=1.2,bins=30, alpha=0.5)\n",
    "    plt.hist(number_of_unique_words, label='number_of_unique_words',edgecolor='black', linewidth=1.2,bins=30, alpha=0.5)\n",
    "    plt.title('histogram')\n",
    "    plt.xlabel('length_of_sentinse')\n",
    "    plt.ylabel('number_of_unique_words')\n",
    "    # plt.tight_layout()\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "bf894bf5f98822c4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-28T17:53:24.747773900Z",
     "start_time": "2023-11-28T17:53:24.525460100Z"
    }
   },
   "outputs": [],
   "source": [
    "# لكل النص\n",
    "# histogram(train_data['length_of_sentinse'],train_data['number_of_unique_words'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "674223e30b687e8a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-28T17:53:24.966337700Z",
     "start_time": "2023-11-28T17:53:24.746176900Z"
    }
   },
   "outputs": [],
   "source": [
    "# # أمراض الجهاز التنفسي\n",
    "# histogram(train_data[train_data['label']=='أمراض الجهاز التنفسي']['length_of_sentinse'],train_data[train_data['label']=='أمراض الجهاز التنفسي']['number_of_unique_words'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "3c448c8ccbd1775a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-28T17:53:25.179734200Z",
     "start_time": "2023-11-28T17:53:24.963334300Z"
    }
   },
   "outputs": [],
   "source": [
    "# # أمراض الدم\n",
    "# histogram(train_data[train_data['label']=='أمراض الدم']['length_of_sentinse'],train_data[train_data['label']=='أمراض الدم']['number_of_unique_words'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "7fe7cc50508fb9b2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-28T17:53:25.398841600Z",
     "start_time": "2023-11-28T17:53:25.178733400Z"
    }
   },
   "outputs": [],
   "source": [
    "# # أمراض الغدد الصماء\n",
    "# histogram(train_data[train_data['label']=='أمراض الغدد الصماء']['length_of_sentinse'],train_data[train_data['label']=='أمراض الغدد الصماء']['number_of_unique_words'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "9f90261cce6f277d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-28T17:53:25.625906200Z",
     "start_time": "2023-11-28T17:53:25.398841600Z"
    }
   },
   "outputs": [],
   "source": [
    "# # ارتفاع ضغط الدم\n",
    "# histogram(train_data[train_data['label']=='ارتفاع ضغط الدم']['length_of_sentinse'],train_data[train_data['label']=='ارتفاع ضغط الدم']['number_of_unique_words'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "30d9687be40b9584",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-28T17:53:25.854342800Z",
     "start_time": "2023-11-28T17:53:25.629420Z"
    }
   },
   "outputs": [],
   "source": [
    "# # الأورام الخبيثة والحميدة\n",
    "# histogram(train_data[train_data['label']=='الأورام الخبيثة والحميدة']['length_of_sentinse'],train_data[train_data['label']=='الأورام الخبيثة والحميدة']['number_of_unique_words'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "faa22e8858b487d7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-28T17:53:26.064133700Z",
     "start_time": "2023-11-28T17:53:25.853339600Z"
    }
   },
   "outputs": [],
   "source": [
    "# # جراحة العظام\n",
    "# histogram(train_data[train_data['label']=='جراحة العظام']['length_of_sentinse'],train_data[train_data['label']=='جراحة العظام']['number_of_unique_words'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "84a6d01ad78172f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-28T17:53:26.289134Z",
     "start_time": "2023-11-28T17:53:26.063132100Z"
    }
   },
   "outputs": [],
   "source": [
    "# # جراحة عامة\n",
    "# histogram(train_data[train_data['label']=='جراحة عامة']['length_of_sentinse'],train_data[train_data['label']=='جراحة عامة']['number_of_unique_words'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "a6c4311d2ebf74ab",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-28T17:53:26.517446500Z",
     "start_time": "2023-11-28T17:53:26.291149400Z"
    }
   },
   "outputs": [],
   "source": [
    "# # مرض السكري\n",
    "# histogram(train_data[train_data['label']=='مرض السكري']['length_of_sentinse'],train_data[train_data['label']=='مرض السكري']['number_of_unique_words'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1835943bcbf4693",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-28T17:53:26.519451500Z",
     "start_time": "2023-11-28T17:53:26.515428200Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "eaf1b8732c7316e1",
   "metadata": {
    "id": "oFmJ2HWqRNZa"
   },
   "source": [
    "# Question [3]: Text Cleaning and Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68fbb0f8741051d0",
   "metadata": {
    "id": "cwaCNfS9RNZc"
   },
   "source": [
    "## [3.1] \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "274c297e31ec5bf",
   "metadata": {
    "id": "LhllhojDRNZd",
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "# train_data\n",
    "# clean_tweet = re.sub(r'^RT(\\s)+|https?\\S+|#|@\\S+', '', tweet) ## من أجل أزالت \n",
    "#r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+'\n",
    "def remove_links(text):\n",
    "    clean_text = re.sub(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', '', text)\n",
    "    return clean_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove_links('https://chat.openai.com/c/dc21a08a-5be6-42e3-a6e4-0c1d26791a46 hi')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c720cae3191f34c8",
   "metadata": {
    "id": "MbJeF_CxRNZd"
   },
   "source": [
    "## [3.2] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def remove_doctors_prefix(text):\n",
    "    pattern = r'\\b(?:الدكتور(?:ة)?)\\s+(?:الصيدلاني(?:ة)?)?\\s*(?:([\\u0600-\\u06FF]+)\\s*[\\u0600-\\u06FF]+\\s*([\\u0600-\\u06FF]+))\\b'\n",
    "\n",
    "    def replacer(match):\n",
    "        full_name = match.group(1)\n",
    "        first_name = full_name.split()[0]\n",
    "        return f'د.{first_name}'\n",
    "\n",
    "    result = re.sub(pattern, replacer, text)\n",
    "\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = train_data[train_data['label']=='أمراض الدم']['answer']\n",
    "# texts.apply(remove_doctors_prefix)[0]\n",
    "# for text in texts[:5]:\n",
    "#     # print(text)\n",
    "#     print(remove_doctors_prefix(text))\n",
    "#     print(\"=================================================================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "774c28db38969b02",
   "metadata": {
    "id": "GY-HhD2YRNZe"
   },
   "source": [
    "## [3.3] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "11b9f861c5d67e1b",
   "metadata": {
    "id": "l-5TSuHWRNZe"
   },
   "outputs": [],
   "source": [
    "def remove_sentences_after_doctor(text):\n",
    "    result = re.sub(r'(الدكتور(?:ة)[^\\n]*\\n|د\\.[^\\n]*\\n)([^\\n]*\\n){0,6}', r'\\1', text)\n",
    "    return result\n",
    "\n",
    "# for text in texts[:5]:\n",
    "#     print(remove_sentences_after_doctor(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_sentences_after_doctor(text):\n",
    "    text_without_empty_lines = re.sub(r'\\n\\s*\\n', '\\n', text)\n",
    "    lines = text_without_empty_lines.splitlines()\n",
    "    lines.pop()\n",
    "    new_text = '\\n'.join(lines)   \n",
    "    return new_text\n",
    "\n",
    "# for text in texts[:100]:\n",
    "#     print(remove_sentences_after_doctor(text))\n",
    "#     print(\"/////////////////////////////////////////////\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad580f71f7101173",
   "metadata": {
    "id": "KV9D86kbRNZe"
   },
   "source": [
    "## [3.4] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "ffdb6de003d15333",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_duplicate_letters_except_alif(text):\n",
    "    pattern = re.compile(r'(?:(?<!ا)ا(?!ا)|(.)(?:\\1)+)', re.UNICODE) # بشلي كلشي أحرف مكررة وخاصة الألف مابكررها اكتر من مرة\n",
    "    result = pattern.sub(lambda x: x.group(1) if x.group(1) else 'ا', text)\n",
    "    return result\n",
    "\n",
    "# for text in texts[:5]:\n",
    "#     print(remove_duplicate_letters_except_alif(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "978ddb92b76b7f1f",
   "metadata": {},
   "source": [
    "## [3.5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1041d0eeb848aab",
   "metadata": {
    "id": "3fM_FPldSCxp"
   },
   "source": [
    "### [3.5.1] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "# هاد بيقلب لل 1و2و3و4و....\n",
    "def convert_numbers_to_arabic_v2(text):\n",
    "    digit_mapping = {'٠': '0', '١': '1', '٢': '2', '٣': '3', '٤': '4', '٥': '5', '٦': '6', '٧': '7', '٨': '8', '٩': '9'}\n",
    "    english_text = re.sub(r'[٠-٩]', lambda x: digit_mapping[x.group()], text)\n",
    "\n",
    "    return english_text\n",
    "\n",
    "\n",
    "# for text in texts[:5]:\n",
    "#     print(convert_numbers_to_arabic_v2(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "59d7bd3624496465",
   "metadata": {
    "id": "ohGu5R14RNZe"
   },
   "outputs": [],
   "source": [
    "# هاد بيلقلب لل ١,٢,٣,٤\n",
    "def convert_numbers_to_arabic(text):\n",
    "    digit_mapping = {'0': '٠', '1': '١', '2': '٢', '3': '٣', '4': '٤', '5': '٥', '6': '٦', '7': '٧', '8': '٨', '9': '٩'}\n",
    "    arabic_text = re.sub(r'[0-9]', lambda x: digit_mapping[x.group()], text)\n",
    "\n",
    "    return arabic_text\n",
    "\n",
    "\n",
    "# for text in texts[:5]:\n",
    "#     print(convert_numbers_to_arabic(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9efb2c08d7a0eae5",
   "metadata": {
    "id": "N2z1KmQXSAFs"
   },
   "source": [
    "### [3.5.2] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "584a485a3ab32cff",
   "metadata": {
    "id": "fmOXIMWoSAFt"
   },
   "outputs": [],
   "source": [
    "def standardize_numbers(text, replacement_symbol='T'):\n",
    "    digit_pattern = r'[0-9٠-٩]+'\n",
    "    standardized_text = re.sub(digit_pattern, lambda x: replacement_symbol, text)\n",
    "\n",
    "    return standardized_text\n",
    "\n",
    "\n",
    "# for text in texts[:5]:\n",
    "#     print(standardize_numbers(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a45d054195bd5cc",
   "metadata": {
    "id": "fCBHc2y6SAL5"
   },
   "source": [
    "### [3.5.3] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "db36eb4fffdef9e7",
   "metadata": {
    "id": "QJX_PtSzSAL6"
   },
   "outputs": [],
   "source": [
    "def remove_numbers(text):\n",
    "    digit_pattern = r'\\d+'\n",
    "    \n",
    "    text_without_numbers = re.sub(digit_pattern, '', text)\n",
    "\n",
    "    return text_without_numbers\n",
    "\n",
    "# for text in texts[:5]:\n",
    "#     print(remove_numbers(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "124e337a2e9808a9",
   "metadata": {},
   "source": [
    "### [3.5.4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "4d298a9526a3ae7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = train_data[train_data['label']=='أمراض الجهاز التنفسي']['answer'].values\n",
    "\n",
    "def remove_dates_and_times(text):\n",
    "    date_time_patterns = [\n",
    "        r'\\d{4}-\\d{2}-\\d{2}\\s+\\d{1,2}:\\d{2}:\\d{2}'  # Match dates and times like 2015-01-04 20:08:51\n",
    "        # r'\\d{1,2}/\\d{1,2}/\\d{2,4}',         # Match dates like 12/31/2022\n",
    "        # r'\\d{1,2}-\\d{1,2}-\\d{2,4}',         # Match dates like 12-31-2022\n",
    "        # r'\\d{1,2}:\\d{2}:\\d{2}'               # Match times like 12:34:56\n",
    "        # r'\\d{1,2}\\s+(?:Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)\\s*\\d{2,4}',  # Match dates like 31 Dec 2022\n",
    "        # r'\\d{1,2}:\\d{2}\\s*(?:AM|PM|am|pm)?',  # Match times like 12:34 AM\n",
    "    ]\n",
    "\n",
    "    for pattern in date_time_patterns:\n",
    "        text = re.sub(pattern, '', text)\n",
    "\n",
    "    return text\n",
    "\n",
    "# for text in texts[:5]:\n",
    "#     print(remove_dates_and_times(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c897aae5bc65e80",
   "metadata": {
    "id": "P1aLjPbzRNZe"
   },
   "source": [
    "## [3.6] "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c7db1ab76a0c0f8",
   "metadata": {
    "id": "dbqN7BceSRhS"
   },
   "source": [
    "### [3.6.1] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string\n",
    "string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "8e6d37a9e0c0365",
   "metadata": {
    "id": "caLTQ8EtSRhT"
   },
   "outputs": [],
   "source": [
    "def remove_punctuation(text):\n",
    "    text=re.sub(r'[!\\\"#$%&\\'()*+,\\-./:;<=>?@\\[\\\\\\]^_`{|}~،؛؟ـ]',' ',text)\n",
    "    return text\n",
    "\n",
    "# for text in texts[:5]:\n",
    "#     print(remove_punctuation(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aa2beec5d82e166",
   "metadata": {
    "id": "rad8wPeSSRhU"
   },
   "source": [
    "### [3.6.2] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "426725eac6677209",
   "metadata": {
    "id": "UZuYtSHaSRhU"
   },
   "outputs": [],
   "source": [
    "def keep_arabic_punctuations_numbers(text):\n",
    "    text= re.sub(r'[^\\u0600-\\u06FF0-9،؛؟ـ!\\\"#$%&\\'()*+,\\-./:;<=>?@\\[\\\\\\]^_`{|}~]', ' ', text)\n",
    "    return text\n",
    "\n",
    "# for text in texts[:5]:\n",
    "#     print(keep_arabic_punctuations_numbers(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f48de648b46f28a6",
   "metadata": {
    "id": "N4WDktTgRNZf"
   },
   "source": [
    "## [3.7] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "94667dc98883d735",
   "metadata": {
    "id": "B6icsYsFRNZf"
   },
   "outputs": [],
   "source": [
    "stop_word=set(stop_words.classed_stopwords_list())\n",
    "Some_stop_words_we_need=[\"قبلما\",\"بعدما\",\"فقط\",\"مازال\",\"ليست\",\"ليس\",\"ليسا\",\"لسنا\",\n",
    "                         \"لسن\",\"لازالت\",\"مساء\",\"صباح\",\"قبل\",\"بعد\",\"إياك\",\"إياكن\",\n",
    "                         \"إياكما\",\"إياكم\",\"لم\",\"عدا\",\"إلا\",\"د\",\"كلا\",\"عامة\",\"لا\",\n",
    "                         \"حبذا\",\"أقل\",\"أكثر\"]\n",
    "Some_stop_words_dont_we_need=['عليكم','السلام','انا','أرجو','الرد','المزيد','إقرأ']\n",
    "for word in Some_stop_words_we_need:\n",
    "    if word in stop_word:\n",
    "        stop_word.remove(word)\n",
    "\n",
    "for word in Some_stop_words_we_need:\n",
    "    if word is not stop_word:\n",
    "        stop_word.add(word)\n",
    "\n",
    "def remove_stop_words(text):\n",
    "    text =re.sub(\"[\\_\\-\\/]\",\" \",text)\n",
    "    text=word_tokenize(text)\n",
    "    text=[w for w in text if not w in stop_word]\n",
    "    return text\n",
    "\n",
    "# for text in texts[:5]:\n",
    "#     print(remove_stop_words(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ملاحــــــــــــــــــظه\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46805849c2d2623b",
   "metadata": {
    "id": "wbGPWNtoR4p-"
   },
   "source": [
    "## [3.8] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def stem_arabic_words_farasa(text):\n",
    "#     stemmer = FarasaStemmer()\n",
    "#     text=stemmer.stem(text)\n",
    "#     return text\n",
    "\n",
    "# text = \"الكتب في المكتبة متنوعة ومفيدة\"\n",
    "# stemmed_text = stem_arabic_words_farasa(text)\n",
    "# print(stemmed_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def stem_arabic_words_tashaphyne(text):\n",
    "#     stemmer = ArabicLightStemmer()\n",
    "#     words = text.split()\n",
    "#     stemmed_words = [stemmer.light_stem(word) for word in words]\n",
    "#     return ' '.join(stemmed_words)\n",
    "\n",
    "# text = \"الكتب في المكتبة متنوعة ومفيدة\"\n",
    "# stemmed_text = stem_arabic_words_tashaphyne(text)\n",
    "# print(stemmed_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stem_arabic_words_ISRIStemmer(text):\n",
    "    stemmer = ISRIStemmer()\n",
    "    words = text.split()\n",
    "    stemmed_words = [stemmer.stem(word) for word in words]\n",
    "    return ' '.join(stemmed_words)\n",
    "\n",
    "# text = \"الكتب في المكتبة متنوعة ومفيدة\"\n",
    "# stemmed_text = stem_arabic_words_ISRIStemmer(text)\n",
    "# print(stemmed_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stem_arabic_words_snowballstemmer(text):\n",
    "    ar_stemmer = stemmer(\"arabic\")\n",
    "    words = text.split()\n",
    "    stemmed_words = [ar_stemmer.stemWord(word) for word in words]\n",
    "    return ' '.join(stemmed_words)\n",
    "\n",
    "# text = \"الكتب في المكتبة متنوعة ومفيدة\"\n",
    "# stemmed_text = stem_arabic_words_snowballstemmer(text)\n",
    "# print(stemmed_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for text in texts[:5]:\n",
    "#     print(stem_arabic_words_snowballstemmer(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for text in texts[:5]:\n",
    "#     print(stem_arabic_words_tashaphyne(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for text in texts[:5]:\n",
    "#     print(stem_arabic_words_farasa(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for text in texts[:5]:\n",
    "#     print(stem_arabic_words_ISRIStemmer(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80016c32a6453254",
   "metadata": {
    "id": "V-rCzzoMR41T"
   },
   "source": [
    "## [3.9] "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43f9be9fd522e36a",
   "metadata": {
    "id": "iXF2L4Z-Sd3N"
   },
   "source": [
    "### [3.9.1] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "d09fdfc13c8e9d32",
   "metadata": {
    "id": "wk-Z3Gw4Sd3O"
   },
   "outputs": [],
   "source": [
    "def unify_hamzat(text):\n",
    "    unified_text = re.sub(r'[ءؤئ]', 'ء', text)\n",
    "    return unified_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for text in texts[:5]:\n",
    "#     print(unify_hamzat(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "952d14ba938901a4",
   "metadata": {
    "id": "ij5gdJA4Sd3O"
   },
   "source": [
    "### [3.9.2] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "64925506e5dfc1d4",
   "metadata": {
    "id": "bDLOsd4tSd3O"
   },
   "outputs": [],
   "source": [
    "def unify_Alfat(text):\n",
    "    unified_text = re.sub(r'[أإآ]', 'أ', text)\n",
    "    return unified_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for text in texts[:5]:\n",
    "#     print(unify_Alfat(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6914083b35745e6",
   "metadata": {
    "id": "H6k8JnJDSd3P"
   },
   "source": [
    "### [3.9.3] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "ac11de1bf61d2af7",
   "metadata": {
    "id": "JhSOCYLmSd3P"
   },
   "outputs": [],
   "source": [
    "def remove_tatweel(text):\n",
    "    text =re.sub( r'ـ', '', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for text in texts[:5]:\n",
    "#     print(remove_tatweel(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9eeb1a5672b59b",
   "metadata": {
    "id": "1TmDb343Sls9"
   },
   "source": [
    "### [3.9.4] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "5c81a4e0fcae8662",
   "metadata": {
    "id": "wWJ-Z11_Sls-"
   },
   "outputs": [],
   "source": [
    "def remove_arabic_diacritics(text):\n",
    "    text=re.sub(r'[\\u064B-\\u065F]', '', text)\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for text in texts[:5]:\n",
    "#     print(remove_arabic_diacritics(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b18e9774ff7851a",
   "metadata": {
    "id": "0V5LD9EWVgY7"
   },
   "source": [
    "## [3.10] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "f2e4214581691a7b",
   "metadata": {
    "id": "AkVzG3AWVgZA"
   },
   "outputs": [],
   "source": [
    "def remove_space(text):\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for text in texts[:5]:\n",
    "#     print(remove_space(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8d8811ac8263334",
   "metadata": {
    "id": "exdhuH6GtLez"
   },
   "source": [
    "## Extra [3.11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c02431e73710b9c",
   "metadata": {
    "id": "_UOzIn6-tKlJ"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "22792fd6fb27623",
   "metadata": {
    "id": "Jp-C0ZwrS8AY"
   },
   "source": [
    "# Question [4]: Prepare Training Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10dd4630ce82793b",
   "metadata": {
    "id": "pwtVy8DES8AZ"
   },
   "source": [
    "## [4.1] \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "548f25e826fab2dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\r\\nما هي مميزات و عيوب الدواء جلوكوفانس 500 5 و ايضا الأنسولين مكس تارد 30 \\r\\n\\r\\nلكل علاج ايجابيته وسلبياته والتي تعتمد على حالة المريض فما قد يناسب احدهم قد لا يناسب الاخر وهذا يحدده الطبيب خلال مراجعة المريض له\\r\\n1\\r\\n2015-01-04 20:08:51\\r\\n\\r\\n\\r\\nالدكتور يزن علي خليف\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n/اسئلة-طبية/مرض-السكري/ما-هي-مميزات-و-عيوب-الدواء-جلوكوفانس-و-ايضا-الانسولين-351157\\r\\n\\r\\nالدكتور يزن علي خليف \\r\\n\\r\\n\\r\\nالغدد الصماء \\r\\n\\r\\n\\r\\n',\n",
       " '\\r\\nاليك نتيجة تحليل هرمونات الغدة الدرقية علما بانه تم استأصال الغدة منذ اكثر من سنتينTT3=163TT4=12.6TSH=.01 هل مطلوب تعديل الجرعة \\r\\n\\r\\nنعم. يجب تخفيض الجرعة، الا اذا كان سبب استئصال الغدة هو سرطان الغدة\\r\\n0\\r\\n2017-02-13 06:34:19\\r\\n\\r\\n\\r\\nالدكتور باسم مرقص\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n/اسئلة-طبية/امراض-الغدد-الصماء/اليك-نتيجة-تحليل-هرمونات-الغدة-الدرقية-علما-بانه-تم-909154\\r\\n\\r\\nالدكتور باسم مرقص \\r\\n\\r\\n\\r\\nجراحة عامة \\r\\n\\r\\n\\r\\n',\n",
       " '\\r\\nحلول منزلية لأعراض ارتفاع ضغط الدم \\r\\n\\r\\nيفضل عدم الاستغناء عن العلاج الدوائي لمرضى الضغط لكن يعد النظام الغذائي و الحركي اليومي للمريض جزء مهم و أساسي في الحفاظ على ضغط المريض ضمن الحدود الطبيعية.\\r\\nينصح ممارسة دورية للرياضة و المشي و محاولة تخفيف الوزن، التخفيف من المنبهات بأنواعها و محاولة الإقلاع عن التدخين.\\r\\nتناول الأطعمة الغنية بالبوتاسيوم كالموز و تجنب الأطعمة التي تحتوي على أملاح لتأثيرها المباشر على الضغط، و تناول المغذيات مثل مستخلص الثوم و زيوت السمك.\\r\\nتجنّب التوتر و المحافظة على نظام يومي هادئ و المتابعة الدورية لقراءات الضغط.\\r\\n3\\r\\n2014-12-09 18:05:33\\r\\n\\r\\n\\r\\nالدكتور انور سالم العواودة\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n/اسئلة-طبية/ارتفاع-ضغط-الدم/حلول-منزلية-لاعراض-ارتفاع-ضغط-الدم-342609\\r\\n\\r\\nالدكتور انور سالم العواودة \\r\\n\\r\\n\\r\\nالقلب والاوعية الدموية \\r\\n\\r\\n\\r\\n',\n",
       " '\\r\\nعملت عملية دوالي الساقين قسطرة الليزر من شهر وعندي الم من اسفل ساق الى اعلى ساق و برودة اطراف القدم وحرقان وخز و امكان يابسة وحكة و طيبة عليا هل هدا طبيعي احس في سكين تقطع \\r\\n\\r\\nراجع طبيبك من اجري الجراحه افضل من يجيب لانه شاهد الحاله علي وضعها الاول وهوا من قام بلفعل تمنياتي بلشفاء\\r\\n0\\r\\n2023-06-17 05:56:49\\r\\n\\r\\n\\r\\nالدكتور حسن ابراهيم خليفة\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n/اسئلة-طبية/جراحة-عامة/عملت-عملية-دوالي-الساقين-قسطرة-الليزر-من-شهر-وعندي-1715059\\r\\n\\r\\nالدكتور حسن ابراهيم خليفة \\r\\n\\r\\n\\r\\nجراحة عامة \\r\\n\\r\\n\\r\\n',\n",
       " '\\r\\nما حقيقة ان تمرين العضلة النعلية يخفض السكر بالدم؟؟ \\r\\n\\r\\nإذا قصدت تدليك العضلة فهذا كلام غير صحيح . ولكن ممارسة الرياضة مثل المشي يعني حركة العضلات والمساهمة في حرق السكر\\r\\n0\\r\\n2023-06-03 07:52:03\\r\\n\\r\\n\\r\\nالدكتور اغيد محمد بيريص\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n/اسئلة-طبية/مرض-السكري/ما-حقيقة-ان-تمرين-العضلة-النعلية-يخفض-السكر-بالدم-1709936\\r\\n\\r\\nالدكتور اغيد محمد بيريص \\r\\n\\r\\n\\r\\nالغدد الصماء \\r\\n\\r\\n\\r\\n']"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def preparing_training_data(text):\n",
    "    text_after_cleaning_links = remove_links(text)\n",
    "    text_after_removing_doctors_prefix = remove_doctors_prefix(text_after_cleaning_links)\n",
    "    text_after_removing_specialization = remove_sentences_after_doctor(text_after_removing_doctors_prefix)\n",
    "    text_after_removing_doublication_chars = remove_duplicate_letters_except_alif(text_after_removing_specialization)\n",
    "    text_after_removing_date_and_time = remove_dates_and_times(text_after_removing_doublication_chars)\n",
    "    text_after_removing_numbers = remove_numbers(text_after_removing_date_and_time)\n",
    "    text_after_removing_non_arabic_words = keep_arabic_punctuations_numbers(text_after_removing_numbers)\n",
    "    # text after removing stop words\n",
    "    text_after_unify_hamzat = unify_hamzat(text_after_removing_non_arabic_words)\n",
    "    text_after_unify_alafat = unify_Alfat(text_after_unify_hamzat)\n",
    "    text_after_removing_tatwel = remove_tatweel(text_after_unify_alafat)\n",
    "    text_after_removing_tashkel = remove_arabic_diacritics(text_after_removing_tatwel)\n",
    "    text_after_removing_spaces_and_keeping_one_space = remove_space(text_after_removing_tashkel)\n",
    "    # text after removing wrong arabic words and correct them\n",
    "\n",
    "\n",
    "    return text_after_removing_spaces_and_keeping_one_space\n",
    "    \n",
    "    \n",
    "    \n",
    "questions = train_data['question'].values\n",
    "answers = train_data['answer'].values\n",
    "\n",
    "train_data['cleann_questions']=questions\n",
    "train_data['cleann_answer']=answers\n",
    "questions_answers = questions[0:5] + answers[0:5]\n",
    "\n",
    "questions_answers.tolist()\n",
    "# for i in questions_answers:\n",
    "#     print(preparing_training_data(i))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77be5ba87989eb2b",
   "metadata": {
    "id": "npObP2JLUBgw"
   },
   "source": [
    "## [4.2] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "b945acaab7ef7888",
   "metadata": {
    "id": "V_KxmQdgUBg5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1939864\n"
     ]
    }
   ],
   "source": [
    "all_train_data = questions + answers\n",
    "all_train_data_After_cleaning = []\n",
    "all_tokens = []\n",
    "for line in all_train_data:\n",
    "    training_data_after_cleaning = preparing_training_data(line)\n",
    "    all_train_data_After_cleaning.append(training_data_after_cleaning)\n",
    "    # print(training_data_after_cleaning)\n",
    "\n",
    "\n",
    "# all_train_data_After_cleaning\n",
    "# # print(tokens)\n",
    "for phrase in all_train_data_After_cleaning:\n",
    "    tokens = nltk.word_tokenize(phrase)\n",
    "    all_tokens.append(tokens)   \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "flattened_all_tokens = [item for sublist in all_tokens for item in sublist]\n",
    "print(len(flattened_all_tokens)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "عدد الكلمات الفريدة: 128022\n"
     ]
    }
   ],
   "source": [
    "# حساب عدد الكلمات الفريدة :\n",
    "total_unique_words=len(set(flattened_all_tokens))\n",
    "print(\"عدد الكلمات الفريدة:\", total_unique_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "# نلاحظ ان عدد الكلمات الكلية قد قلت هنا أما عدد الكلمات المميزة قد زاد"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9545f003ed979ad",
   "metadata": {
    "id": "pTIrTECWUBg5"
   },
   "source": [
    "## [4.3] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "e8358e6f447495af",
   "metadata": {
    "id": "OqmCKe5lUBg6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32598\n",
      "32598\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df = pd.DataFrame(all_train_data_After_cleaning)\n",
    "\n",
    "df = df.map(lambda x: '' if isinstance(x, (float, int)) else x.strip() if isinstance(x, str) and len(x) >= 5 else None)\n",
    "\n",
    "df = df.dropna()\n",
    "print(len(df))\n",
    "print(len(all_train_data_After_cleaning))\n",
    "# all_train_data_After_cleaning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43333415d395ec67",
   "metadata": {},
   "source": [
    "# Question [5]: Baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3f3f370c22073bd",
   "metadata": {},
   "source": [
    "The Comparison Dictionary example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from keras.backend import clear_session\n",
    "# clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "779a44f1fdc3be2",
   "metadata": {},
   "outputs": [],
   "source": [
    "comparison_table = {}\n",
    "\n",
    "comparison_table['question_step_number'] = []\n",
    "comparison_table['model_name'] = []\n",
    "comparison_table['features'] = []\n",
    "comparison_table['model_parameters'] = []\n",
    "comparison_table['preprocessing_methods'] = []\n",
    "comparison_table['accuracy'] = []\n",
    "comparison_table['balance_accuracy'] = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "100516edd49d694f",
   "metadata": {},
   "source": [
    "Filling the dictionary example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "7af1fa16c41d67ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# comparison_table['question_step_number'].append(\"3.5\")\n",
    "# comparison_table['model_name'].append('logistic_regression')\n",
    "# comparison_table['features'].append('bag of words')\n",
    "# comparison_table['model_parameters'].append('default')\n",
    "# comparison_table['preprocessing_methods'].append(\"remove links\")\n",
    "# comparison_table['accuracy'].append(accuracy)\n",
    "# comparison_table['balance_accuracy'].append(balance_accuracy_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ea60039714ed596",
   "metadata": {},
   "source": [
    "## [5.1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Prepare_data():\n",
    "    X_train ,y_train,X_test,y_test,X_validation,y_validation=Prepare()\n",
    "    X_train_combined = X_train['question'] + ' ' + X_train['answer']\n",
    "    X_test_combined = X_test['question'] + ' ' + X_test['answer']\n",
    "    X_validation_combined = X_validation['question'] + ' ' + X_validation['answer']\n",
    "    return X_train_combined,X_test_combined,X_validation_combined\n",
    "\n",
    "def Prepare():\n",
    "    X_train, y_train = train_data[['question', 'answer']], train_data['label']\n",
    "    X_test, y_test = test_data[['question', 'answer']], test_data['label']\n",
    "    X_validation, y_validation = validation_data[['question', 'answer']], validation_data['label']\n",
    "    return X_train ,y_train,X_test,y_test,X_validation,y_validation\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "32f1bc127a2a5794",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train, y_train = train_data[['question', 'answer']], train_data['label']\n",
    "# X_test, y_test = test_data[['question', 'answer']], test_data['label']\n",
    "# X_validation, y_validation = validation_data[['question', 'answer']], validation_data['label']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "237204d8bc60265f",
   "metadata": {},
   "source": [
    "## [5.2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_combined ,X_test_combined,X_validation_combined = Prepare_data()\n",
    "X_train ,y_train,X_test,y_test,X_validation,y_validation =Prepare()\n",
    "\n",
    "\n",
    "def logistic_regression(x_train,x_test,x_val):\n",
    "\n",
    "    vectorizer = CountVectorizer()\n",
    "\n",
    "    X_train_bow = vectorizer.fit_transform(x_train)\n",
    "    X_test_bow = vectorizer.transform(x_test)\n",
    "    X_validation_bow = vectorizer.transform(x_val)\n",
    "\n",
    "    logreg_model = LogisticRegression(max_iter=1000)\n",
    "\n",
    "    logreg_model.fit(X_train_bow, y_train)\n",
    "\n",
    "    y_test_pred = logreg_model.predict(X_test_bow)\n",
    "    y_validation_pred = logreg_model.predict(X_validation_bow)\n",
    "\n",
    "    accuracy_test = accuracy_score(y_test, y_test_pred)\n",
    "    accuracy_validation = accuracy_score(y_validation, y_validation_pred)\n",
    "    balanced_accuracy_test = balanced_accuracy_score(y_test, y_test_pred)\n",
    "\n",
    "\n",
    "    print(f\"Test Accuracy: {accuracy_test}\")\n",
    "    print(f\"Validation Accuracy: {accuracy_validation}\")\n",
    "    print(f\"Balanced Accuracy: {balanced_accuracy_test}\")\n",
    "\n",
    "\n",
    "    return accuracy_test, accuracy_validation, balanced_accuracy_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "# accuracy_test, accuracy_validation, balanced_accuracy_test = logistic_regression(X_train_combined,X_test_combined,X_validation_combined)\n",
    "\n",
    "# comparison_table['question_step_number'].append(\"5.2\")\n",
    "# comparison_table['model_name'].append('logistic_regression')\n",
    "# comparison_table['features'].append('bag of words')\n",
    "# comparison_table['model_parameters'].append('default')\n",
    "# comparison_table['preprocessing_methods'].append(\"nothing\")\n",
    "# comparison_table['accuracy'].append(accuracy_test)\n",
    "# comparison_table['balance_accuracy'].append(balanced_accuracy_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "ملاحظااااااااااااااااااااااااااااااااااات\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "نلاحظ ان ال test accuracy دقتو أكثر من ال balanced accuracy بنسبة بسيطة\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8b0b98768fcd170",
   "metadata": {},
   "source": [
    "## [5.3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "26a46fbbad90de4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # تنظيف النص من الروابط\n",
    "\n",
    "# X_train_combined = X_train['question'].apply(remove_links) + ' ' + X_train['answer'].apply(remove_links)\n",
    "\n",
    "\n",
    "# # print(len(X_train_without_links))\n",
    "# print(len(X_train_combined))\n",
    "\n",
    "# accuracy_test, accuracy_validation, balanced_accuracy_test = logistic_regression(X_train_combined,X_test_combined,X_validation_combined)\n",
    "\n",
    "# comparison_table['question_step_number'].append(\"5.3\")\n",
    "# comparison_table['model_name'].append('logistic_regression')\n",
    "# comparison_table['features'].append('bag of words')\n",
    "# comparison_table['model_parameters'].append('default')\n",
    "# comparison_table['preprocessing_methods'].append(\"Removing Links\")\n",
    "# comparison_table['accuracy'].append(accuracy_test)\n",
    "# comparison_table['balance_accuracy'].append(balanced_accuracy_test)\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # حذف اسماء الأطباء مع اللقب\n",
    "# X_train_combined = X_train['question'].apply(remove_doctors_prefix) + ' ' + X_train['answer'].apply(remove_doctors_prefix)\n",
    "\n",
    "\n",
    "# # print(len(X_train_without_links))\n",
    "# print(len(X_train_combined))\n",
    "\n",
    "# accuracy_test, accuracy_validation, balanced_accuracy_test = logistic_regression(X_train_combined,X_test_combined,X_validation_combined)\n",
    "\n",
    "# comparison_table['question_step_number'].append(\"5.3\")\n",
    "# comparison_table['model_name'].append('logistic_regression')\n",
    "# comparison_table['features'].append('bag of words')\n",
    "# comparison_table['model_parameters'].append('default')\n",
    "# comparison_table['preprocessing_methods'].append(\"ٌRemoving doctors prefix\")\n",
    "# comparison_table['accuracy'].append(accuracy_test)\n",
    "# comparison_table['balance_accuracy'].append(balanced_accuracy_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # حذف المحارف المكررة من النص\n",
    "# X_train_combined = X_train['question'].apply(remove_duplicate_letters_except_alif) + ' ' + X_train['answer'].apply(remove_duplicate_letters_except_alif)\n",
    "\n",
    "\n",
    "# # print(len(X_train_without_links))\n",
    "# print(len(X_train_combined))\n",
    "\n",
    "# accuracy_test, accuracy_validation, balanced_accuracy_test = logistic_regression(X_train_combined,X_test_combined,X_validation_combined)\n",
    "\n",
    "# comparison_table['question_step_number'].append(\"5.3\")\n",
    "# comparison_table['model_name'].append('logistic_regression')\n",
    "# comparison_table['features'].append('bag of words')\n",
    "# comparison_table['model_parameters'].append('default')\n",
    "# comparison_table['preprocessing_methods'].append(\"Removing duplicate letters except alif\")\n",
    "# comparison_table['accuracy'].append(accuracy_test)\n",
    "# comparison_table['balance_accuracy'].append(balanced_accuracy_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # تحويل الارقام الى العربية\n",
    "# X_train_combined = X_train['question'].apply(convert_numbers_to_arabic_v2) + ' ' + X_train['answer'].apply(convert_numbers_to_arabic_v2)\n",
    "\n",
    "\n",
    "# # print(len(X_train_without_links))\n",
    "# print(len(X_train_combined))\n",
    "\n",
    "# accuracy_test, accuracy_validation, balanced_accuracy_test = logistic_regression(X_train_combined,X_test_combined,X_validation_combined)\n",
    "\n",
    "# comparison_table['question_step_number'].append(\"5.3\")\n",
    "# comparison_table['model_name'].append('logistic_regression')\n",
    "# comparison_table['features'].append('bag of words')\n",
    "# comparison_table['model_parameters'].append('default')\n",
    "# comparison_table['preprocessing_methods'].append(\"Removing Links\")\n",
    "# comparison_table['accuracy'].append(accuracy_test)\n",
    "# comparison_table['balance_accuracy'].append(balanced_accuracy_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # تحويل الارقام الى رمز من اختياري\n",
    "# X_train_combined = X_train['question'].apply(standardize_numbers) + ' ' + X_train['answer'].apply(standardize_numbers)\n",
    "\n",
    "\n",
    "# # print(len(X_train_without_links))\n",
    "# print(len(X_train_combined))\n",
    "\n",
    "# accuracy_test, accuracy_validation, balanced_accuracy_test = logistic_regression(X_train_combined,X_test_combined,X_validation_combined)\n",
    "\n",
    "# comparison_table['question_step_number'].append(\"5.3\")\n",
    "# comparison_table['model_name'].append('logistic_regression')\n",
    "# comparison_table['features'].append('bag of words')\n",
    "# comparison_table['model_parameters'].append('default')\n",
    "# comparison_table['preprocessing_methods'].append(\"standardize numbers to specific char\")\n",
    "# comparison_table['accuracy'].append(accuracy_test)\n",
    "# comparison_table['balance_accuracy'].append(balanced_accuracy_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # حذف الأرقام كلها\n",
    "# X_train_combined = X_train['question'].apply(remove_numbers) + ' ' + X_train['answer'].apply(remove_numbers)\n",
    "\n",
    "\n",
    "# # print(len(X_train_without_links))\n",
    "# print(len(X_train_combined))\n",
    "\n",
    "# accuracy_test, accuracy_validation, balanced_accuracy_test = logistic_regression(X_train_combined,X_test_combined,X_validation_combined)\n",
    "\n",
    "# comparison_table['question_step_number'].append(\"5.3\")\n",
    "# comparison_table['model_name'].append('logistic_regression')\n",
    "# comparison_table['features'].append('bag of words')\n",
    "# comparison_table['model_parameters'].append('default')\n",
    "# comparison_table['preprocessing_methods'].append(\"Removing numbers\")\n",
    "# comparison_table['accuracy'].append(accuracy_test)\n",
    "# comparison_table['balance_accuracy'].append(balanced_accuracy_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # حذف التاريخ والوقت\n",
    "# X_train_combined = X_train['question'].apply(remove_dates_and_times) + ' ' + X_train['answer'].apply(remove_dates_and_times)\n",
    "\n",
    "\n",
    "# # print(len(X_train_without_links))\n",
    "# print(len(X_train_combined))\n",
    "\n",
    "# accuracy_test, accuracy_validation, balanced_accuracy_test = logistic_regression(X_train_combined,X_test_combined,X_validation_combined)\n",
    "\n",
    "# comparison_table['question_step_number'].append(\"5.3\")\n",
    "# comparison_table['model_name'].append('logistic_regression')\n",
    "# comparison_table['features'].append('bag of words')\n",
    "# comparison_table['model_parameters'].append('default')\n",
    "# comparison_table['preprocessing_methods'].append(\"Removing Date and time\")\n",
    "# comparison_table['accuracy'].append(accuracy_test)\n",
    "# comparison_table['balance_accuracy'].append(balanced_accuracy_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # حذف المحارف الغير عربية\n",
    "# X_train_combined = X_train['question'].apply(keep_arabic_punctuations_numbers) + ' ' + X_train['answer'].apply(keep_arabic_punctuations_numbers)\n",
    "\n",
    "\n",
    "# # print(len(X_train_without_links))\n",
    "# print(len(X_train_combined))\n",
    "\n",
    "# accuracy_test, accuracy_validation, balanced_accuracy_test = logistic_regression(X_train_combined,X_test_combined,X_validation_combined)\n",
    "\n",
    "# comparison_table['question_step_number'].append(\"5.3\")\n",
    "# comparison_table['model_name'].append('logistic_regression')\n",
    "# comparison_table['features'].append('bag of words')\n",
    "# comparison_table['model_parameters'].append('default')\n",
    "# comparison_table['preprocessing_methods'].append(\"Remove non arabic words\")\n",
    "# comparison_table['accuracy'].append(accuracy_test)\n",
    "# comparison_table['balance_accuracy'].append(balanced_accuracy_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # حذف كلمات التوقف\n",
    "# X_train_combined = X_train['question'].apply(lambda x: ' '.join(remove_stop_words(x))) + ' ' + X_train['answer'].apply(lambda x: ' '.join(remove_stop_words(x)))\n",
    "\n",
    "# # print(len(X_train_without_links))\n",
    "# print(len(X_train_combined))\n",
    "\n",
    "# accuracy_test, accuracy_validation, balanced_accuracy_test = logistic_regression(X_train_combined,X_test_combined,X_validation_combined)\n",
    "\n",
    "# comparison_table['question_step_number'].append(\"5.3\")\n",
    "# comparison_table['model_name'].append('logistic_regression')\n",
    "# comparison_table['features'].append('bag of words')\n",
    "# comparison_table['model_parameters'].append('default')\n",
    "# comparison_table['preprocessing_methods'].append(\"Remove non arabic words\")\n",
    "# comparison_table['accuracy'].append(accuracy_test)\n",
    "# comparison_table['balance_accuracy'].append(balanced_accuracy_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # تجذير الكلمات العربية\n",
    "# X_train_combined = X_train['question'].apply(stem_arabic_words_snowballstemmer) + ' ' + X_train['answer'].apply(stem_arabic_words_snowballstemmer) # 0,55 time : 2:50 m\n",
    "# # X_train_combined = X_train['question'].apply(stem_arabic_words_tashaphyne) + ' ' + X_train['answer'].apply(stem_arabic_words_tashaphyne) # 0,54 time : 5:26 m\n",
    "# # X_train_combined = X_train['question'].apply(stem_arabic_words_farasa) + ' ' + X_train['answer'].apply(stem_arabic_words_farasa) # **** time more than 35:00 m and he stills running\n",
    "# # X_train_combined = X_train['question'].apply(stem_arabic_words_ISRIStemmer) + ' ' + X_train['answer'].apply(stem_arabic_words_ISRIStemmer) # 0.53 time 2:30 m \n",
    "\n",
    "# stem_arabic_words_ISRIStemmer\n",
    "# # print(len(X_train_without_links))\n",
    "# print(len(X_train_combined))\n",
    "\n",
    "# accuracy_test, accuracy_validation, balanced_accuracy_test = logistic_regression(X_train_combined,X_test_combined,X_validation_combined)\n",
    "\n",
    "# comparison_table['question_step_number'].append(\"5.3\")\n",
    "# comparison_table['model_name'].append('logistic_regression')\n",
    "# comparison_table['features'].append('bag of words')\n",
    "# comparison_table['model_parameters'].append('default')\n",
    "# comparison_table['preprocessing_methods'].append(\"stem arabic words\")\n",
    "# comparison_table['accuracy'].append(accuracy_test)\n",
    "# comparison_table['balance_accuracy'].append(balanced_accuracy_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # توحيد طريقة كتابة الهمزات\n",
    "# X_train_combined = X_train['question'].apply(unify_hamzat) + ' ' + X_train['answer'].apply(unify_hamzat)\n",
    "\n",
    "\n",
    "# # print(len(X_train_without_links))\n",
    "# print(len(X_train_combined))\n",
    "\n",
    "# accuracy_test, accuracy_validation, balanced_accuracy_test = logistic_regression(X_train_combined,X_test_combined,X_validation_combined)\n",
    "\n",
    "# comparison_table['question_step_number'].append(\"5.3\")\n",
    "# comparison_table['model_name'].append('logistic_regression')\n",
    "# comparison_table['features'].append('bag of words')\n",
    "# comparison_table['model_parameters'].append('default')\n",
    "# comparison_table['preprocessing_methods'].append(\"unify hamzat\")\n",
    "# comparison_table['accuracy'].append(accuracy_test)\n",
    "# comparison_table['balance_accuracy'].append(balanced_accuracy_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # توحيد طريقة كتابة الألفات\n",
    "# X_train_combined = X_train['question'].apply(unify_Alfat) + ' ' + X_train['answer'].apply(unify_Alfat)\n",
    "\n",
    "\n",
    "# # print(len(X_train_without_links))\n",
    "# print(len(X_train_combined))\n",
    "\n",
    "# accuracy_test, accuracy_validation, balanced_accuracy_test = logistic_regression(X_train_combined,X_test_combined,X_validation_combined)\n",
    "\n",
    "# comparison_table['question_step_number'].append(\"5.3\")\n",
    "# comparison_table['model_name'].append('logistic_regression')\n",
    "# comparison_table['features'].append('bag of words')\n",
    "# comparison_table['model_parameters'].append('default')\n",
    "# comparison_table['preprocessing_methods'].append(\"unify Alafat\")\n",
    "# comparison_table['accuracy'].append(accuracy_test)\n",
    "# comparison_table['balance_accuracy'].append(balanced_accuracy_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # حذف التطويل\n",
    "# X_train_combined = X_train['question'].apply(remove_tatweel) + ' ' + X_train['answer'].apply(remove_tatweel)\n",
    "\n",
    "\n",
    "# # print(len(X_train_without_links))\n",
    "# print(len(X_train_combined))\n",
    "\n",
    "# accuracy_test, accuracy_validation, balanced_accuracy_test = logistic_regression(X_train_combined,X_test_combined,X_validation_combined)\n",
    "\n",
    "# comparison_table['question_step_number'].append(\"5.3\")\n",
    "# comparison_table['model_name'].append('logistic_regression')\n",
    "# comparison_table['features'].append('bag of words')\n",
    "# comparison_table['model_parameters'].append('default')\n",
    "# comparison_table['preprocessing_methods'].append(\"remove tatweel\")\n",
    "# comparison_table['accuracy'].append(accuracy_test)\n",
    "# comparison_table['balance_accuracy'].append(balanced_accuracy_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # حذف التشكيل\n",
    "# X_train_combined = X_train['question'].apply(remove_arabic_diacritics) + ' ' + X_train['answer'].apply(remove_arabic_diacritics)\n",
    "\n",
    "\n",
    "# # print(len(X_train_without_links))\n",
    "# print(len(X_train_combined))\n",
    "\n",
    "# accuracy_test, accuracy_validation, balanced_accuracy_test = logistic_regression(X_train_combined,X_test_combined,X_validation_combined)\n",
    "\n",
    "# comparison_table['question_step_number'].append(\"5.3\")\n",
    "# comparison_table['model_name'].append('logistic_regression')\n",
    "# comparison_table['features'].append('bag of words')\n",
    "# comparison_table['model_parameters'].append('default')\n",
    "# comparison_table['preprocessing_methods'].append(\"remove arabic diacritics\")\n",
    "# comparison_table['accuracy'].append(accuracy_test)\n",
    "# comparison_table['balance_accuracy'].append(balanced_accuracy_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # حذف الفراغات\n",
    "# X_train_combined = X_train['question'].apply(remove_space) + ' ' + X_train['answer'].apply(remove_space)\n",
    "\n",
    "\n",
    "# # print(len(X_train_without_links))\n",
    "# print(len(X_train_combined))\n",
    "\n",
    "# accuracy_test, accuracy_validation, balanced_accuracy_test = logistic_regression(X_train_combined,X_test_combined,X_validation_combined)\n",
    "\n",
    "# comparison_table['question_step_number'].append(\"5.3\")\n",
    "# comparison_table['model_name'].append('logistic_regression')\n",
    "# comparison_table['features'].append('bag of words')\n",
    "# comparison_table['model_parameters'].append('default')\n",
    "# comparison_table['preprocessing_methods'].append(\"remove spaces with one space\")\n",
    "# comparison_table['accuracy'].append(accuracy_test)\n",
    "# comparison_table['balance_accuracy'].append(balanced_accuracy_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # حذف اختصاص الطبيب\n",
    "# X_train_combined = X_train['question'].apply(remove_sentences_after_doctor) + ' ' + X_train['answer'].apply(remove_sentences_after_doctor)\n",
    "\n",
    "\n",
    "# # print(len(X_train_without_links))\n",
    "# print(len(X_train_combined))\n",
    "\n",
    "# accuracy_test, accuracy_validation, balanced_accuracy_test = logistic_regression(X_train_combined,X_test_combined,X_validation_combined)\n",
    "\n",
    "# comparison_table['question_step_number'].append(\"5.3\")\n",
    "# comparison_table['model_name'].append('logistic_regression')\n",
    "# comparison_table['features'].append('bag of words')\n",
    "# comparison_table['model_parameters'].append('default')\n",
    "# comparison_table['preprocessing_methods'].append(\"remove doctore specialization\")\n",
    "# comparison_table['accuracy'].append(accuracy_test)\n",
    "# comparison_table['balance_accuracy'].append(balanced_accuracy_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91088d7e7ab7f906",
   "metadata": {},
   "source": [
    "## [5.4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0.6437688353128878\n",
    "def callfunction(text):\n",
    "    # text=text.apply(remove_links) # 0.6434142882467647\n",
    "    # text=text.apply(remove_doctors_prefix) # 0.639691544052473\n",
    "    # text=text.apply(remove_duplicate_letters_except_alif) # 0.6353483424924659\n",
    "    # text=text.apply(convert_numbers_to_arabic_v2) # 0.6424392838149264\n",
    "    # text=text.apply(standardize_numbers) # 0.6348165218932813\n",
    "    # text=text.apply(remove_numbers) # 0.6348165218932813\n",
    "    # text=text.apply(remove_dates_and_times) # 0.6217869172132601\n",
    "    # text=text.apply(keep_arabic_punctuations_numbers) # 0.6435915617798262\n",
    "    # text=text.apply(lambda x: ' '.join(remove_stop_words(x))) # 0.6085800390001773\n",
    "    # text=text.apply(stem_arabic_words_snowballstemmer) # 0.553270696684985\n",
    "    # text=text.apply(unify_hamzat) # 0.6403120014181882\n",
    "    text=text.apply(unify_Alfat) # 0.6448324765112569  # good\n",
    "    text=text.apply(remove_tatweel) # 0.6437688353128878 # same \n",
    "    # text=text.apply(remove_arabic_diacritics) # 0.6422620102818649\n",
    "    text=text.apply(remove_space) # 0.6437688353128878 # same\n",
    "    # text=text.apply(remove_sentences_after_doctor) # 0.6217869172132601\n",
    "    \n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train_combined = X_train['question'] + ' ' + X_train['answer']\n",
    "# X_train_combined = callfunction(X_train_combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "# accuracy_test, accuracy_validation, balanced_accuracy_test = logistic_regression(X_train_combined,X_test_combined,X_validation_combined)\n",
    "\n",
    "# comparison_table['question_step_number'].append(\"5.4\")\n",
    "# comparison_table['model_name'].append('logistic_regression')\n",
    "# comparison_table['features'].append('bag of words')\n",
    "# comparison_table['model_parameters'].append('default')\n",
    "# comparison_table['preprocessing_methods'].append(\"callfunction\")\n",
    "# comparison_table['accuracy'].append(accuracy_test)\n",
    "# comparison_table['balance_accuracy'].append(balanced_accuracy_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8de567c997ade2a8",
   "metadata": {},
   "source": [
    "## [5.5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train_combined ,X_test_combined,X_validation_combined =Prepare_data()\n",
    "\n",
    "# X_train_combined=callfunction(X_train_combined)\n",
    "# # X_test_combined=callfunction(X_test_combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tfidf_vectorizer  = TfidfVectorizer()\n",
    "# train_data_X_train_combined = np.array(X_train_combined)\n",
    "# train_data_X_test_combined = np.array(X_test_combined)\n",
    "# train_data_X_validation_combined = np.array(X_validation_combined)\n",
    "# tfidf_matrix_X_train_combined = tfidf_vectorizer .fit_transform(train_data_X_train_combined)\n",
    "# tfidf_matrix_X_test_combined = tfidf_vectorizer .transform(train_data_X_test_combined)\n",
    "# tfidf_matrix_X_validation_combined = tfidf_vectorizer .transform(train_data_X_validation_combined)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from keras.backend import clear_session\n",
    "# clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # accuracy_test, accuracy_validation, balanced_accuracy_test = logistic_regression(tfidf_matrix_X_train_combined,tfidf_matrix_X_test_combined,tfidf_matrix_X_validation_combined)\n",
    "# # model = make_pipeline(\n",
    "# #     StandardScaler(with_mean=False),\n",
    "# #     LogisticRegression(max_iter=1000, solver='lbfgs')\n",
    "# # )\n",
    "\n",
    "\n",
    "# model = LogisticRegression(max_iter=1000)\n",
    "\n",
    "# model.fit(tfidf_matrix_X_train_combined, y_train)\n",
    "# # predict_proba\n",
    "# y_test_pred = model.predict(tfidf_matrix_X_test_combined)\n",
    "# y_validation_pred = model.predict(tfidf_matrix_X_validation_combined)\n",
    "\n",
    "# accuracy_test = accuracy_score(y_test, y_test_pred)\n",
    "# accuracy_validation = accuracy_score(y_validation, y_validation_pred)\n",
    "# balanced_accuracy_test = balanced_accuracy_score(y_test, y_test_pred)\n",
    "\n",
    "\n",
    "# print(f\"Test Accuracy: {accuracy_test}\")\n",
    "# print(f\"Validation Accuracy: {accuracy_validation}\")\n",
    "# print(f\"Balanced Accuracy: {balanced_accuracy_test}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "id": "964993c49f7a92a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# comparison_table['question_step_number'].append(\"5.5\")\n",
    "# comparison_table['model_name'].append('logistic_regression')\n",
    "# comparison_table['features'].append('TF-IDF')\n",
    "# comparison_table['model_parameters'].append('default')\n",
    "# comparison_table['preprocessing_methods'].append(\"callfunction\")\n",
    "# comparison_table['accuracy'].append(accuracy_test)\n",
    "# comparison_table['balance_accuracy'].append(balanced_accuracy_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea32f682a2894679",
   "metadata": {},
   "source": [
    "## [5.6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "id": "27abe69834b04663",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectorizer  = TfidfVectorizer(min_df=2,max_df=0.5,ngram_range=(1,2))\n",
    "\n",
    "# train_data_X_train_combined = np.array(X_train_combined)\n",
    "# train_data_X_test_combined = np.array(X_test_combined)\n",
    "# train_data_X_validation_combined = np.array(X_validation_combined)\n",
    "# tfidf_matrix_X_train_combined = vectorizer .fit_transform(train_data_X_train_combined)\n",
    "# tfidf_matrix_X_test_combined = vectorizer .transform(train_data_X_test_combined)\n",
    "# tfidf_matrix_X_validation_combined = vectorizer .transform(train_data_X_validation_combined)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = LogisticRegression(max_iter=1000)\n",
    "\n",
    "# model.fit(tfidf_matrix_X_train_combined, y_train)\n",
    "# # predict_proba\n",
    "# y_test_pred = model.predict(tfidf_matrix_X_test_combined)\n",
    "# y_validation_pred = model.predict(tfidf_matrix_X_validation_combined)\n",
    "\n",
    "# accuracy_test = accuracy_score(y_test, y_test_pred)\n",
    "# accuracy_validation = accuracy_score(y_validation, y_validation_pred)\n",
    "# balanced_accuracy_test = balanced_accuracy_score(y_test, y_test_pred)\n",
    "\n",
    "\n",
    "# print(f\"Test Accuracy: {accuracy_test}\")\n",
    "# print(f\"Validation Accuracy: {accuracy_validation}\")\n",
    "# print(f\"Balanced Accuracy: {balanced_accuracy_test}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "# comparison_table['question_step_number'].append(\"5.6\")\n",
    "# comparison_table['model_name'].append('logistic_regression')\n",
    "# comparison_table['features'].append('TF-IDF with set transactions')\n",
    "# comparison_table['model_parameters'].append('default')\n",
    "# comparison_table['preprocessing_methods'].append(\"callfunction\")\n",
    "# comparison_table['accuracy'].append(accuracy_test)\n",
    "# comparison_table['balance_accuracy'].append(balanced_accuracy_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "aff6ddfc1ceb5594",
   "metadata": {},
   "source": [
    "# Question [6]: Semantic Representation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a880357d5d916109",
   "metadata": {},
   "source": [
    "## [6.1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "id": "931b87ba185753e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# X_train_combined = X_train['question'] + ' ' + X_train['answer']\n",
    "\n",
    "\n",
    "# vectorizer = CountVectorizer()\n",
    "\n",
    "# train_data_bow = vectorizer.fit_transform(X_train_combined)\n",
    "\n",
    "# def get_word_occurrences(word, bow_matrix, texts):\n",
    "#     # لحتى جيب الانديكس للكلمة من ضمن مصفوفة الباغ اوف ووردز\n",
    "#     word_index = vectorizer.vocabulary_.get(word)\n",
    "\n",
    "#     if word_index is not None:\n",
    "#         # امشيلي عالنص واطبعلي الكلمة وكم كرة مكررة\n",
    "#         for i, text in enumerate(texts):\n",
    "#             occurrences = bow_matrix[i, word_index]\n",
    "#             print(f\"Text {i + 1}: Word '{word}' occurs {occurrences} times\")\n",
    "#     else:\n",
    "#         print(f\"The word '{word}' is not found in the bag-of-words matrix.\")\n",
    "\n",
    "# # Example usage\n",
    "# # Assuming 'مميزات' is the word you want to check\n",
    "# get_word_occurrences('حرارة', train_data_bow, X_train_combined)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da09109b1d0dea66",
   "metadata": {},
   "source": [
    "## [6.2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "id": "40ccfb829f657e4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_context_words(target_word, bow_matrix, texts, context_size=10):\n",
    "#     word_index = vectorizer.vocabulary_.get(target_word)\n",
    "\n",
    "#     if word_index is not None:\n",
    "#         context_words = set()  \n",
    "\n",
    "#         for i, text in enumerate(texts):\n",
    "#             occurrences = bow_matrix[i, word_index]\n",
    "\n",
    "#             if occurrences > 0: # اذا تكررت الكلمة مرة أو أكثر\n",
    "#                 # جبلنا الكونتيكست ووردز ضمن ال الكونتيكست سايز\n",
    "#                 start_index = max(0, word_index - context_size)\n",
    "#                 end_index = min(len(vectorizer.get_feature_names_out()), word_index + context_size + 1)\n",
    "#                 context_words.update(vectorizer.get_feature_names_out()[start_index:end_index])\n",
    "\n",
    "#         return context_words\n",
    "#     else:\n",
    "#         print(f\"The word '{target_word}' is not found in the bag-of-words matrix.\")\n",
    "#         return None\n",
    "\n",
    "# target_word = 'حرارة'\n",
    "# context_words = get_context_words(target_word, train_data_bow, X_train_combined)\n",
    "\n",
    "# if context_words is not None:\n",
    "#     print(f\"Context words for '{target_word}': {context_words}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62e9c6831c1fcf5b",
   "metadata": {},
   "source": [
    "## [6.3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Notes]\n",
    "\n",
    "عادة لا يتم استخدام تجذيع او تجذير الكلمات لانها تؤدي الى فقدان معلومات مهمة والدقيقة حول الكلمات -\n",
    "لا حاجة لعمليات تقطيع اخرى -\n",
    "في معظم الحالات، لا تحتاج جميع الكلمات إلى أن تمتلك أشعة داللية (word embeddings). يمكن ترك الكلمات النادرة أو التي ليس لها تأثير كبير على النموذج بدون تمثيل. هذا يقلل من حجم مصفوفة الأشعة الداللية ويحسن كفاءة التدريب -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary:\n",
      "{'<pad>': 0, 'question': 1, 'answer': 2}\n",
      "\n",
      "Inverse Vocabulary:\n",
      "{0: '<pad>', 1: 'question', 2: 'answer'}\n",
      "\n",
      "Example Sequence:\n",
      "[1]\n",
      "\n",
      "Positive Skip-Grams:\n"
     ]
    }
   ],
   "source": [
    "# import tensorflow as tf\n",
    "# import pandas as pd\n",
    "# import tensorflow as tf\n",
    "# from tensorflow.keras import layers\n",
    "# import numpy as np\n",
    "# import tqdm\n",
    "# import re\n",
    "\n",
    "# # Assuming you have already defined X_train as mentioned in your question\n",
    "# X_train_combined = X_train['question'] + ' ' + X_train['answer']\n",
    "\n",
    "# # Tokenize the text in X_train\n",
    "# tokens_list = [list(sentence.lower().split()) for sentence in X_train]\n",
    "# tokens = [token for tokens in tokens_list for token in tokens]\n",
    "\n",
    "# # Build vocabulary\n",
    "# vocab, index = {}, 1  # start indexing from 1\n",
    "# vocab['<pad>'] = 0  # add a padding token\n",
    "# for token in tokens:\n",
    "#     if token not in vocab:\n",
    "#         vocab[token] = index\n",
    "#         index += 1\n",
    "# vocab_size = len(vocab)\n",
    "\n",
    "# # Print vocabulary\n",
    "# print(\"Vocabulary:\")\n",
    "# print(vocab)\n",
    "\n",
    "# # Create inverse vocabulary\n",
    "# inverse_vocab = {index: token for token, index in vocab.items()}\n",
    "\n",
    "# # Print inverse vocabulary\n",
    "# print(\"\\nInverse Vocabulary:\")\n",
    "# print(inverse_vocab)\n",
    "\n",
    "# # Convert text to sequences\n",
    "# sequences = [[vocab[word] for word in tokens] for tokens in tokens_list]\n",
    "\n",
    "# # Print example sequence\n",
    "# print(\"\\nExample Sequence:\")\n",
    "# print(sequences[0])\n",
    "\n",
    "# # Generate positive skip-grams\n",
    "# window_size = 2\n",
    "# positive_skip_grams, _ = tf.keras.preprocessing.sequence.skipgrams(\n",
    "#     sequences[0],\n",
    "#     vocabulary_size=vocab_size,\n",
    "#     window_size=window_size,\n",
    "#     negative_samples=0\n",
    "# )\n",
    "\n",
    "# # Print positive skip-grams\n",
    "# print(\"\\nPositive Skip-Grams:\")\n",
    "# for target, context in positive_skip_grams[:5]:\n",
    "#     print(f\"({target}, {context}): ({inverse_vocab[target]}, {inverse_vocab[context]})\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # Generates skip-gram pairs with negative sampling for a list of sequences\n",
    "# # (int-encoded sentences) based on window size, number of negative samples\n",
    "# # and vocabulary size.\n",
    "# # Generates skip-gram pairs with negative sampling for a list of sequences\n",
    "# # (int-encoded sentences) based on window size, number of negative samples\n",
    "# # and vocabulary size.\n",
    "# def generate_training_data(sequences, window_size, num_ns, vocab_size, seed):\n",
    "#   # Elements of each training example are appended to these lists.\n",
    "#     targets, contexts, labels = [], [], []\n",
    "\n",
    "#     # Build the sampling table for `vocab_size` tokens.\n",
    "#     sampling_table = tf.keras.preprocessing.sequence.make_sampling_table(vocab_size)\n",
    "\n",
    "#     # Iterate over all sequences (sentences) in the dataset.\n",
    "#     for sequence in tqdm.tqdm(sequences):\n",
    "\n",
    "#         # Generate positive skip-gram pairs for a sequence (sentence).\n",
    "#         positive_skip_grams, _ = tf.keras.preprocessing.sequence.skipgrams(\n",
    "#             sequence,\n",
    "#             vocabulary_size=vocab_size,\n",
    "#             sampling_table=sampling_table,\n",
    "#             window_size=window_size,\n",
    "#             negative_samples=0)\n",
    "\n",
    "#         # Iterate over each positive skip-gram pair to produce training examples\n",
    "#         # with a positive context word and negative samples.\n",
    "#         for target_word, context_word in positive_skip_grams:\n",
    "#         context_class = tf.expand_dims(\n",
    "#             tf.constant([context_word], dtype=\"int64\"), 1)\n",
    "#         negative_sampling_candidates, _, _ = tf.random.log_uniform_candidate_sampler(\n",
    "#             true_classes=context_class,\n",
    "#             num_true=1,\n",
    "#             num_sampled=num_ns,\n",
    "#             unique=True,\n",
    "#             range_max=vocab_size,\n",
    "#             seed=seed,\n",
    "#             name=\"negative_sampling\")\n",
    "\n",
    "#         # Build context and label vectors (for one target word)\n",
    "#         context = tf.concat([tf.squeeze(context_class,1), negative_sampling_candidates], 0)\n",
    "#         label = tf.constant([1] + [0]*num_ns, dtype=\"int64\")\n",
    "\n",
    "#         # Append each element from the training example to global lists.\n",
    "#         targets.append(target_word)\n",
    "#         contexts.append(context)\n",
    "#         labels.append(label)\n",
    "\n",
    "#     return targets, contexts, labels\n",
    "\n",
    "# # Assuming you have already loaded your dataset and X_train, as you mentioned\n",
    "# text_ds = tf.data.Dataset.from_tensor_slices(X_train).filter(lambda x: tf.cast(tf.strings.length(x), bool))\n",
    "\n",
    "# # ... (unchanged)\n",
    "\n",
    "# # Vectorize the data in text_ds.\n",
    "# text_vector_ds = text_ds.batch(1024).prefetch(tf.data.AUTOTUNE).map(vectorize_layer).unbatch()\n",
    "\n",
    "# sequences = list(text_vector_ds.as_numpy_iterator())\n",
    "\n",
    "# # ... (unchanged)\n",
    "\n",
    "# targets, contexts, labels = generate_training_data(\n",
    "#     sequences=sequences,\n",
    "#     window_size=2,\n",
    "#     num_ns=4,\n",
    "#     vocab_size=vocab_size,\n",
    "#     seed=SEED)\n",
    "\n",
    "# # ... (unchanged)\n",
    "\n",
    "# # Rest of the code remains the same\n",
    "\n",
    "# BATCH_SIZE = 1024\n",
    "# BUFFER_SIZE = 10000\n",
    "# dataset = tf.data.Dataset.from_tensor_slices(((targets, contexts), labels))\n",
    "# dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
    "\n",
    "# dataset = dataset.cache().prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "# print(dataset)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "254/254 [==============================] - 152s 584ms/step - loss: 8.8956 - accuracy: 0.5990\n",
      "Epoch 2/5\n",
      "254/254 [==============================] - 134s 527ms/step - loss: 6.9999 - accuracy: 0.6410\n",
      "Epoch 3/5\n",
      "254/254 [==============================] - 130s 510ms/step - loss: 5.6593 - accuracy: 0.6563\n",
      "Epoch 4/5\n",
      "254/254 [==============================] - 192s 755ms/step - loss: 5.3157 - accuracy: 0.6444\n",
      "Epoch 5/5\n",
      "254/254 [==============================] - 141s 557ms/step - loss: 4.7400 - accuracy: 0.6726\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x254ddd47e20>"
      ]
     },
     "execution_count": 264,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Embedding, Dense, Reshape, Input, Dot\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.backend import clear_session\n",
    "from keras.preprocessing.sequence import skipgrams\n",
    "import numpy as np\n",
    "\n",
    "def build_skipgram_model(vocab_size, embedding_dim):\n",
    "    input_target = Input((1,))\n",
    "    input_context = Input((1,))\n",
    "    \n",
    "    embedding = Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=1)\n",
    "    target = embedding(input_target)\n",
    "    context = embedding(input_context)\n",
    "    \n",
    "    dot_product = Dot(axes=2)([target, context])\n",
    "    dot_product = Reshape((1,), input_shape=(1, 1))(dot_product)\n",
    "    \n",
    "    output = Dense(vocab_size, activation='softmax')(dot_product)  \n",
    "    \n",
    "    model = Model(inputs=[input_target, input_context], outputs=output)\n",
    "    model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "def generate_skipgram_data(X_train_seq, vocab_size, window_size, negative_samples, batch_size):\n",
    "    while True:\n",
    "        X, y = [], []\n",
    "        for seq in X_train_seq:\n",
    "            pairs, labels = skipgrams(seq, vocabulary_size=vocab_size, window_size=window_size, negative_samples=negative_samples)\n",
    "            if pairs:\n",
    "                target_words, context_words = zip(*pairs)\n",
    "                X.extend(zip(target_words, context_words))\n",
    "                y.extend(labels)\n",
    "\n",
    "            if len(X) >= batch_size:\n",
    "                yield [np.array(X)[:, 0], np.array(X)[:, 1]], np.array(y)\n",
    "                X, y = [], []\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def prep_data(x_train):\n",
    "    x_train = x_train.apply(remove_links)\n",
    "    x_train = x_train .apply(remove_dates_and_times)\n",
    "    x_train = x_train.apply(remove_numbers)\n",
    "    x_train = x_train.apply(lambda x: ' '.join(remove_stop_words(x)))\n",
    "    # x_train = x_train.apply(stem_arabic_words_snowballstemmer)\n",
    "    x_train = x_train.apply(unify_hamzat)\n",
    "    x_train = x_train.apply(unify_Alfat)\n",
    "    x_train = x_train.apply(remove_tatweel)\n",
    "    x_train = x_train.apply(remove_space)\n",
    "    return x_train    \n",
    "\n",
    "# Example usage:\n",
    "embedding_dim = 50\n",
    "window_size = 5\n",
    "negative_samples = 5\n",
    "batch_size = 128\n",
    "vocab_size = 10000\n",
    "\n",
    "tokenizer = Tokenizer(num_words=vocab_size)\n",
    "# X_train_combined = prep_data(X_train_combined)\n",
    "tokenizer.fit_on_texts(X_train_combined)\n",
    "\n",
    "# total_words = len(tokenizer.word_index) + 1\n",
    "\n",
    "# print(vocab_size)\n",
    "\n",
    "\n",
    "X_train_seq = tokenizer.texts_to_sequences(X_train_combined)\n",
    "\n",
    "model = build_skipgram_model(vocab_size, embedding_dim)\n",
    "generator = generate_skipgram_data(X_train_seq, vocab_size, window_size, negative_samples, batch_size)\n",
    "\n",
    "clear_session()\n",
    "model.fit(generator, epochs=5, steps_per_epoch=len(X_train_seq) // batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from keras.backend import clear_session\n",
    "# clear_session()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff51d40919fe48e8",
   "metadata": {},
   "source": [
    "## [6.4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # import matplotlib.pyplot as plt\n",
    "\n",
    "# # embedding_layer = model.get_layer('embedding')\n",
    "# # weights = embedding_layer.get_weights()[0]\n",
    "\n",
    "# # word_index = tokenizer.word_index\n",
    "# # reverse_word_index = {v: k for k, v in word_index.items()}\n",
    "\n",
    "# # num_words_to_visualize = 100\n",
    "# # words_to_visualize = [reverse_word_index[i] for i in range(1, num_words_to_visualize + 1)]\n",
    "\n",
    "# # word_embeddings = weights[1:num_words_to_visualize + 1]\n",
    "\n",
    "# # plt.figure(figsize=(10, 8))\n",
    "# # for i, word in enumerate(words_to_visualize):\n",
    "# #     plt.scatter(word_embeddings[i, 0], word_embeddings[i, 1])\n",
    "# #     plt.text(word_embeddings[i, 0], word_embeddings[i, 1], word)\n",
    "\n",
    "# # plt.title('Word Embeddings Visualization')\n",
    "# # plt.xlabel('Dimension 1')\n",
    "# # plt.ylabel('Dimension 2')\n",
    "# # plt.show()\n",
    "\n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "# from arabic_reshaper import arabic_reshaper\n",
    "# from bidi.algorithm import get_display\n",
    "\n",
    "# arabic_font_path = 'font/NotoNaskhArabic-VariableFont_wght.ttf'\n",
    "\n",
    "# embedding_layer = model.get_layer('embedding')\n",
    "# weights = embedding_layer.get_weights()[0]\n",
    "\n",
    "# word_index = tokenizer.word_index\n",
    "# reverse_word_index = {v: k for k, v in word_index.items()}\n",
    "\n",
    "# num_words_to_visualize = 100\n",
    "# words_to_visualize = [reverse_word_index[i] for i in range(1, num_words_to_visualize + 1)]\n",
    "\n",
    "# word_embeddings = weights[1:num_words_to_visualize + 1]\n",
    "\n",
    "# plt.figure(figsize=(10, 8))\n",
    "\n",
    "# for i, word in enumerate(words_to_visualize):\n",
    "#     reshaped_word = arabic_reshaper.reshape(word)\n",
    "#     bidi_word = get_display(reshaped_word)\n",
    "\n",
    "#     plt.scatter(word_embeddings[i, 0], word_embeddings[i, 1])\n",
    "#     plt.text(word_embeddings[i, 0], word_embeddings[i, 1], bidi_word, fontproperties='Arial')\n",
    "\n",
    "# arabic_font = FontProperties(fname='font/NotoNaskhArabic-VariableFont_wght.ttf')\n",
    "# plt.title(get_display(arabic_reshaper.reshape('تصوير التضمين باللغة العربية')), fontproperties=arabic_font)\n",
    "# plt.xlabel(get_display(arabic_reshaper.reshape('البُعد الأول')), fontproperties=arabic_font)\n",
    "# plt.ylabel(get_display(arabic_reshaper.reshape('البُعد الثاني')), fontproperties=arabic_font)\n",
    "# plt.show()\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5f600c066a9f996",
   "metadata": {},
   "source": [
    "## [6.5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def analogy(word_a, word_b, word_c, model, tokenizer):\n",
    "    \n",
    "#     embeddings = model.get_layer('embedding').get_weights()[0]\n",
    "#     word_index = tokenizer.word_index\n",
    "\n",
    "#     if all(word in word_index for word in [word_a, word_b, word_c]):\n",
    "#         a_idx, b_idx, c_idx = [word_index[word] for word in [word_a, word_b, word_c]]\n",
    "#         vec_a, vec_b, vec_c = embeddings[a_idx], embeddings[b_idx], embeddings[c_idx]\n",
    "\n",
    "#         # Calculate the analogy vector\n",
    "#         analogy_vector = vec_b - vec_a + vec_c\n",
    "\n",
    "#         # Find the closest word in the embedding space\n",
    "#         closest_word_idx = np.argmin(np.linalg.norm(embeddings - analogy_vector, axis=1))\n",
    "\n",
    "#         # Get the word from the index\n",
    "#         closest_word = list(word_index.keys())[list(word_index.values()).index(closest_word_idx)]\n",
    "#         return closest_word\n",
    "#     else:\n",
    "#         return None\n",
    "\n",
    "# # Example usage:\n",
    "# for _ in range(5):\n",
    "#     word_a = \"ضغط\"\n",
    "#     word_b = \"علاج\"\n",
    "#     word_c = \"التنفسي\"\n",
    "\n",
    "#     result = analogy(word_a, word_b, word_c, model, tokenizer)\n",
    "#     print(f\"{word_a} is to {word_b} as {word_c} is to {result}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "198b722c1bbcdcf5",
   "metadata": {},
   "source": [
    "# Question [7]: Deep Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aff5925a2f8f2a0",
   "metadata": {},
   "source": [
    "## [7.1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from keras.backend import clear_session\n",
    "# clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_combined ,X_test_combined,X_validation_combined = Prepare_data()\n",
    "X_train ,y_train,X_test,y_test,X_validation,y_validation =Prepare()\n",
    "X_train_combined=callfunction(X_train_combined)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97f755371c085f16",
   "metadata": {},
   "source": [
    "### [7.1.1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_tf_sparse(X):\n",
    "    coo = X.tocoo()\n",
    "    indices = np.mat([coo.row, coo.col]).transpose()\n",
    "    return tf.sparse.reorder(tf.sparse.SparseTensor(indices, coo.data.astype(np.float32), coo.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "id": "6daf2e41c9731bb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectorizer = CountVectorizer()\n",
    "# X_train_bow = vectorizer.fit_transform(X_train_combined)\n",
    "# X_test_bow = vectorizer.transform(X_test_combined)\n",
    "# X_validation_bow = vectorizer.transform(X_validation_combined)\n",
    "\n",
    "# X_train_sparse = convert_to_tf_sparse(X_train_bow)\n",
    "# X_test_sparse = convert_to_tf_sparse(X_test_bow)\n",
    "# X_validation_sparse = convert_to_tf_sparse(X_validation_bow)\n",
    "\n",
    "# label_encoder = LabelEncoder()\n",
    "# y_train_encoded = label_encoder.fit_transform(y_train)\n",
    "# y_test_encoded = label_encoder.transform(y_test)\n",
    "# y_validation_encoded = label_encoder.transform(y_validation)\n",
    "\n",
    "# y_train_categorical = to_categorical(y_train_encoded, num_classes=8)\n",
    "# y_test_categorical = to_categorical(y_test_encoded, num_classes=8)\n",
    "# y_validation_categorical = to_categorical(y_validation_encoded, num_classes=8)\n",
    "\n",
    "# model = Sequential()\n",
    "# model.add(Dense(64, activation='relu', input_dim=X_train_sparse.shape[1]))\n",
    "# # model.add(Dropout(0.5))\n",
    "# model.add(Dense(32, activation='relu'))\n",
    "# # model.add(Dropout(0.5))\n",
    "# model.add(Dense(8, activation='softmax')) \n",
    "\n",
    "# model.compile(optimizer='adam',\n",
    "#               loss='categorical_crossentropy',\n",
    "#               metrics=['accuracy'])\n",
    "\n",
    "# model.fit(X_train_sparse, y_train_categorical, epochs=10, batch_size=32, validation_data=(X_validation_sparse, y_validation_categorical))\n",
    "\n",
    "# loss, accuracy = model.evaluate(X_test_sparse, y_test_categorical)\n",
    "# print(f'Loss: {loss}, Accuracy: {accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "# comparison_table['question_step_number'].append(\"7.1.1\")\n",
    "# comparison_table['model_name'].append('Deep Neural Network')\n",
    "# comparison_table['features'].append('BOW')\n",
    "# comparison_table['model_parameters'].append('default')\n",
    "# comparison_table['preprocessing_methods'].append(\"callfunction\")\n",
    "# comparison_table['accuracy'].append(accuracy)\n",
    "# comparison_table['balance_accuracy'].append(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e30d9a52819bcc24",
   "metadata": {},
   "source": [
    "### [7.1.2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "id": "8be530a9b8e25845",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectorizer  = TfidfVectorizer(min_df=2,max_df=0.5,ngram_range=(1,2))\n",
    "\n",
    "# train_data_X_train_combined = np.array(X_train_combined)\n",
    "# train_data_X_test_combined = np.array(X_test_combined)\n",
    "# train_data_X_validation_combined = np.array(X_validation_combined)\n",
    "# tfidf_matrix_X_train_combined = vectorizer .fit_transform(train_data_X_train_combined)\n",
    "# tfidf_matrix_X_test_combined = vectorizer .transform(train_data_X_test_combined)\n",
    "# tfidf_matrix_X_validation_combined = vectorizer .transform(train_data_X_validation_combined)\n",
    "\n",
    "# tfidf_matrix_X_train_combined=convert_to_tf_sparse(tfidf_matrix_X_train_combined)\n",
    "# tfidf_matrix_X_test_combined=convert_to_tf_sparse(tfidf_matrix_X_test_combined)\n",
    "# tfidf_matrix_X_validation_combined=convert_to_tf_sparse(tfidf_matrix_X_validation_combined)\n",
    "\n",
    "# label_encoder = LabelEncoder()\n",
    "# y_train_encoded = label_encoder.fit_transform(y_train)\n",
    "# y_test_encoded = label_encoder.transform(y_test)\n",
    "# y_validation_encoded = label_encoder.transform(y_validation)\n",
    "\n",
    "# y_train_categorical = to_categorical(y_train_encoded, num_classes=8)\n",
    "# y_test_categorical = to_categorical(y_test_encoded, num_classes=8)\n",
    "# y_validation_categorical = to_categorical(y_validation_encoded, num_classes=8)\n",
    "\n",
    "\n",
    "# model = Sequential()\n",
    "# model.add(Dense(64, activation='relu', input_dim=tfidf_matrix_X_train_combined.shape[1]))\n",
    "# model.add(Dropout(0.5))\n",
    "# model.add(Dense(32, activation='relu'))\n",
    "# model.add(Dropout(0.5))\n",
    "# model.add(Dense(y_train_categorical.shape[1], activation='softmax')) \n",
    "\n",
    "# model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# model.fit(tfidf_matrix_X_train_combined, y_train_categorical, epochs=10, batch_size=32,validation_data=(tfidf_matrix_X_validation_combined, y_validation_categorical))\n",
    "\n",
    "# loss, accuracy = model.evaluate(tfidf_matrix_X_test_combined, y_test_categorical)\n",
    "# print(f'Loss: {loss}, Accuracy: {accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "# comparison_table['question_step_number'].append(\"7.1.2\")\n",
    "# comparison_table['model_name'].append('Deep Neural Network')\n",
    "# comparison_table['features'].append('TF-IDF')\n",
    "# comparison_table['model_parameters'].append('default')\n",
    "# comparison_table['preprocessing_methods'].append(\"callfunction\")\n",
    "# comparison_table['accuracy'].append(accuracy)\n",
    "# comparison_table['balance_accuracy'].append(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "248bd4a3e1cb55a6",
   "metadata": {},
   "source": [
    "## [7.2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26576136e27fd19c",
   "metadata": {},
   "source": [
    "### [7.2.1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_layer_weights = model.layers[2].get_weights()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "id": "bf9b0d5ff2781c1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train_seq_padded shape: (32598, 5)\n",
      "Epoch 1/5\n",
      "815/815 [==============================] - 9s 8ms/step - loss: 1.9806 - accuracy: 0.1983 - val_loss: 1.9867 - val_accuracy: 0.2055\n",
      "Epoch 2/5\n",
      "815/815 [==============================] - 6s 8ms/step - loss: 1.9719 - accuracy: 0.2052 - val_loss: 1.9850 - val_accuracy: 0.2118\n",
      "Epoch 3/5\n",
      "815/815 [==============================] - 6s 7ms/step - loss: 1.9700 - accuracy: 0.2043 - val_loss: 1.9823 - val_accuracy: 0.2100\n",
      "Epoch 4/5\n",
      "815/815 [==============================] - 7s 9ms/step - loss: 1.9690 - accuracy: 0.2062 - val_loss: 1.9841 - val_accuracy: 0.1969\n",
      "Epoch 5/5\n",
      "815/815 [==============================] - 6s 8ms/step - loss: 1.9681 - accuracy: 0.2037 - val_loss: 1.9806 - val_accuracy: 0.2132\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x2574601d2a0>"
      ]
     },
     "execution_count": 303,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from keras.models import Sequential\n",
    "# from keras.layers import Embedding, Conv1D, MaxPooling1D, GlobalMaxPooling1D, Dense\n",
    "# from keras.initializers import Constant\n",
    "\n",
    "# # فرضاً لدينا مصفوفة الأشعة من Word2Vec\n",
    "# word_vectors = embedding_layer_weights\n",
    "\n",
    "# # إنشاء النموذج\n",
    "# cnn_model_1 = Sequential()\n",
    "# cnn_model_1.add(Embedding(input_dim=word_vectors.shape[0],\n",
    "#                     output_dim=word_vectors.shape[1],\n",
    "#                     embeddings_initializer=Constant(word_vectors),\n",
    "#                     trainable=False))  \n",
    "# cnn_model_1.add(Conv1D(128, 5, activation='relu'))\n",
    "# cnn_model_1.add(MaxPooling1D(5))\n",
    "# cnn_model_1.add(Conv1D(128, 5, activation='relu'))\n",
    "# cnn_model_1.add(GlobalMaxPooling1D())\n",
    "# cnn_model_1.add(Dense(8, activation='softmax')) \n",
    "\n",
    "# # تجميع النموذج\n",
    "# cnn_model_1.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "# # model.fit(x_train, y_train, batch_size=32, epochs=10)\n",
    "# cnn_model_1.fit(X_train, y_train, epochs=5, batch_size=32, validation_split=0.2)\n",
    "\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, Conv1D, GlobalMaxPooling1D, Dense\n",
    "from keras.initializers import Constant\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.backend import clear_session\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming your training data (X_train_combined) and labels (y_train)\n",
    "# Assuming you have trained your Skip-gram model as per your previous code\n",
    "\n",
    "# Assuming your sequence length is 5 and embedding dimension is 50 (increased sequence length)\n",
    "sequence_length = 5\n",
    "embedding_dim = 50\n",
    "\n",
    "# Assuming you have the vocabulary size and word vectors from the Skip-gram model\n",
    "vocab_size = 10000\n",
    "word_vectors = model.layers[2].get_weights()[0]  # Assuming model is your trained Skip-gram model\n",
    "\n",
    "# Create and compile the CNN model\n",
    "cnn_model_1 = Sequential()\n",
    "cnn_model_1.add(Embedding(input_dim=vocab_size,\n",
    "                          output_dim=embedding_dim,\n",
    "                          embeddings_initializer=Constant(word_vectors),\n",
    "                          input_length=sequence_length,\n",
    "                          trainable=False))  # Fix the pre-trained word vectors\n",
    "cnn_model_1.add(Conv1D(128, 5, activation='relu'))\n",
    "cnn_model_1.add(GlobalMaxPooling1D())\n",
    "cnn_model_1.add(Dense(8, activation='softmax')) \n",
    "\n",
    "# Compile the model\n",
    "cnn_model_1.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Assuming you have your training data (X_train_combined) and labels (y_train)\n",
    "tokenizer = Tokenizer(num_words=vocab_size)\n",
    "tokenizer.fit_on_texts(X_train_combined)\n",
    "X_train_seq = tokenizer.texts_to_sequences(X_train_combined)\n",
    "\n",
    "# Assuming your y_train is one-hot encoded\n",
    "y_onehot1 = pd.get_dummies(train_data['lable'])\n",
    "y_train = y_onehot1.values  # Convert to numpy array\n",
    "\n",
    "# Assuming your X_train_seq is a list of sequences\n",
    "X_train_seq_padded = pad_sequences(X_train_seq, maxlen=sequence_length, padding='post', truncating='post')\n",
    "\n",
    "# Print shapes for debugging\n",
    "print(\"X_train_seq_padded shape:\", X_train_seq_padded.shape)\n",
    "\n",
    "# Train the CNN model\n",
    "cnn_model_1.fit(X_train_seq_padded, y_train, epochs=5, batch_size=32, validation_split=0.2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1659f00477d637aa",
   "metadata": {},
   "source": [
    "### [7.2.2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84450bd910df78ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# فرضاً لدينا مصفوفة الأشعة من Word2Vec\n",
    "word_vectors = # مصفوفة الأشعة من Word2Vec\n",
    "\n",
    "# إنشاء النموذج\n",
    "cnn_model_2 = Sequential()\n",
    "cnn_model_2.add(Embedding(input_dim=word_vectors.shape[0],\n",
    "                    output_dim=word_vectors.shape[1],\n",
    "                    embeddings_initializer=Constant(word_vectors),\n",
    "                    trainable=True))  # السماح بتعديل طبقة التضمين\n",
    "cnn_model_2.add(Conv1D(128, 5, activation='relu'))\n",
    "cnn_model_2.add(MaxPooling1D(5))\n",
    "cnn_model_2.add(Conv1D(128, 5, activation='relu'))\n",
    "cnn_model_2.add(GlobalMaxPooling1D())\n",
    "cnn_model_2.add(Dense(8, activation='softmax'))  # 8 عقد للتصنيف الثماني\n",
    "\n",
    "# تجميع النموذج\n",
    "cnn_model_2.compile(optimizer='adam',\n",
    "               loss='categorical_crossentropy',\n",
    "                 metrics=['accuracy'])\n",
    "\n",
    "# تدريب النموذج (استبدل x_train و y_train ببياناتك)\n",
    "# model.fit(x_train, y_train, batch_size=32, epochs=10)\n",
    "cnn_model_2.fit(X_train, y_train, epochs=5, batch_size=32, validation_split=0.2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d846ec6bd7b7b91f",
   "metadata": {},
   "source": [
    "### [7.2.3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67f8cddd3a0aabc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# فرضاً لدينا حجم الفوكابولاري وأبعاد الأشعة\n",
    "vocab_size = # حجم الفوكابولاري\n",
    "embedding_dim = # أبعاد الأشعة\n",
    "\n",
    "# إنشاء النموذج\n",
    "cnn_model_3 = Sequential()\n",
    "cnn_model_3.add(Embedding(input_dim=vocab_size, output_dim=embedding_dim))\n",
    "cnn_model_3.add(Conv1D(128, 5, activation='relu'))\n",
    "cnn_model_3.add(MaxPooling1D(5))\n",
    "cnn_model_3.add(Conv1D(128, 5, activation='relu'))\n",
    "cnn_model_3.add(GlobalMaxPooling1D())\n",
    "cnn_model_3.add(Dense(8, activation='softmax'))  # 8 عقد للتصنيف الثماني\n",
    "\n",
    "# تجميع النموذج\n",
    "cnn_model_3.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# تدريب النموذج (استبدل x_train و y_train ببياناتك)\n",
    "# model.fit(x_train, y_train, batch_size=32, epochs=10)\n",
    "cnn_model_3.fit(X_train, y_train, epochs=5, batch_size=32, validation_split=0.2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5f15d18ac74bcc2",
   "metadata": {},
   "source": [
    "### [7.2.4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fdcd49751c5a953",
   "metadata": {},
   "outputs": [],
   "source": [
    "# تحميل مصفوفة التضمين (مثال فقط، استبدلها بمصفوفتك)\n",
    "embeddings_index = # تحميل مصفوفة التضمين\n",
    "\n",
    "# إنشاء النموذج\n",
    "cnn_model_4 = Sequential()\n",
    "cnn_model_4.add(Embedding(input_dim=len(embeddings_index), \n",
    "                    output_dim=embedding_dim, \n",
    "                    embeddings_initializer=Constant(embeddings_index),\n",
    "                    trainable=False))  # جمد طبقة التضمين\n",
    "cnn_model_4.add(Conv1D(128, 5, activation='relu'))\n",
    "cnn_model_4.add(MaxPooling1D(5))\n",
    "cnn_model_4.add(Conv1D(128, 5, activation='relu'))\n",
    "cnn_model_4.add(GlobalMaxPooling1D())\n",
    "cnn_model_4.add(Dense(8, activation='softmax'))  # على سبيل المثال، لـ 8 فئات\n",
    "\n",
    "# تجميع النموذج\n",
    "cnn_model_4.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# تدريب النموذج (استبدل x_train و y_train ببياناتك)\n",
    "# model.fit(x_train, y_train, batch_size=32, epochs=10)\n",
    "cnn_model_4.fit(X_train, y_train, epochs=5, batch_size=32, validation_split=0.2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d3f4520e4493202",
   "metadata": {},
   "source": [
    "## [7.3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b878b51740738504",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "52a6d74027ab71b",
   "metadata": {},
   "source": [
    "## Extra [7.4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32b1af7e548effea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d4c5fcdd614e46d",
   "metadata": {},
   "source": [
    "# Final Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42a27f608e5a696b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(comparison_table)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b30ba7f2c380c036",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"[your_name].csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "KFerFDFqPwPI",
    "-4LEgCtvPwaQ",
    "ZgzDVRqCQ72Q",
    "qQlGbCI3Q77R",
    "hIwnQVuas5hE",
    "7orTRN5vs9vp",
    "cwaCNfS9RNZc",
    "MbJeF_CxRNZd",
    "GY-HhD2YRNZe",
    "KV9D86kbRNZe",
    "3fM_FPldSCxp",
    "N2z1KmQXSAFs",
    "fCBHc2y6SAL5",
    "P1aLjPbzRNZe",
    "dbqN7BceSRhS",
    "rad8wPeSSRhU",
    "FbIUIkezSRhU",
    "N4WDktTgRNZf",
    "wbGPWNtoR4p-",
    "V-rCzzoMR41T",
    "iXF2L4Z-Sd3N",
    "ij5gdJA4Sd3O",
    "H6k8JnJDSd3P",
    "1TmDb343Sls9",
    "0V5LD9EWVgY7",
    "vc96CuQStFA9",
    "exdhuH6GtLez",
    "pwtVy8DES8AZ",
    "npObP2JLUBgw",
    "pTIrTECWUBg5",
    "6v0njm5qUBg6"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
